{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07442a1-ea8f-4069-b0be-1487341f1582",
   "metadata": {},
   "source": [
    "# La représentation vectorielle des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793e6c4-380c-4e6f-a4e5-013fc4f8b3fb",
   "metadata": {},
   "source": [
    "Après une introduction à la notion d’espace vectoriel qui a permis de poser un cadre formel pour manipuler des vecteurs grâce à des opérations mathématiques, nous allons nous intéresser à des méthodes qui entrent dans la définition du modèle sémantique vectoriel pour représenter des documents sous une forme numérique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14e9b7-1b9d-4dd2-89b3-49c342a7e43d",
   "metadata": {},
   "source": [
    "## Constituer un sac de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651cf71-a20b-4897-9ee0-9baf0b09b6b5",
   "metadata": {},
   "source": [
    "La technique du sac de mots est un moyen élégant pour inscrire des documents dans un même espace vectoriel, les mots figurant ici les dimensions de l’espace. Si elle inclut toujours une étape de tokenisation, la suite dépendra souvent de l’objectif ou de choix scientifiques assumés. Dans certains cas, on considérera les lemmes ; dans d’autres, les mots-formes uniques, les racines, seulement les entités nommées ou les adjectifs, et peut-être encore ne conserverait-on pas uniquement des *n*-grammes ?\n",
    "\n",
    "Plus qu’une simple technique, le sac de mots devient un véritable modèle dont les choix devront être étayés par des arguments scientifiques voire validés par des méthodes d’évaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6dfd2-1627-414c-84d0-e26f40197840",
   "metadata": {},
   "source": [
    "### Étape 1 : tokenisation du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1f547-87b9-499f-a603-3901fa957a0a",
   "metadata": {},
   "source": [
    "Tokeniser un texte, c’est le segmenter en plusieurs parties (tokens) sans que l’unité ne soit imposée. En TALN, il est fréquent de rencontrer le mot comme unité de la segmentation. Et si là encore il existe plusieurs méthodes, nous en retiendrons une avec la bibliothèque NLTK. Prenons comme exemple le texte de *Salammbô* de Gustave Flauebrt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9482b-b557-42c7-863a-ed12c63392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# textual content of the file\n",
    "with open('./data/salammbo.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# a tokenizer based on a regular expression\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "# processing…\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# save in a file, one word a line\n",
    "with open('./data/salammbo_tokenized.txt', 'w') as f:\n",
    "    for token in tokens:\n",
    "        f.write(token)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa0231-9ce6-461d-adb6-6b81501fdb5e",
   "metadata": {},
   "source": [
    "### Étape 2 : étiquetage et lemmatisation des tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85844569-63d4-4f4b-9f07-d865f28c5bc6",
   "metadata": {},
   "source": [
    "NLTK ne fournissant pas de support pour l’étiquetage en parties du discours pour le français, nous choisissons une option fournie par *TreeTagger* qui a l’avantage de lemmatiser dans la foulée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b7b98-e877-4027-ad1e-827aa735cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./data/salammbo_tokenized.txt | ./TreeTagger/tree-tagger-macos -token -lemma ./TreeTagger/french.par > ./data/salammbo_tagged.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebc29c-09c7-4807-991f-9e408162da6e",
   "metadata": {},
   "source": [
    "### Étape 3 : filtrer les tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b69f9d-07ec-428c-adc9-9a7f7bb3952c",
   "metadata": {},
   "source": [
    "Il est rare que l’on souhaite conserver tous les tokens après la phase de segmentation. Rien que dans les dix premiers de la liste, nous remarquons des éléments que nous ne souhaitons pas :\n",
    "\n",
    "- Les chiffres ;\n",
    "- les majuscules ;\n",
    "- les mots vides de sens.\n",
    "\n",
    "La liste des exclusions n’est d’une part pas exhaustive et, d’autre part, il peut se justifier de ne pas retenir l’une ou l’autre des options. Appliquons malgré tout nos choix à la liste des tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c39f0-672a-4a10-a682-725563e4dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "with open('./data/salammbo_tagged.tsv', newline='', encoding='utf-8') as inputfile, \\\n",
    "open('./data/salammbo_filtered.tsv', 'w', newline='', encoding='utf-8') as outputfile:\n",
    "\n",
    "    # fields\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    # files\n",
    "    reader = csv.DictReader(inputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    # filtered tokens\n",
    "    filtered_rows = [\n",
    "        row for row in reader\n",
    "        if row['token'].lower() not in stopwords.words('french')\n",
    "        and row['token'] not in string.punctuation\n",
    "        and not any(char in string.digits for char in row['token'])\n",
    "    ]\n",
    "\n",
    "    # writing in file\n",
    "    writer.writerows(filtered_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecadbff-0b29-4798-b902-6f9d105f7334",
   "metadata": {},
   "source": [
    "### Étape 4 : nettoyage des entrées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9335775-bd3f-4900-bb6b-b95c72f73874",
   "metadata": {},
   "source": [
    "D’autres traitements peuvent encore être appliqués après toutes ces étapes. Corrigeons juste les entrées qui répondent à ces critères : si l’étiquette est à `NAM` et le lemme à `<unknown>`, alors considérons que le lemme prend la valeur du token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a912cc-3307-4c95-9327-0682ac0402fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/salammbo_filtered.tsv', newline='', encoding='utf-8') as inputfile, \\\n",
    "open('./data/salammbo_clean.tsv', 'w', newline='', encoding='utf-8') as outputfile:\n",
    "\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    reader = csv.DictReader(inputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    for row in reader:\n",
    "        if row['lemma'] == '<unknown>' and row['tag'] == 'NAM':\n",
    "            row['lemma'] = row['token']\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394207ed-8eca-487d-9a15-9eeb6852f743",
   "metadata": {},
   "source": [
    "### Étape 5 : constituer le sac de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef0ac6-81c6-44b7-a558-d5305aa15c64",
   "metadata": {},
   "source": [
    "La phase de pré-traitement achevée, nous pouvons à présent constituer notre sac de mots afin de définir l’espace vectoriel de notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e6488-f0c6-452c-b0c5-7524b8092aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/salammbo_clean.tsv', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    reader = csv.DictReader(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    lemmas = [\n",
    "        row['lemma']\n",
    "        for row in reader\n",
    "        if row['lemma'] != '<unknown>'\n",
    "    ]\n",
    "\n",
    "    BoW = set(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6b76a-21a7-45bd-947f-613b2ab25cc5",
   "metadata": {},
   "source": [
    "## Établir une matrice d’occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28801791-9831-4876-8298-afb0db0f7e8d",
   "metadata": {},
   "source": [
    "Le sac de mots obtenu représente le vocabulaire de notre corpus (constitué en l’occurrence d’un seul document). Si l’on interroge la cardinalité de l’objet `BoW`, nous remarquons immédiatement que nous évoluons dans un espace à très haute dimensionnalité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990baa36-3c7e-48a7-9065-7de2fbc7f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(BoW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd534eb-8b17-4b60-93bb-cc8f42eeb14e",
   "metadata": {},
   "source": [
    "Plutôt que de conserver `BoW` en l’état, nous souhaitons maintenant obtenir un dictionnaire des fréquences de chaque lemme dans le texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fe2fa-1721-4baa-ab95-7191cabdc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "occurrences = Counter(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f769a-985f-452d-a62b-1b3929b2571c",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à instancier une matrice des occurrences avec *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4340b82-70ca-46f8-ac85-f8c715771023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# a null matrix\n",
    "matrix = np.zeros((1, len(BoW)))\n",
    "\n",
    "# fill the occurrences matrix\n",
    "for i, lemma in enumerate(BoW):\n",
    "    matrix[0][i] = occurrences[lemma]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

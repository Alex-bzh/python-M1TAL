{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07442a1-ea8f-4069-b0be-1487341f1582",
   "metadata": {},
   "source": [
    "# La représentation vectorielle des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793e6c4-380c-4e6f-a4e5-013fc4f8b3fb",
   "metadata": {},
   "source": [
    "Après une introduction à la notion d’espace vectoriel qui a permis de poser un cadre formel pour manipuler des vecteurs grâce à des opérations mathématiques, nous allons nous intéresser à des méthodes qui entrent dans la définition du modèle sémantique vectoriel pour représenter des documents sous une forme numérique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14e9b7-1b9d-4dd2-89b3-49c342a7e43d",
   "metadata": {},
   "source": [
    "## Constituer un sac de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651cf71-a20b-4897-9ee0-9baf0b09b6b5",
   "metadata": {},
   "source": [
    "La technique du sac de mots est un moyen élégant pour inscrire des documents dans un même espace vectoriel, les mots figurant ici les dimensions de l’espace. Si elle inclut toujours une étape de tokenisation, la suite dépendra souvent de l’objectif ou de choix scientifiques assumés. Dans certains cas, on considérera les lemmes ; dans d’autres, les mots-formes uniques, les racines, seulement les entités nommées ou les adjectifs, et peut-être encore ne conserverait-on pas uniquement des *n*-grammes ?\n",
    "\n",
    "Plus qu’une simple technique, le sac de mots devient un véritable modèle dont les choix devront être étayés par des arguments scientifiques voire validés par des méthodes d’évaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6dfd2-1627-414c-84d0-e26f40197840",
   "metadata": {},
   "source": [
    "### Étape 1 : tokenisation du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1f547-87b9-499f-a603-3901fa957a0a",
   "metadata": {},
   "source": [
    "Tokeniser un texte, c’est le segmenter en plusieurs parties (tokens) sans que l’unité ne soit imposée. En TALN, il est fréquent de rencontrer le mot comme unité de la segmentation. Et si là encore il existe plusieurs méthodes, nous en retiendrons une avec la bibliothèque NLTK. Prenons comme exemple le texte de *Salammbô* de Gustave Flauebrt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9482b-b557-42c7-863a-ed12c63392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# textual content of the file\n",
    "with open('./data/salammbo.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# a tokenizer based on a regular expression\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "# processing…\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# save in a file, one word a line\n",
    "with open('./data/salammbo_tokenized.txt', 'w') as f:\n",
    "    for token in tokens:\n",
    "        f.write(token)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa0231-9ce6-461d-adb6-6b81501fdb5e",
   "metadata": {},
   "source": [
    "### Étape 2 : étiquetage et lemmatisation des tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85844569-63d4-4f4b-9f07-d865f28c5bc6",
   "metadata": {},
   "source": [
    "NLTK ne fournissant pas de support pour l’étiquetage en parties du discours pour le français, nous choisissons une option fournie par *TreeTagger* qui a l’avantage de lemmatiser dans la foulée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b7b98-e877-4027-ad1e-827aa735cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./data/salammbo_tokenized.txt | ./TreeTagger/tree-tagger-macos -token -lemma ./TreeTagger/french.par > ./data/salammbo_tagged.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebc29c-09c7-4807-991f-9e408162da6e",
   "metadata": {},
   "source": [
    "### Étape 3 : filtrer les tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b69f9d-07ec-428c-adc9-9a7f7bb3952c",
   "metadata": {},
   "source": [
    "Il est rare que l’on souhaite conserver tous les tokens après la phase de segmentation. Rien que dans les dix premiers de la liste, nous remarquons des éléments que nous ne souhaitons pas :\n",
    "\n",
    "- Les chiffres ;\n",
    "- les majuscules ;\n",
    "- les mots vides de sens.\n",
    "\n",
    "La liste des exclusions n’est d’une part pas exhaustive et, d’autre part, il peut se justifier de ne pas retenir l’une ou l’autre des options. Appliquons malgré tout nos choix à la liste des tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c39f0-672a-4a10-a682-725563e4dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "with open('./data/salammbo_tagged.tsv', newline='', encoding='utf-8') as inputfile, \\\n",
    "open('./data/salammbo_filtered.tsv', 'w', newline='', encoding='utf-8') as outputfile:\n",
    "\n",
    "    # fields\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    # files\n",
    "    reader = csv.DictReader(inputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    # filtered tokens\n",
    "    filtered_rows = [\n",
    "        row for row in reader\n",
    "        if row['token'].lower() not in stopwords.words('french')\n",
    "        and row['token'] not in string.punctuation\n",
    "        and not any(char in string.digits for char in row['token'])\n",
    "    ]\n",
    "\n",
    "    # writing in file\n",
    "    writer.writerows(filtered_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecadbff-0b29-4798-b902-6f9d105f7334",
   "metadata": {},
   "source": [
    "### Étape 4 : nettoyage des entrées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9335775-bd3f-4900-bb6b-b95c72f73874",
   "metadata": {},
   "source": [
    "D’autres traitements peuvent encore être appliqués après toutes ces étapes. Corrigeons juste les entrées qui répondent à ces critères : si l’étiquette est à `NAM` et le lemme à `<unknown>`, alors considérons que le lemme prend la valeur du token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a912cc-3307-4c95-9327-0682ac0402fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/salammbo_filtered.tsv', newline='', encoding='utf-8') as inputfile, \\\n",
    "open('./data/salammbo_clean.tsv', 'w', newline='', encoding='utf-8') as outputfile:\n",
    "\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    reader = csv.DictReader(inputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    for row in reader:\n",
    "        if row['lemma'] == '<unknown>' and row['tag'] == 'NAM':\n",
    "            row['lemma'] = row['token']\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394207ed-8eca-487d-9a15-9eeb6852f743",
   "metadata": {},
   "source": [
    "### Étape 5 : constituer le sac de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef0ac6-81c6-44b7-a558-d5305aa15c64",
   "metadata": {},
   "source": [
    "La phase de pré-traitement achevée, nous pouvons à présent constituer notre sac de mots afin de définir l’espace vectoriel de notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e6488-f0c6-452c-b0c5-7524b8092aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/salammbo_clean.tsv', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "    fieldnames = ['token', 'tag', 'lemma']\n",
    "\n",
    "    reader = csv.DictReader(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "\n",
    "    lemmas = [\n",
    "        row['lemma']\n",
    "        for row in reader\n",
    "        if row['lemma'] != '<unknown>'\n",
    "    ]\n",
    "\n",
    "    BoW = sorted(set(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6b76a-21a7-45bd-947f-613b2ab25cc5",
   "metadata": {},
   "source": [
    "## Établir une matrice d’occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28801791-9831-4876-8298-afb0db0f7e8d",
   "metadata": {},
   "source": [
    "Le sac de mots obtenu représente le vocabulaire de notre corpus (constitué en l’occurrence d’un seul document). Si l’on interroge la cardinalité de l’objet `BoW`, nous remarquons immédiatement que nous évoluons dans un espace à très haute dimensionnalité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990baa36-3c7e-48a7-9065-7de2fbc7f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(BoW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86990ef-6600-4fd1-98f9-410fba133c0e",
   "metadata": {},
   "source": [
    "### Étape 1 : un dictionnaire des fréquences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd534eb-8b17-4b60-93bb-cc8f42eeb14e",
   "metadata": {},
   "source": [
    "Plutôt que de conserver `BoW` en l’état, nous souhaitons maintenant obtenir un dictionnaire des fréquences de chaque lemme dans le texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fe2fa-1721-4baa-ab95-7191cabdc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "occurrences = Counter(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718eb5d9-4fb4-4ce0-9bf2-238ef590c57d",
   "metadata": {},
   "source": [
    "### Étape 2 : bienvenue dans la matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f769a-985f-452d-a62b-1b3929b2571c",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à instancier une matrice des occurrences avec *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4340b82-70ca-46f8-ac85-f8c715771023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# a null matrix\n",
    "matrix = np.zeros((1, len(BoW)))\n",
    "\n",
    "# fill the occurrences matrix\n",
    "for i, lemma in enumerate(BoW):\n",
    "    matrix[0][i] = occurrences[lemma]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa5faa-ad11-443c-a8e5-52ac3648d7b9",
   "metadata": {},
   "source": [
    "Pour plus de clarté, on peut la visualiser dans *Pandas* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea4e2d-6066-4fd6-bdbe-3738ab1af2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=BoW)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78751b-9cf9-442b-9229-afe670407ceb",
   "metadata": {},
   "source": [
    "### Intuition du fléau de la dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b629a0-bee2-43c3-9e81-e06c9320b804",
   "metadata": {},
   "source": [
    "En affichant la matrice d’occurrences, on se rend mieux compte de l’ampleur du phénomène : un espace en 5798 dimensions, c’est énorme ! D’autant que la plupart de ces dimensions ont des valeurs très faibles. Et encore, nous n’avons vectorisé qu’un seul document, ce qui nous assure que tous les mots du vocabulaire apparaissent au moins une fois.\n",
    "\n",
    "Admettons avant de poursuivre que nous rajoutons un document qui comporte une seule phrase :\n",
    "\n",
    "```txt\n",
    "(1) Le petit chat boit du lait.\n",
    "```\n",
    "\n",
    "En lui appliquant le même traitement que pour le texte de *Salammbô*, nous le réduisons aux mots : *petit*, *chat*, *boire* et *lait*. Nous nous assurons qu’ils appartiennent au vocabulaire de l’œuvre et pouvons maintenant le définir comme le **vocabulaire commun** aux deux documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb37ed-9f76-4f37-b1bb-bb5564df8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Occurrences des mots du vocabulaire du document (1) dans le document (0) :\",\n",
    "    f\"petit : {df.petit.values[0]}\",\n",
    "    f\"chat : {df.chat.values[0]}\",\n",
    "    f\"boire : {df.boire.values[0]}\",\n",
    "    f\"lait : {df.lait.values[0]}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb33e4-ead5-453e-b877-041a9f32551f",
   "metadata": {},
   "source": [
    "Qu’en est-il des occurrences du vocabulaire de *Salammbô* dans le document (1) ? Sur 5798 dimensions, 5794 sont fixées avec la valeur 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393b905-987c-49fb-a4d0-b3aa7f4c83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new document with only zeros\n",
    "df.loc[len(df)] = np.zeros(len(BoW))\n",
    "\n",
    "# data entered manually\n",
    "df.loc[1].petit = 1\n",
    "df.loc[1].chat = 1\n",
    "df.loc[1].boire = 1\n",
    "df.loc[1].lait = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b6df4-870d-4cc4-84c4-d101f65988a5",
   "metadata": {},
   "source": [
    "### Des matrices creuses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff5113-bcba-4cc4-9b8b-f889e1d1612a",
   "metadata": {},
   "source": [
    "On parle alors de **matrice creuse** (ou *sparse matrix* en anglais). La taille de la matrice peut déjà poser problème aux algorithmes d’apprentissage, et sa sparsité, en augmentant la complexité du calcul, peut également accroître les risques d’instabilité. On peut conseiller dans ces cas d’utiliser des structures plus adaptées comme **CSR** (*Compressed Sparse Row*) ou **CSC** (*Compressed Sparse Column*) dans `scipy.sparse` qui stockent les éléments non nuls de manière comprimée pour économiser de la mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d891ee7-abcc-4303-bd10-962d6c92db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# a compressed sparse matrix\n",
    "sparse_matrix = csr_matrix(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c22a33-92ef-4191-9c98-0b325e1982a8",
   "metadata": {},
   "source": [
    "### Évolution vers d’autres méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8ae06-fcf9-4583-b4c9-e764d3b44a9b",
   "metadata": {},
   "source": [
    "La matrice d’occurrences n’est pas une finalité du modèle vectoriel – elle ne rend compte que de fréquences brutes sans juger de leur importance relative – mais elle amène à un autre concept qui sera développé plus tard, celui de la pondération TF-IDF.\n",
    "\n",
    "Maintenant que nous avons été confronté·e aux limites des matrices d’occurrences simples, nous comprenons mieux en quoi il est primordial de bien travailler son modèle de sac de mots en fonction de ses objectifs : plus on contrôle la taille du vocabulaire, mieux les outils mathématiques que nous mobiliserons par la suite se comporteront."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

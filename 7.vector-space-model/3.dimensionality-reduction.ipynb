{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c357dc92-be8d-4318-bbd1-c27a62a7be6b",
   "metadata": {},
   "source": [
    "# Introduction à la réduction de la dimensionnalité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29004b3-3f95-485e-8fc5-b163a201afc5",
   "metadata": {},
   "source": [
    "Notre parcours dans la définition du modèle vectoriel nous a conduit·e de la constitution d’un sac de mots par divers procédés à l’établissement d’une matrice d’occurrences. Nous avons d’ailleur quitté le chapitre précédent avec un double problème : d’une part la seule approche fréquentielle était incapable de rendre compte de l’importance relative d’un terme dans un document et encore moins dans un corpus ; d’autre part la constitution d’une matrice d’occurrences nous avait rapidement amené·e, en raison de la taille du vocabulaire commun à considérer, à évoluer dans des espaces en très haute dimension qui sont majoritairement remplis de vide.\n",
    "\n",
    "Dans ce chapitre, nous allons remédier au premier et dessiner l’ébauche d’une méthode pour projeter les documents dans un espace réduit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d17ddd-3385-42cb-af64-98e16239e357",
   "metadata": {},
   "source": [
    "## Évaluer l’importance d’un terme dans un document (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3b737-77e4-43d8-a2bf-da53e551e0da",
   "metadata": {},
   "source": [
    "Nous l’avons vu précédemment, certains termes dans un corpus sont plus importants que d’autres pour caractériser un texte par rapport à un autre, et leur importance n’est souvent pas proportionnelle à leur fréquence. De là découle la nécessité de repérer les mots vides de sens, les *stopwords*, pour les retirer du sac de mots (BoW) qui le représente. Pour autant, la méthode BoW se contente d’une mesure fréquentielle sans établir de rapport d’importance entre les termes. La matrice d’occurrences reste ainsi assez pauvre pour rendre compte de sémantique.\n",
    "\n",
    "Une autre approche, largement répandue dans le traitement automatique du langage naturel, parvient à inférer, de l’analyse fréquentielle, une certaine valeur d’importance aux termes contenus dans le sac de mots. Cette approche repose sur deux principes : la fréquence du terme (TF) et la fréquence du terme dans le corpus (IDF) qui prêtent une signification à la rareté d’un terme.\n",
    "\n",
    "Sans parler de robustesse, la justification de cette approche repose sur la loi de Zipf qui prévoit que la fréquence d’un terme dans un texte est liée à son rang dans l’ordre des fréquences : le mot le plus fréquent apparaîtrait dix fois plus souvent que le dixième mot le plus fréquent, cent fois plus que le centième etc. En grande partie pour cette raison, la méthode TF-IDF ne souffre pas de la présence des mots vides dans le sac de mots.\n",
    "\n",
    "Dans sa formulation générale, l’expression mathématique vaut :\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\ln\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- *t* est le token ;\n",
    "- *d* figure le nombre total de tokens dans le document ;\n",
    "- *N* représente le nombre total de documents ;\n",
    "- et $\\text{df}(t)$ calcule la fréquence du token dans le document.\n",
    "\n",
    "On lui préfère toutefois souvent la version lissée afin d’éviter des divisions par zéro et de privilégier des mots très rares qui pourraient avoir une empreinte trop importante :\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\ln\\left(\\frac{N}{1 + \\text{df}(t)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60acdf-3a82-407a-827e-237ee58a0a9e",
   "metadata": {},
   "source": [
    "### La fréquence du terme (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed49f0-90d1-4cf5-959d-45042ace6ad8",
   "metadata": {},
   "source": [
    "De l’anglais *term frequency*, la fréquence du terme établit un rapport entre le nombre d’occurrences d’un token ($t$) dans un document et le nombre total de mots dans ce document ($d$) :\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{t}{d}\n",
    "$$\n",
    "\n",
    "Prenons un corpus constitué de trois textes :\n",
    "\n",
    "```txt\n",
    "(A) Le petit chat boit du lait. Le lait n’est pas bon pour les chats.\n",
    "(B) Les petits chiens boivent de l’eau. L’eau irait aussi aux chats.\n",
    "(C) À partir d’un moment, eau ou lait, ils peuvent bien boire ce qu’ils veulent.\n",
    "```\n",
    "\n",
    "La taille en mots du document *A* est de 15, quand elle est de 13 pour le document *B* et de 16 pour le document *C*. Intéressons-nous au mot *lait* ($1$) qui apparaît deux fois dans *A* et une fois dans *C*, mais jamais dans *B*. Ses fréquences seront :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{TF}_{1, A} &= \\frac{2}{15} = 0,1333 \\\\\n",
    "    \\text{TF}_{1, B} &= \\frac{0}{13} = 0 \\\\\n",
    "    \\text{TF}_{1, C} &= \\frac{1}{16} = 0,0625 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3395d10-d5e5-468a-a6e0-202c1d989332",
   "metadata": {},
   "source": [
    "### La fréquence inverse de document (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04eb88d-c378-4802-b33e-ff77fb21864f",
   "metadata": {},
   "source": [
    "Quand la mesure TF s’attachait au terme dans un document, la mesure IDF (*inverse document frequency*) va s’intéresser à la présence du terme dans le corpus entier selon la relation suivante où $d$ représente le nombre de documents où le terme apparaît et $N$ le nombre total de documents :\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\ln{\\frac{N}{\\text{df}(t)}}\n",
    "$$\n",
    "\n",
    "Dans notre exemple, la mesure IDF pour le mot *lait* vaut :\n",
    "\n",
    "$$\n",
    "\\text{IDF}_1 = \\ln{\\frac{3}{2}} \\approx 0.4055\n",
    "$$\n",
    "\n",
    "Le calcul du logarithme permet de pondérer le rapport entre $N$ et $d$ dans la mesure où, lors de l’obtention de TF, le résultat était situé dans un intervalle $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd33d-8068-41c7-b91e-87833efcd2a9",
   "metadata": {},
   "source": [
    "### La mesure TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdca72-25e9-4fd6-9ffd-7188af3fc954",
   "metadata": {},
   "source": [
    "Au final, la formule TF-IDF est un produit entre TF et IDF. Pour notre exemple avec le mot *lait* :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{TF-IDF}_{1, A} &= \\frac{2}{15} \\cdot ln \\frac{3}{2} \\approx 0,0541 \\\\\n",
    "    \\text{TF-IDF}_{1, B} &= \\frac{0}{13} \\cdot ln \\frac{3}{2} = 0 \\\\\n",
    "    \\text{TF-IDF}_{1, C} &= \\frac{1}{16} \\cdot ln \\frac{3}{2} \\approx 0,0253 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Au regard du mot *lait*, le document *A* apparaît ainsi comme plus pertinent. Notons que si nous avions opté pour le TF-IDF avec lissage, nous aurions obtenu des scores de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70c0a1-db3a-47d2-b779-4c302c369398",
   "metadata": {},
   "source": [
    "### Limitations du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f01a26-a2a5-4e59-935b-ace590b608d7",
   "metadata": {},
   "source": [
    "Quoique très largement utilisé pour sa facilité de mise en œuvre en dépit du coût en termes de calcul machine, la mesure TF-IDF souffre principalement de son incapacité à traiter la sémantique du terme.\n",
    "\n",
    "Gardons par ailleurs à l’esprit que la formule accordera mécaniquement davantage d’importance aux documents très volumineux, aussi faudra-t-il penser à les pénaliser ou bien à ne travailler que sur des corpus équilibrés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8474df-c021-4f26-bde9-69b3a4aa0164",
   "metadata": {},
   "source": [
    "### Normalisation des pondérations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5146c75-ee17-4886-a435-3711a9965dc6",
   "metadata": {},
   "source": [
    "Pour atténuer l’effet des documents de tailles différentes qui auront des valeurs plus élevées, les scores TF-IDF sont souvent normalisés. Deux normes sont retenues :\n",
    "\n",
    "1. La norme $L^1$ qui divise chaque pondération par la somme absolue des pondérations ;\n",
    "2. La norme $L^2$ qui divise chaque pondération TF-IDF par la norme euclidienne du vecteur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c41c95-2bd9-4257-8f1d-40743526a09a",
   "metadata": {},
   "source": [
    "#### Norme $L^1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d98c4-d91c-4172-a279-86216aa097ac",
   "metadata": {},
   "source": [
    "La formule vaut :\n",
    "\n",
    "$$\n",
    "\\text{Norme } L^1 : \\frac{x_i}{\\sum |x_i|}\n",
    "$$\n",
    "\n",
    "Si nous l’appliquons au mot *lait*, nous calculons la somme absolue des pondérations :\n",
    "\n",
    "$$\n",
    "\\sum |x_i| = 0.0541 + 0 + 0.0253 = 0.0794\n",
    "$$\n",
    "\n",
    "Avant de diviser chaque valeur avec :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{TFIDF}_{1, A} &= \\frac{0.0541}{0.0794} = 0.6814 \\\\\n",
    "    \\text{TFIDF}_{1, B} &= \\frac{0}{0.0794} = 0 \\\\\n",
    "    \\text{TFIDF}_{1, C} &= \\frac{0.0253}{0.0794} = 0.3186 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Cette fois-ci, l’addition des pondérations sera toujours égale à 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91445f2c-c965-42a8-afd0-0f299cea6f28",
   "metadata": {},
   "source": [
    "#### Norme $L^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a77962-f3d8-4776-ad00-4b47541b6072",
   "metadata": {},
   "source": [
    "Il s’agit de la formule retenue par défaut par les algorithmes des bibliothèques spécialisées et s’exprime par :\n",
    "\n",
    "$$\n",
    "\\text{Norme } L^2 : \\frac{x_i}{\\sqrt{\\sum x_i^2}}\n",
    "$$\n",
    "\n",
    "Si nous calculons d’abord le dénominateur :\n",
    "\n",
    "$$\n",
    "\\sqrt{\\sum x_i^2} = \\sqrt{(0.0541)^2 + (0)^2 + (0.0253)^2} = \\sqrt{0.002927 + 0 + 0.000640} = 0.0597\n",
    "$$\n",
    "\n",
    "Nous pouvons ensuite normaliser les pondérations du mot *lait* :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{TFIDF}_{1, A} &= \\frac{0.0541}{0.0597} = 0.9062 \\\\\n",
    "    \\text{TFIDF}_{1, B} &= \\frac{0}{0.0597} = 0 \\\\\n",
    "    \\text{TFIDF}_{1, C} &= \\frac{0.0253}{0.0597} = 0.4238 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Si avec la norme $L^2$ la somme des pondérations n’est plus égale à 1, la norme du vecteur $\\vec{\\text{lait}}$ l’est toutefois :\n",
    "\n",
    "$$\n",
    "\\| \\vec{\\text{lait}} \\| = \\sqrt{(0.9061)^2 + (0)^2 + (0.4238)^2} = \\sqrt{0.821 + 0 + 0.179} = \\sqrt{1} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456b93b-fc3a-4ab2-85fa-1232480c856d",
   "metadata": {},
   "source": [
    "## Réduire la dimensionnalité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40435660-6bf4-43b0-97ac-fabf79b78f66",
   "metadata": {},
   "source": [
    "Avec la matrice de pondération TF-IDF normalisée selon $L^1$ ou $L^2$, nous avons réintroduit une représentation vectorielle plus équilibrée des documents. Si ces méthodes permettent de mieux comparer les documents entre eux et de capturer des relations plus subtiles entre les termes, la représentation reste de haute dimension, ce qui peut poser des défis en termes de calculs et d’interprétabilité.\n",
    "\n",
    "L’esprit humain n’est pas habitué à se représenter des figures géométriques au-delà de trois dimensions – à part peut-être pour l’octachore, un 4-cube, popularisé sous la dénomination de tesseract, aussi est-il souvent nécessaire d’adopter une stratégie pour réduire leur dimensionnalité. En changeant de perspective, un cube peut se réduire à un carré, un carré à une ligne, une ligne à un point.\n",
    "\n",
    "Grâce à des techniques comme **l’analyse sémantique latente** (*Latent Semantic Analysis* en anglais, ou LSA), nous pouvons projeter les vecteurs représentant les documents dans un sous-espace où des structures sémantiques latentes, jusque-là invisibles, émergent. Cette transformation nous permet non seulement de rendre les calculs plus efficaces, mais aussi de révéler des relations sémantiques profondes entre les documents, facilitant ainsi des tâches comme la recherche d’information, la classification, ou le clustering.\n",
    "\n",
    "![Hypercube 0D to 4D](./images/hypercube_0D-4D.gif)\n",
    "\n",
    "Vitaly Ostrosablin. – *From Point to Tesseract*. – CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab508c99-5bbe-4307-8b37-7a707ad566d8",
   "metadata": {},
   "source": [
    "### Le fléau de la dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da662d55-f786-4e62-8ab3-281d14ccdbf7",
   "metadata": {},
   "source": [
    "À l’heure actuelle, notre document, le texte de *Salammbô*, est représenté par une matrice de dimension $ \\times 5798$, soit bien plus qu’un tesseract – et on a vu le grabuge que ça a fini par causer aux Avengers ! Épargnons-leur des ennuis à venir et optons pour la réduction de nos données à une dimension moindre. D’autres bénéfices bien réels sont attendus :\n",
    "- une visualisation facilitée des données ;\n",
    "- la réduction des coûts de calculs ou d’acquisition ;\n",
    "- l’amélioration de la qualité de modèles d’apprentissage.\n",
    "\n",
    "Une fois les avantages énumérés, qu’avons-nous réellement dit du problème ? Pourquoi la dimensionnalité serait-elle un fléau ? L’expression *Curse of dimensionality* vient du mathématicien Richard Ernest Bellman pour qualifier entre autres le coût de résolution d’un problème pour une variable supplémentaire (Richard Ernest Bellman, *Adaptive control processes: a guided tour*, Princeton University Press, 1961). Un statisticien américain, Leo Breiman, fournit un exemple de comparaison. Considérons une matrice de dimension $100 \\times 1$ avec des valeurs comprises dans l’intervalle $[0,1]$ :\n",
    "\n",
    "|Observation|Valeur|\n",
    "|-|-|\n",
    "|$n_1$|0.1891|\n",
    "|$n_2$|0.7023|\n",
    "|$n_3$|0.0452|\n",
    "|$n_i$|…|\n",
    "|$n_{100}$|0.9036|\n",
    "\n",
    "Ces observations occupent une certaine place dans l’espace unidimensionnel et il est assez simple d’en obtenir une visualisation ou d’inférer des résultats. Si maintenant nous envisageons un espace à seulement dix dimensions, il faudrait $10^{20}$ observations pour obtenir une densité équivalente et permettre une analyse statistique comparable.\n",
    "\n",
    "Autre exemple peut-être un peu plus parlant : la distance moyenne entre deux points pris au hasard dans un carré de 1 cm de côtés (un hypercube 2D unité) sera de 0,52 cm alors qu’elle sera de 0,66 cm dans un cube ! Pour aller plus loin, la distance moyenne entre deux points sera de 4 cm dans un hypercube unité à 10 dimensions, de 12 cm dans un hypercube unité à 1000 dimensions etc. En somme, les espaces de grande dimension sont majoritairement constitués de vide.\n",
    "\n",
    "Un autre fait amusant est que dans un carré unité, il est très improbable qu’un point tiré au hasard soit proche d’un bord. Les chances d’être à moins de 0,001 cm d’un côté sont à peu près de 0,4 % dans un carré de 1 cm. Et dans les espaces de grande dimension, c’est l’inverse ! Autrement dit, plus on multiplie les variables, plus il y a de risques de rencontrer des valeurs extrêmes dans une dimension !\n",
    "\n",
    "Pour finir de se convaincre, prenons une dizaine d’observations décrites par deux variables :\n",
    "\n",
    "|Observation|Variable 1|Variable 2|\n",
    "|-|-|-|\n",
    "|$n_1$|1,2|6,3|\n",
    "|$n_2$|4,3|5|\n",
    "|$n_3$|3,5|4|\n",
    "|$n_4$|1|4|\n",
    "|$n_5$|2.3|5|\n",
    "|$n_6$|4|9.7|\n",
    "|$n_7$|6.8|5.4|\n",
    "|$n_8$|8.1|9|\n",
    "|$n_9$|6|1|\n",
    "|$n_{10}$|5|5|\n",
    "\n",
    "Et projetons-les dans un plan :\n",
    "\n",
    "![Projection 2D](./images/2D-projection.png)\n",
    "\n",
    "Le cercle en orange délimite une frontière d’un rayon de 2 unités autour de l’observation $n_{10}$ afin de montrer que ses plus proches voisins sont les observations $n_2$, $n_3$ et $n_7$.\n",
    "\n",
    "Si maintenant nous projetons les mêmes données dans un espace unidimensionnel en considérant que la variable 2 est inutile pour les décrire, nous voyons que les plus proches voisins ne $n_{10}$ sont maintenant $n_2$, $n_6$ et $n_9$ :\n",
    "\n",
    "![Projection 1D](./images/1D-projection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bf376-1307-4f14-9059-7ca40e852632",
   "metadata": {},
   "source": [
    "### Méthodes de réduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e141bf-4145-470a-a31c-982fff37d9ee",
   "metadata": {},
   "source": [
    "La réduction de dimension repose sur l’hypothèse, très souvent vérfiée, selon laquelle des données dans un espace *n*-dimensionnel sont situées à proximité d’un sous-espace *k*-dimensionnel. Quel que soit l’algorithme de réduction envisagé, il implique nécessairement une perte d’information qu’il faudra minimiser en le paramétrant justement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c12e7-2563-40dc-b7b2-c763a725b099",
   "metadata": {},
   "source": [
    "#### L’approche naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286295b9-551c-4699-937c-fdabd0e45796",
   "metadata": {},
   "source": [
    "Dans un projet, en fonction de l’objet d’étude, il ne sera pas nécessaire de conserver toutes les variables qui décrivent les observations. Si l’on veut étudier la relation entre la taille et le poids d’un individu, il sera sans doute intéressant de connaître son âge, son sexe, son régime alimentaire, son activité physique, mais inutile de retenir la marque de sa voiture, le nombre de voyages qu’il a effectués ou s’il a fréquenté un établissement hospitalier dans les cinq dernières années.\n",
    "\n",
    "L’apport de la littérature scientifique ou de l’avis d’un·e expert·e du domaine pourront être d’une aide précieuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba71d6-7453-41b2-a80a-0b61c6fe355e",
   "metadata": {},
   "source": [
    "#### Révéler des structures sous-jacentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805258c7-398a-4eb0-a8a8-8e53b95a2ec9",
   "metadata": {},
   "source": [
    "L’idée principale des méthodes de réduction de la dimensionnalité est de révéler des structures latentes reliant les dimensions, alors que la majorité d’entre elles ne contribue qu’à une infime partie aux phénomènes étudiés. Pour y parvenir, ces méthodes s’appuient sur des opérations fondamentales et robustes de l’algèbre linéaire (produit matriciel, produit extérieur, décomposition matricielle, etc.) afin de manipuler les données et faire émerger des schémas significatifs, comme des variances inter et intra-clusters ou encore des relations sémantiques.\n",
    "\n",
    "Parmi la multitude de méthodes, nous pouvons citer les suivantes :\n",
    "\n",
    "- **L’analyse en composantes principales (PCA) :** Une méthode non supervisée qui cherche à maximiser la variance expliquée. Elle est basée sur la décomposition en valeurs singulières (SVD) ou sur la diagonalisation d’une matrice de covariance pour identifier les axes principaux des données.\n",
    "- **La factorisation de matrices non-négatives (NMF) :** Une méthode non supervisée qui approxime une matrice par le produit de deux matrices de rang inférieur avec des éléments non négatifs. Elle est souvent utilisée pour extraire des composantes significatives comme des sujets dans des documents.\n",
    "- **L’analyse discriminante linéaire (LDA) :** Utilisée principalement pour la classification, cette méthode supervisée cherche à maximiser la séparation entre les classes tout en minimisant la variance intra-classe.\n",
    "- **L’analyse sémantique latente (LSA) :** En utilisant la décomposition SVD sur une matrice termes-documents, la LSA parvient à capturer des relations entre les termes.\n",
    "- **L’allocation de Dirichlet latente :** Une méthode probabiliste qui identifie des thèmes latents dans des documents en représentant chaque document comme une distribution de sujets et chaque sujet comme une distribution de mots.\n",
    "- **La t-SNE :** Une méthode probabiliste non linéaire qui réduit la dimensionnalité tout en préservant les relations locales, utile pour visualiser des données complexes dans deux ou trois dimensions.\n",
    "\n",
    "Chacune a ses avantages, ses limites et des contextes d’application spécifiques qui impliquent de les choisir avec soin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08600511-26b5-4e4d-b8d5-dcbc432cf4f9",
   "metadata": {},
   "source": [
    "# L’analyse sémantique latente (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7e043-5008-4a56-acdd-c71529a7a1db",
   "metadata": {},
   "source": [
    "Le modèle vectoriel décrit par Gérard Salton dans son article [*A vector space model for automatic indexing*](https://dl.acm.org/doi/10.1145/361219.361220) publié en 1970, propose de représenter un document par un sac de mots, eux-mêmes réduits à leur racine, avant de les transformer en une matrice d’occurrences pondérée par TF-IDF qui peut servir de base à un calcul de similarité entre les documents.\n",
    "\n",
    "Si la pondération TF-IDF permet de réintroduire une relation entre le terme et le document, elle échoue toutefois à rendre compte de sémantique. C’est ensuite en 1990 que Scott Deerwester et al. publient un papier sur une méthode brevetée deux ans plus tôt et intitulé : [*Indexing by Latent Semantic Analysis*](https://web.stanford.edu/class/linguist289/lsi.pdf). Leur approche se justifie par plusieurs aspects :\n",
    "\n",
    "- **Sparsité :** La matrice TF-IDF est trop creuse et contient davantage les termes propres à chaque document que ceux qui les relient tous.\n",
    "- **Synonymie :** Par extension, les termes ne sont pas reliés par une notion de synonymie mais uniquement par une notion de rareté.\n",
    "- **Bruit :** Certains termes n’apparaissent que trop rarement dans un corpus et la pondération TF-IDF peut leur accorder trop d’importance.\n",
    "- **Taille :** La très haute dimensionnalité de l’espace, en plus d’être majoritairement constitué de vide, peut être supérieure aux capacités pratiques de calcul des machines.\n",
    "\n",
    "En trouvant une matrice de rang inférieur par décomposition, la LSA vise à capturer les relations latentes dans la matrice initiale en projetant les termes et les documents dans un espace réduit qui représente des concepts. Les résultats obtenus pourront à leur tour servir à améliorer la performance des modèles de classification ou de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943f224-7c8f-4295-9534-001edfffc944",
   "metadata": {},
   "source": [
    "## Une chaîne de traitement pour la LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e646f99-c440-48ed-8e3a-1be0791f4ffb",
   "metadata": {},
   "source": [
    "Une chaîne de traitement envisageable passerait par les étapes successives :\n",
    "\n",
    "1. Modélisation BoW\n",
    "    - tokenisation\n",
    "    - étiquetage morpho-syntaxique\n",
    "    - racinisation (ou, mieux, lemmatisation)\n",
    "2. Pondération TF-IDF (éventuellement normalisation)\n",
    "3. Décomposition en valeurs singulières (SVD)\n",
    "5. Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f9089-f023-4dd6-a710-00ff8c4ea61c",
   "metadata": {},
   "source": [
    "## Décomposition en valeurs singulières"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552f18a-e9fe-491a-8532-2e2bce9e61cc",
   "metadata": {},
   "source": [
    "Partons de la situation où nous disposons d’une matrice TF-IDF normalisée avec la norme $L^2$ et constituée à partir d’un corpus de trois textes de Nietzsche préalablement lemmatisés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a036bc-3162-43d5-98bf-ee4638f55805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/fool.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "vocabulary = data['vocabulary']\n",
    "matrix = data['matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5da68e-aa97-4430-9e8b-299b0be47713",
   "metadata": {},
   "source": [
    "Visualisons-la dans un *data frame* avec *Pandas* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48781686-3ce4-4bed-b329-13fb2a12aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(matrix, columns=vocabulary)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd272ba-ee4d-40e6-bd27-929a787ae74f",
   "metadata": {},
   "source": [
    "La décomposition SVD revient à considérer la matrice TF-IDF comme un produit de trois matrices plus simples :\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- $U$ est une matrice unitaire constituée des vecteurs propres de $AA^T$\n",
    "- $\\Sigma$ une matrice des valeurs singulières de $A$\n",
    "- et $V^T$ une matrice unitaire constituée des vecteurs propres de $A^TA$\n",
    "\n",
    "Dans le cadre de la LSA, on va plutôt interpréter les matrices ainsi :\n",
    "\n",
    "- $U$ pour les termes\n",
    "- $\\Sigma$ pour l’importance des concepts\n",
    "- $V^T$ pour les documents\n",
    "\n",
    "La décomposition se résoud en une passe avec la méthode `.linalg.svd()` de *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a59884-0681-454f-8501-283c52c0fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "U, S, VT = np.linalg.svd(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534e092-7abf-4fcb-94f9-cae97616e1a4",
   "metadata": {},
   "source": [
    "La matrice $\\Sigma$ contient désormais le vecteur des concepts. Comme il y a autant de concepts que de documents dans la matrice d’origine, le tout est de déterminer le nombre de concepts nécessaires pour expliquer un maximum la variance des données. La visualisation d’un **diagramme d’éboulis** peut aider à représenter le phénomène : à partir du moment où la pente s’arrête et qu’un coude se matérialise, nous pouvons en déduire que les concepts supplémentaires n’apportent pas d’explication significative à la variance.\n",
    "\n",
    "Avant de regarder ce diagramme, il faut noter que les valeurs dans les matrices ne sont pas triés par ordre de grandeur. Or, si nous voulons visualiser un phénomène décroissant, nous devons au préalable trier les matrices sans perdre les indices d’assignation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecef53b-06cf-41e0-83d5-0c7c2e10578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of indices\n",
    "indices = np.argsort(S)[::-1]\n",
    "\n",
    "# sort S and the others as well\n",
    "S = np.sort(S)[::-1]\n",
    "U = U[:, indices]\n",
    "VT = VT[indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a32e08-2822-4a6f-a6df-3268ccf780fc",
   "metadata": {},
   "source": [
    "Jetons à présent un œil au diagramme d’éboulis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa40c3-be0f-4ca9-ba0b-d794e9b4f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=range(1, len(S) + 1), y=S, marker=\"o\")\n",
    "\n",
    "# title and labels\n",
    "plt.title(\"Diagramme d’éboulis\")\n",
    "plt.xlabel(\"Concepts\")\n",
    "plt.ylabel(\"Valeurs singulières\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35002413-aa29-4cf7-89d4-79e1f7a27b5a",
   "metadata": {},
   "source": [
    "La courbe décroissante semble parfaitement droite. Il n’y a en fait pas grand chose d’étonnant à cette curiosité comme on a normalisé notre matrice TF-IDF avec la norme $L^2$ avant la décomposition SVD. Le propre de la norme $L^2$ est de faire que les normes des vecteurs documents soient égales à 1.\n",
    "\n",
    "Passons à une meilleure option : comparer la variance cumulative expliquée au seuil de 90 % :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9387c45-712c-4a06-a45c-3a5e8d58ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = (S ** 2) / sum(S ** 2)\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=range(1, len(S) + 1), y=cumulative_variance, marker=\"o\")\n",
    "\n",
    "plt.axhline(y=0.9, color=\"r\", linestyle=\"--\", label=\"90% Variance\")\n",
    "\n",
    "# title and labels\n",
    "plt.title(\"Variance cumulative expliquée\")\n",
    "plt.xlabel(\"Concepts\")\n",
    "plt.ylabel(\"Variance Cumulative\")\n",
    "\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b44ee-cb9b-4e4b-b500-dec370727b8b",
   "metadata": {},
   "source": [
    "Selon ce diagramme, le nombre idéal de concepts à retenir se situe entre 2 et 3. Comme l’objectif est de réduire la dimensionnalité de notre jeu de données, nous nous contentons des deux premiers concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6254d4-2882-4329-92e7-711f79ec0ec3",
   "metadata": {},
   "source": [
    "## Projection des données dans la matrice réduite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f38863-f04d-455b-a17b-7cd2b0b5d765",
   "metadata": {},
   "source": [
    "Des trois concepts, nous n’avons retenu que les deux premiers au titre qu’ils expliquent 70 % de la variance totale. En pratique, nous ne devrions pas nous contenter d’un taux si bas mais ce résultat s’explique par le peu de concepts qui ont émergé des documents.\n",
    "\n",
    "Constituons le sous-espace des concepts latents et projetons dedans les documents et les termes de notre étude :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1283d-d2a3-4efa-802d-cb1ddcd0e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of concepts to keep\n",
    "k = 2\n",
    "\n",
    "# remove unwanted data\n",
    "U_k = U[:, :k]\n",
    "S_k = np.diag(S[:k])\n",
    "VT_k = VT[:k, :]\n",
    "\n",
    "# documents and terms are projected in the space of the concepts\n",
    "reduced_matrix = np.dot(np.dot(U_k, S_k), VT_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35b6a8-e994-4f40-8e86-a06a07867437",
   "metadata": {},
   "source": [
    "L’objet `reduced_matrix` est maintenant une version approximée de la matrice originale. De la même manière, nous pouvons d’un côté projeter les documents dans l’espace des concepts et, de l’autre côté, les termes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b101ae0-62f2-48ad-8a87-a03acf651544",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = np.dot(U_k, S_k)\n",
    "term_matrix = np.dot(S_k, VT_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d12b6e-8745-44d8-8be2-ad4e25fea52d",
   "metadata": {},
   "source": [
    "Ces deux projections nous servent à identifier visuellement des regroupements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ddd31-d6df-489c-9bf8-f3f60ac88052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and subgraphs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "fig.suptitle(\"Projection dans l’espace des concepts\", fontsize=16)\n",
    "\n",
    "# documents projected in the concept space\n",
    "axes[0].scatter(document_matrix[:, 0], document_matrix[:, 1], color='blue')\n",
    "axes[0].set_title(\"Documents\")\n",
    "axes[0].set_xlabel(\"Concept 1\")\n",
    "axes[0].set_ylabel(\"Concept 2\")\n",
    "\n",
    "# terms projected in the concept space\n",
    "axes[1].scatter(term_matrix[0, :], term_matrix[1, :], color='red')\n",
    "axes[1].set_title(\"Termes\")\n",
    "axes[1].set_xlabel(\"Concept 1\")\n",
    "axes[1].set_ylabel(\"Concept 2\")\n",
    "\n",
    "# annotations\n",
    "for i, (x, y) in enumerate(zip(document_matrix[:, 0], document_matrix[:, 1])):\n",
    "    axes[0].annotate(i, (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "for i, (x, y) in enumerate(zip(term_matrix[0, :], term_matrix[1, :])):\n",
    "    axes[1].annotate(vocabulary[i], (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05390e04-916a-4135-992a-0a3d25c79bf0",
   "metadata": {},
   "source": [
    "## Vers d’autres analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb208fee-0b1c-4263-af80-2cd5d7b04fb4",
   "metadata": {},
   "source": [
    "La LSA n’est qu’une étape, certes révélatrice, vers d’autres types d’analyses. En dévoilant une relation sémantique jusque-là invisible entre les documents et les termes d’un corpus nietzschéen réduit, elle nous permet désormais d’effectuer des calculs de distance ou de similarité, d’appliquer des méthodes de partitionnement pour les classer dans des catégories ou pour simplement les regrouper dans des ensembles homogènes.\n",
    "\n",
    "Dans le contexte d’une recherche d’information, nous devrions d’abord vectoriser la requête d’un utilisateur en attribuant aux mots présents dans le vocabulaire la valeur de leur IDF. Si un mot apparaît plusieurs fois dans la requête, on peut multiplier son poids afin de refléter son importance. Une fois le vecteur de requête reconstitué, il doit ensuite être projeté dans l’espace des concepts à l’aide de la matrice $VT_k$. Il restera enfin à calculer la similarité cosinus entre ce vecteur projeté et les projections des documents ($U_k$), puis à analyser les résultats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

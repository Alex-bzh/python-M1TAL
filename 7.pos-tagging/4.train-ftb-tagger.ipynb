{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entraîner un étiqueteur grammatical basé sur le *French Treebank*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Un corpus arboré pour le français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le [*French Treebank*](http://ftb.linguist.univ-paris-diderot.fr/), développé à l’Université de Paris depuis 1997, est un corpus annoté (annotations lexicales et syntaxiques) à partir des articles du journal *Le Monde* pour la période 1990-1993. Plus-value indéniable : toutes les annotations ont été validées à la main.\n",
    "\n",
    "Il totalise, dans sa version 1.0, 21 550 phrases pour 664 500 tokens.\n",
    "\n",
    "Un extrait de 2072 phrases (pour 58 527 tokens), dans une version appauvrie au format *word/tag*, est disponible dans le dossier *./ftb*.\n",
    "\n",
    "Pour charger les quatre fichiers de cet extrait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "\n",
    "reader = TaggedCorpusReader('./data/ftb', r'.*\\.pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entraîner un étiqueteur pour unigrammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi outillé par NLTK, le corpus est facilement exploitable pour entraîner un étiqueteur pour unigrammes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "train = reader.tagged_sents()\n",
    "tagger = UnigramTagger(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Considérons trois phrases que nous souhaitons étiqueter avec ce modèle entraîné sur le FTB. La première phrase est issue de ce modèle, la seconde est extraite de la version complète du FTB mais ne figure pas dans le modèle, et quant à la troisième, elle est purement fictive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sents = [\n",
    "    ['La', 'tâche', 'des', 'secouristes', 'est', 'immense', ',', 'faute', 'de', 'moyens', 'matériels', 'et', 'humains', '.'],\n",
    "    ['Elle', 'ne', 'sera', 'vaincue', 'que', 'grâce', 'à', 'une', 'alliance', 'franco', '-', 'allemande', 'âprement', 'négociée', '.'],\n",
    "    ['Julien', 'est', 'tombé', 'lourdement', 'de', 'sa', 'chaise', '.']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nous connaissons l’étiquetage grammatical des deux premières et soumettons celui de la troisième :\n",
    "```txt\n",
    "(1) La/D tâche/N des/P+D secouristes/N est/V immense/A ,/PONCT faute/N de/P moyens/N matériels/A et/C humains/A ./PONCT\n",
    "(2) Elle/CL ne/ADV sera/V vaincue/V que/ADV grâce/N à/P une/D alliance/N franco/A -/PONCT allemande/A âprement/ADV négociée/V ./PONCT\n",
    "(3) Julien/N est/V tombé/V lourdement/ADV de/P sa/D chaise/N ./PONCT\n",
    "```\n",
    "\n",
    "Pour lancer l’étiquetage automatique, l’interface `UnigramTagger` fournit deux méthodes :\n",
    "- `.tag()` pour une seule phrase ;\n",
    "- `.tag_sents()` pour une liste de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sents_tagged = tagger.tag_sents(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparons le résultat de l’étiquetage automatique pour la première phrase avec la solution connue :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[ tup for tup in sents_tagged[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Aucune erreur, le résultat est pleinement satisfaisant. Ce n’est pas une surprise, la phrase étiquetée fait partie du modèle.\n",
    "\n",
    "Pour la seconde phrase, issue du même corpus bien qu’en dehors du modèle, le traitement lève des imprécisions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[ tup for tup in sents_tagged[1] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Les différences à noter sont :\n",
    "- *vaincue* : `None` au lieu de `V`\n",
    "- *que* : `C` au lieu de `ADV`\n",
    "- *âprement* : `None` au lieu de `ADV`\n",
    "\n",
    "Sur 15 tokens, 3 différences, soit un taux d’erreur de 20 % !\n",
    "\n",
    "Pour la dernière phrase, le taux d’erreur (37,5 %) est encore plus prononcé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[ tup for tup in sents_tagged[2] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Rappel :** plus le corpus d’entraînement est large, meilleur sera l’étiquetage. Ici, l’extrait se contente de 1/10e du FTB. Avec la totalité du corpus, il est fort probable que la plupart des erreurs disparaîtraient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Évaluer la performance d’un étiqueteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d’évaluer rapidement la justesse de l’étiqueteur, NLTK met à disposition une méthode qui nécessite de diviser les données en deux jeux :\n",
    "- un jeu pour l’entraînement de l’étiqueteur ;\n",
    "- un jeu pour le tester.\n",
    "\n",
    "Un taux de 80/20 est adapté pour effectuer cette évaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nb_sents = len(reader.tagged_sents())\n",
    "limit = int(nb_sents * 0.2)\n",
    "\n",
    "# split\n",
    "train_data = reader.tagged_sents()[limit:]\n",
    "test_data = reader.tagged_sents()[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Il ne reste plus qu’à entraîner l’étiqueteur puis à l’évaluer avec les données de test. La métrique utilisée ci-dessous est l’exactitude :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tagger = UnigramTagger(train_data)\n",
    "tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Face à une ambiguïté (*que* : `C` ou `ADV` ?), un étiqueteur décide en fonction du contexte grâce à un score de fréquence d’occurrences. Le paramètre `cutoff` fixe un seuil minimal avant d’attribuer une étiquette, au prix d’une diminution des performances globales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tagger = UnigramTagger(train_data, cutoff=3)\n",
    "tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un taux d’exactitude de 82 % pour un jeu de données si réduit se révèle être un bon score, même s’il n’est pas suffisant. À titre de comparaison avec un extrait de même volume du corpus *Brown*, le taux d’exactitude est seulement de 77 % :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "subcorpus = brown.tagged_sents()[:2071]\n",
    "limit = int(len(subcorpus) * 0.2)\n",
    "\n",
    "train_data = subcorpus[limit:]\n",
    "test_data = subcorpus[:limit]\n",
    "\n",
    "tagger = UnigramTagger(train_data)\n",
    "tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "D’autres mesures quantitatives (*precision*, *recall* et *f1-score*) apporteraient plus de finesse à l’évaluation d’un étiqueteur. Elles seront abordées dans un autre chapitre.\n",
    "\n",
    "**Attention :** le test de performance ne juge pas la qualité de l’annotation produite mais celle de l’étiqueteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Améliorer la performance d’un étiqueteur\n",
    "\n",
    "La première piste pour améliorer la performance d’un étiqueteur est de décider, lorsqu’il est face à un contexte inconnu, d’une étiquette par défaut. C’est ce que permet la classe `DefaultTagger` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "default = DefaultTagger('N')\n",
    "tagger = UnigramTagger(train_data, backoff=default)\n",
    "tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une amélioration nette de presque 8 % avec cette simple astuce !\n",
    "\n",
    "La seconde piste, qui découle de la première, consiste à entraîner puis à combiner des étiqueteurs pour *n*-grammes. NLTK fournit les classes `BigramTagger` et `TrigramTagger`.\n",
    "\n",
    "Isolément, leurs performances sont très mauvaises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import BigramTagger\n",
    "\n",
    "bigram_tagger = BigramTagger(train_data)\n",
    "bigram_tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "trigram_tagger = TrigramTagger(train_data)\n",
    "trigram_tagger.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La solution consiste à entraîner chaque étiqueteur avec le précédent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bigram_tagger = BigramTagger(train_data, backoff=tagger)\n",
    "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Le résultat des évaluations montre que l’étiqueteur 2-grammes est le plus performant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bigram_accuracy = bigram_tagger.accuracy(test_data)\n",
    "trigram_accuracy = trigram_tagger.accuracy(test_data)\n",
    "\n",
    "print(\n",
    "    f\"Évaluation de l’étiqueteur 2-grammes : { bigram_accuracy }\",\n",
    "    f\"Évaluation de l’étiqueteur 3-grammes : { trigram_accuracy }\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gérer la sérialisation d’un étiqueteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par *sérialisation*, on entend une opération de codage de l’étiqueteur sous une forme allégée, à fin notamment de sauvegarde.\n",
    "\n",
    "Le module `pickle` de Python permet ainsi de sauvegarder la configuration d’un étiqueteur entraîné pour la charger plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sérialiser un objet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer le module `pickle` puis appeler la méthode `.dump()` sur une ressource de fichier en veillant à ce qu’elle soit disponible en écriture en mode binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/ftb/tagger.pickle', 'wb') as dest:\n",
    "    pickle.dump(bigram_tagger, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Charger un objet sérialisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer le module `pickle` puis appeler la méthode `.load()` sur une ressource de fichier en veillant à ce qu’elle soit disponible en lecture en mode binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/ftb/tagger.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Une fois l’étiqueteur chargé, on peut l’utiliser normalement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sents_tagged = tagger.tag_sents(sents)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

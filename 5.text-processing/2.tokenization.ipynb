{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation est le processus de segmentation d’un texte en unités (tokens) significatives (phrases, mots, symboles…). Elle constitue souvent la première étape avant toute analyse de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importer NLTK\n",
    "2. Installer le module `punkt` (si besoin)\n",
    "3. Charger le tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "import nltk\n",
    "\n",
    "# 2 if needed\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# 3\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation en phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode `.sent_tokenize()` pour segmenter un texte en phrases.\n",
    "\n",
    "Exemple avec deux extraits issus pour le premier des *Adventures of Buster Bear* (Burgess, 1920) et pour le second du *Cid* (Corneille, 1637) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "en = \"Little Joe sat down on the bank and prepared to enjoy his breakfast. He hadn't seen Buster Bear, and he didn't know that he or any one else was anywhere near.\"\n",
    "fr = \"Elvire, m’as-tu fait un rapport bien sincère ? Ne déguises-tu rien de ce qu’a dit mon père ?\"\n",
    "\n",
    "print(\n",
    "    sent_tokenize(en),\n",
    "    sent_tokenize(fr),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pour plus d’efficacité avec des textes longs, charger directement le fichier *pickle* (outil pour sérialiser un objet en Python), et sélectionner celui qui convient à la langue de travail :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/french.pickle')\n",
    "\n",
    "tokenizer.tokenize(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode `.word_tokenize()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(en))\n",
    "\n",
    "print(word_tokenize(fr, language='french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Remarques :**\n",
    "- ponctuation conservée\n",
    "- contractions séparées (en anglais)\n",
    "- segmentation selon les espaces et la ponctuation.\n",
    "\n",
    "Méthode `word_tokenize()` fait par défaut appel à une classe `TreebankWordTokenizer`.\n",
    "\n",
    "D’autres *tokenizers* existent :\n",
    "- `WhitespaceTokenizer`\n",
    "- `SpaceTokenizer`\n",
    "- `WordPunctTokenizer`\n",
    "\n",
    "Comparons les différences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, SpaceTokenizer, WordPunctTokenizer\n",
    "\n",
    "sentence = \"He hadn't seen Buster Bear, and he didn't know that he or any one else was anywhere near.\"\n",
    "\n",
    "tokenizers = [\n",
    "    WhitespaceTokenizer(),\n",
    "    SpaceTokenizer(),\n",
    "    WordPunctTokenizer()\n",
    "]\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Les contractions et la ponctuation sont gérées différemment :\n",
    "\n",
    "| Tokenizer    | Contraction      | Ponctuation |\n",
    "|--------------|:----------------:|:------------|\n",
    "| TreebankWord | *had*, *n't*     | excluse     |\n",
    "| Whitespace   | *hadn't*         | incluse     |\n",
    "| Space        | *hadn't*         | incluse     |\n",
    "| WordPunct    | *hadn*, *'*, *t* | excluse     |\n",
    "\n",
    "Une autre classe de tokenizer permet de personnaliser son modèle de tokenisation : `RegexpTokenizer`. Elle est par exemple très utile pour supprimer toute ponctuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "tokenizer.tokenize(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Et pour inclure les contractions de l’anglais ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "tokenizer.tokenize(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Par défaut, `RegexpTokenizer` travaille sur les tokens, mais on peut lui demander de travailler plutôt sur les espaces :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
    "\n",
    "tokenizer.tokenize(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une autre opération courante consiste à ne conserver que les mots signifiants d’un texte.\n",
    "\n",
    "NLTK fournit une liste de mots vides pour plusieurs langues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample of stopwords in French\n",
    "stopwords.words('french')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Et pour filtrer grâce à cette liste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "fr_stopwords = stopwords.words('french')\n",
    "words = tokenizer.tokenize(fr)\n",
    "\n",
    "[\n",
    "    word\n",
    "    for word in words\n",
    "    if word not in fr_stopwords\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Le module `string`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie standard de Python définit [neuf constantes](https://docs.python.org/3/library/string.html) qui entrent dans des opérations très communes liées aux chaînes de caractères.\n",
    "\n",
    "Ces constantes sont accessibles via le module `string` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Citons trois de ces constantes :\n",
    "- `string.ascii_letters` : alphabet latin non accentué ;\n",
    "- `string.digits` : dix signes numériques (0-9) ;\n",
    "- `string.punctuation` : signes de ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ces constantes peuvent par exemple servir à filtrer une liste de tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# list of tokens minus punctuation\n",
    "[\n",
    "    w\n",
    "    for w in word_tokenize(en)\n",
    "    if w not in string.punctuation\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il est tentant de se fier à cette manipulation aussi pour le français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    w\n",
    "    for w in word_tokenize(fr)\n",
    "    if w not in string.punctuation\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Attention !** La constante `punctuation` ne connaît pas toutes les variantes de guillemets du français !"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

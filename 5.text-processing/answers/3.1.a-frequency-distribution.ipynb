{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La distribution statistique des mots d’un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout au long de cet exercice dirigé, nous apprendrons à préparer nos données en vue de l’affichage d’une courbe de distribution des fréquences des mots d’un texte, d’abord en anglais, puis en français.\n",
    "\n",
    "Pour définir proprement notre objectif, nous allons construire un tableau de fréquences univarié dans lequel le mot le plus fréquent apparaîtra au rang 1, le second mot le plus fréquent au rang 2, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un exemple factice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons pour exemple les dix moyens de transport les plus cités dans une enquête factice sur la mobilité des Parisiens :\n",
    "\n",
    "|Rang|Mot|Fréquence\n",
    "|:-:|:-|:-:|\n",
    "|1|ascenseur|309|\n",
    "|2|avion|291|\n",
    "|3|métro|289|\n",
    "|4|RER|240|\n",
    "|…|…|…|\n",
    "|10|bateau|106|\n",
    "\n",
    "La courbe de distribution statistique de ce tableau de fréquences nous donne la représentation suivante :\n",
    "\n",
    "![Distribution statistique des mots d’un texte](../images/frequency-distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les textes utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre exercice, nous utiliserons tout d’abord le roman *Moby Dick* de Herman Melville puis, dans un second temps, *Salammbô* de Gustave Flaubert. Le premier est disponible dans le corpus *Gutenberg* inclus avec NLTK et le second dans le répertoire `data`.\n",
    "\n",
    "Ces deux textes ne présentent pas tout à fait les mêmes enjeux. Il conviendra notamment d’adapter les techniques de tokenisation afin de récupérer des données propres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick_id = 'melville-moby_dick.txt'\n",
    "path_to_salammbo = '../data/salammbo.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De l’analyse de l’énoncé, nous définissons les étapes successives de notre algorithme :\n",
    "1. découper un texte en mots ;\n",
    "2. filtrer les mots ;\n",
    "3. compter le nombre d’occurrences de chaque mot ;\n",
    "4. ordonner les résultats selon la fréquence ;\n",
    "5. afficher une courbe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer une liste de mots de *Moby Dick*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape consiste à importer le corpus *Gutenberg* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, grâce à la méthode `.words()` appliquée au texte, il est facile de récupérer une liste des mots du corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = gutenberg.words(moby_dick_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrer les mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la seconde étape de notre programme, nous souhaitons filtrer la liste de mots pour ne conserver que les résultats qui nous semblent pertinents. Pour cela, nous allons supprimer les mots vides de l’anglais (*stopwords*) ainsi que les signes de ponctuation.\n",
    "\n",
    "Chargez en mémoire les modules nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons tout d’abord une liste des objets de notre filtre (association des mots vides et des signes de ponctuation) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "filtered = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la liste des objets de notre filtre est en minuscules, il convient de passer toute la liste des mots de *Moby Dick* en bas de casse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = [ w.lower() for w in words ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et seulement après nous pouvons passer la liste des mots au tamis de notre filtre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = filter(lambda w:w not in filtered, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un peu de tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetons un œil aux quinze premiers mots de la liste filtrée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "filtered_words = list(words)\n",
    "filtered_words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxs résultats non pertinents apparaissent :\n",
    "- une date (*1851*) ;\n",
    "- un signe de ponctuation double (*--*).\n",
    "\n",
    "L’une des solutions serait de parcourir à nouveau cette liste et de comparer chaque lettre avec une sélection des caractères autorisés pour ne conserver que les mots pertinents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = list()\n",
    "\n",
    "for word in filtered_words:\n",
    "\n",
    "    # Flag raised? Not a suitable word\n",
    "    flag = False\n",
    "\n",
    "    for c in word:\n",
    "        if c not in string.ascii_lowercase:\n",
    "            flag = True\n",
    "\n",
    "    if not flag:\n",
    "        new_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une meilleure réponse serait d’agir directement lors du découpage du texte en mots, en proposant une tokenisation adaptée.\n",
    "\n",
    "Importons le module `RegexpTokenizer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramétrons à présent un tokenisateur basé sur une règle conforme à l’anglais (`[\\w']+`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profitons à présent de la méthode `.raw()` embarquée avec le module `gutenberg` pour transmettre le texte au tokenisateur et récupérer une liste propre de mots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "text = gutenberg.raw(moby_dick_id)\n",
    "words = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette nouvelle liste étant déjà débarrassée des signes de ponctuation (le module `string` n’est plus nécessaire), il ne reste plus qu’à la transformer en bas de casse et à la passer au tamis des mots vides de l’anglais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = [ w.lower() for w in words ]\n",
    "filtered_words = list(filter(lambda w:w not in stopwords.words('english'), words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est à noter que lors de la configuration de notre tokenisateur, nous avons pris la décision de conserver un signe de ponctuation unique, l’apostrophe (*'*). Nous pouvons inclure ce cas dans notre filtre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "filtered_words = list(filter(lambda w:w not in stopwords.words('english') + [\"'\"], words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compter les occurrences des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous disposons d’une liste propre des mots du roman *Moby Dick*, il nous reste à compter le nombre de fois où chacun apparaît puis à produire un dictionnaire trié de ces occurrences. La classe `FreqDist` du module `nltk.probability` nous permet de réaliser cette opération en une seule fois.\n",
    "\n",
    "Chargeons en mémoire tous ces modules :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import nltk\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il reste à créer une nouvelle instance de la classe `FreqDist` à partir de la liste des mots du texts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "distribution = nltk.FreqDist(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons un aperçu des données avec la méthode `.most_common()` afin de s’assurer que cette liste est triée par ordre de fréquence d’apparition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "distribution.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher un diagramme de la distribution statistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `FreqDist` a cet avantage d’embarquer une méthode pour afficher rapidement un diagramme à partir de la librairie `matplotlib`.\n",
    "\n",
    "Utilisez la méthode `.plot()` afin d’afficher les trente mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "%config InlineBackend.figure_format='retina'\n",
    "_ = distribution.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une courbe de la distribution des fréquences dans le roman *Salammbô*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le français, la seule modification réside dans la manière de découper le texte en mots. En effet, nous ne conserverons pas les apostrophes dans la liste finale.\n",
    "\n",
    "Répétez les opérations pour afficher les trente mots les plus fréquents du roman de Gustave Flaubert :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# modules to import\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# content of Salammbô\n",
    "with open(path_to_salammbo) as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "# a list of tokens in Salammbô in lowercase\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens = [ t.lower() for t in tokens ]\n",
    "\n",
    "# a filtered list of words\n",
    "words = list(filter(lambda t:t not in stopwords.words('french'), tokens))\n",
    "\n",
    "# frequency distribution\n",
    "distribution = nltk.FreqDist(words)\n",
    "\n",
    "# plot the distribution\n",
    "%config InlineBackend.figure_format='retina'\n",
    "_ = distribution.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Améliorer les diagrammes avec `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le diagramme affiché par la méthode `.plot()` de la classe `FreqDist` ne permet pas de personnaliser les étiquettes des axes x et y. Tout juste permet-elle de personnaliser le titre.\n",
    "\n",
    "Afin de gagner en qualité, nous pouvons recourir à `matplotlib`, une librairie très largement utilisée dans le monde scientifique pour afficher des diagrammes de qualité.\n",
    "\n",
    "**Remarque :** Nous partons du principe que la variable `distribution` est un objet de type `FreqDist`.\n",
    "\n",
    "**Étape 1 :** installer la librairie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étape 2 :** récupérer les trente mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = distribution.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étape 3 :** en extraire une liste des mots et une liste des fréquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(dict(most_frequent).keys())\n",
    "frequencies = list(dict(most_frequent).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étape 4 :** paramétrer le diagramme pour affichage, avec toutes les options jugées nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display on retina graphics\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# size of the figure\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "# title\n",
    "plt.title('Distribution statistique des mots dans *Salammbô* de Flaubert', fontsize=18, fontweight='bold')\n",
    "\n",
    "# label of the x axis\n",
    "plt.xlabel('Rang du mot', fontsize=14, fontweight='bold')\n",
    "\n",
    "# label of the y axis\n",
    "plt.ylabel('Nb d’occurrences', fontsize=14, fontweight='bold')\n",
    "\n",
    "# print the words instead of their rank\n",
    "plt.xticks(range(30), words, rotation=60, ha='right')\n",
    "\n",
    "# print the grid\n",
    "plt.grid(True)\n",
    "\n",
    "# plot the frequencies\n",
    "plt.plot(frequencies)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480e1ba-edcc-4dfd-bc9e-547e795be9f4",
   "metadata": {},
   "source": [
    "# Les méthodes d’évaluation d’un modèle prédictif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a87e26-5f2c-4b9b-ac75-0b45258184b1",
   "metadata": {},
   "source": [
    "Après des heures à paramétrer au mieux un modèle d’apprentissage avec la certitude d’avoir écarté les biais qui pourraient orienter les résultats – rappelons qu’un mauvais modèle peut fournir de très mauvais résultats avec une précision étonnante –, les premières prédictions sortent de la machine et nous souhaitons évaluer leur qualité afin de le passer en production ou non.\n",
    "\n",
    "Bien entendu, le cas présenté plus haut ne vaut que pour sa généralité ; dans la pratique, les méthodes d’évaluation sont présentes à chaque étape de la programmation d’un modèle si bien que presque aucun choix ne devrait être pris sans validation par une métrique ou une autre. Comme nous nous sommes concentrés sur deux types d’algorithmes, nous n’aborderons que les méthodes d’évaluation pour les tâches de régression et de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1627dc-dd09-4814-b418-4b83cc091234",
   "metadata": {},
   "source": [
    "## Mesurer une erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1dab6e-842f-4271-99fd-b86703427985",
   "metadata": {},
   "source": [
    "Par métrique, on entend une façon d’évaluer la qualité d’une prédiction par mesure de la distance entre la réalité observée et la valeur calculée par un algorithme. Si l’on souhaite par exemple prédire la note d’un élève au prochain examen de français en se basant uniquement sur sa moyenne dans la matière – disons 12 –, l’algorithme de prédiction vaudra simplement :\n",
    "\n",
    "$$\\hat{y} = \\mu$$\n",
    "\n",
    "L’élève obtient finalement une note de 11. Pour mesurer l’erreur de la prédiction, il suffit de soustraire $\\hat{y}$ de $y$, soit un résultat de $-1$. Remarquons que si sa note avait été de 13, le résultat aurait été positif : $13 - 12 = 1$. Or, $-1$  et $+1$ étant situés à égales distances de la prédiction, ils représentent la même réalité géométrique. Dans les deux cas, l’erreur est réputée être de $1$. On utilise donc plutôt une formule impliquant les valeurs absolues :\n",
    "\n",
    "$$e = \\lvert y - \\hat{y}\\rvert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b623b68-2633-4669-a07f-7b11e681c8ea",
   "metadata": {},
   "source": [
    "## Métriques pour les tâches de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2a6dd-4409-4f72-9c92-48a8eac2a6da",
   "metadata": {},
   "source": [
    "Dans l’exemple de l’introduction, il s’agissait simplement de calculer l’erreur pour un couple unique prédiction/résultat. Qu’en serait-il si nous avions une série de prédictions et une série de résultats ? Plutôt que de calculer indépendamment les erreurs de chaque prédiction, nous préférerions obtenir une mesure de l’ensemble.\n",
    "\n",
    "Et pour corser le tout, il existe plusieurs métriques qui ne répondent pas tout à fait aux mêmes enjeux. Choisir la plus adaptée à la situation peut ainsi devenir une nécessité pour ajuster plus finement encore le modèle.\n",
    "\n",
    "Prenons le cas fictif de la pluviométrie au-dessus de la commune de Pont-Aven avec d’un côté les précipitations mesurées en millimètres pour les mois de janvier à mai 2022 et, de l’autre, des prédictions imaginaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17af8d-4bf5-4056-a2fd-cef33b2370b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# series\n",
    "series = {\n",
    "    \"months\": [\"Jan\", \"Feb\", \"March\", \"April\", \"May\"],\n",
    "    \"rainfall\": [70, 65, 55, 50, 9],\n",
    "    \"predictions\": [35, 60, 75, 45, 20]\n",
    "}\n",
    "# dataframe\n",
    "df = pd.DataFrame(series)\n",
    "\n",
    "# column 'months' still an id var, while two others are registered in a col 'Measure'\n",
    "df2 = pd.melt(df, id_vars=\"months\", var_name=\"Measure\", value_name=\"mm\")\n",
    "\n",
    "# graph\n",
    "_ = sns.lineplot(data=df2, x=\"months\", y=\"mm\", hue=\"Measure\", marker=\"o\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703dd24-a947-4099-b392-d9e05f95d9e2",
   "metadata": {},
   "source": [
    "### Le coefficient de détermination linéaire de Pearson ($R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d95eff-6b04-4457-9766-0ebb8da80d76",
   "metadata": {},
   "source": [
    "Le $R^2$ est un score qui mesure de la qualité de la prédiction d’un modèle de régression linéaire en évaluant la variance d’une variable par rapport à une autre variable. Il est défini par la relation suivante pour un résultat généralement compris dans l’intervalle $[0,1]$ :\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^k(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^k(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Son analyse est très intuitive mais elle implique deux critères :\n",
    "- le modèle est linéaire ;\n",
    "- une seule variable explicative est concernée.\n",
    "\n",
    "Un $R^2$ de 1.0 est un score parfait quand un score de 0.0 indiquerait que le modèle prédit toujours la valeur attendue (la moyenne). Un score négatif reste possible mais serait révélateur d’une erreur de méthodologie (données arbitrairement mauvaises).\n",
    "\n",
    "Dans le cas de notre exemple, la prédiction n’est clairement pas linéaire, aussi le calcul du $R^2$ ne devrait pas servir pour l’évaluation de notre modèle. À titre d’exercice, voyons ce qu’il donne en invoquant la fonction `r2_score()` du module `metrics` de *Scikit-learn* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f555455-f42a-4820-97f6-e47ae4877d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(df.rainfall, df.predictions)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f038b-87db-45ba-a4db-b495d500856b",
   "metadata": {},
   "source": [
    "### L’erreur quadratique moyenne (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f7b56-3f10-4154-8dbd-a010cf5f7143",
   "metadata": {},
   "source": [
    "La MSE (*mean square error*) et sa cousine, la RMSE (*root mean square error*), sont les deux métriques les plus couramment utilisées en *machine learning*. La MSE calcule la moyenne des carrés des erreurs selon la formule :\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{k}\\sum_{i=0}^{k-1}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Comme entre en jeu un calcul au carré, la MSE pénalise plus fortement les grandes erreurs et, dans le même ordre d’idée, sera très sensible aux données aberrantes (*outliers*). La fonction dans *Scikit-learn* est `mean_squared_error()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89b185-ef58-42c7-ae8d-35fbe2399eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(df.rainfall, df.predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59c71a-d7df-4957-8864-55944ab2bbe0",
   "metadata": {},
   "source": [
    "### La racine de l’erreur quadratique moyenne (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ce050-8012-49eb-882c-c9a6edc7b5c9",
   "metadata": {},
   "source": [
    "Plus facile à interpréter que la MSE, la RMSE (*root mean square error*) s’exprime dans l’unité de la variable à prédire en extrayant la racine carrée de la MSE :\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{ \\frac{1}{k}\\sum_{i=0}^{k-1}(y_i - \\hat{y}_i)^2 }$$\n",
    "\n",
    "À noter qu’elle souffre des mêmes limites que la MSE : une grande sensibilité aux *outliers* ainsi qu’une incidence forte sur les erreurs importantes. Pour la calculer avec *Scikit-learn*, il suffit de prendre la racine carrée de la MSE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0104c8-d3e5-4512-b37d-66f62a21163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power of 0.5 = square root\n",
    "rmse = mse ** 0.5\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083a903-fe8e-4ac6-a481-91e499837d39",
   "metadata": {},
   "source": [
    "### L’erreur absolue moyenne (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc387dfe-36f0-4ee6-b5bc-b3be99f4eba7",
   "metadata": {},
   "source": [
    "Quand les valeurs extrêmes d’un jeu de données sont quantitativement importantes, la RMSE pourrait conduire à des erreurs d’interprétation. Dans un tel cas de figure, la MAE (*mean absolute error*) peut lui être préférée : en calculant la moyenne de valeurs absolues, elle ne pénalise plus autant les grandes erreurs et se rend moins sensible aux données aberrantes. La formule de la MAE vaut ainsi :\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{k}\\sum_{i=0}^{k-1} \\lvert y_i - \\hat{y}_i\\rvert$$\n",
    "\n",
    "Dans *Scikit-learn*, la fonction `mean_absolute_error()` se charge du calcul :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b0a8c-b1d0-4201-bceb-60319204c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(df.rainfall, df.predictions)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c8134-16c7-4a70-9ede-b37cc0d48d56",
   "metadata": {},
   "source": [
    "## Métriques pour les tâches de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd51d7e-e304-486d-ab12-33360a0fe851",
   "metadata": {},
   "source": [
    "Dans la grande famille des tâches de classification, nous reconnaissons trois catégories :\n",
    "- **la classification binaire** (l’observation appartient-elle à la classe cible ou non ?) ;\n",
    "- **la classification multi-classes** (parmi toutes, à quelle classe l’observation appartient-elle ?) ;\n",
    "- **la classification multi-étiquettes** (l’observation appartient-elle à plusieurs classes ?).\n",
    "\n",
    "Là encore, selon la nature de la tâche, nous ne choisirons pas forcément la même métrique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa52751-d394-47a2-b16a-2117d88035ec",
   "metadata": {},
   "source": [
    "### Mesures générales de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07799021-aca8-4588-b4cd-7010b1e69f5f",
   "metadata": {},
   "source": [
    "Évaluer les performances d’un classificateur étant plus complexe que pour les tâches de régression, il est nécessaire d’aborder en premier lieu certaines généralités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecceb17-2c23-41ee-8754-1d8dbb771b0b",
   "metadata": {},
   "source": [
    "#### L’exactitude (*accuracy*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b40bd-f0a4-4e93-9578-a57c33836d26",
   "metadata": {},
   "source": [
    "La toute première est de ne jamais oublier qu’un très mauvais classificateur peut obtenir des résultats stupéfiants. Prenons l’exemple d’un jeu de données factice qui comporte cent observations étiquettées selon deux modalités : chat ou pas chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d63299-6307-4839-a654-bceceac127a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# 5 out of an hundred are cats\n",
    "dataset = [\n",
    "    \"cat\" if i < 5 else \"non-cat\"\n",
    "    for i in range(100)\n",
    "]\n",
    "# every day I'm shufflin\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d717ab-edf6-4e56-b729-83e6721b7ac5",
   "metadata": {},
   "source": [
    "Générons une autre liste pour les prédictions avec la seule étiquette *non-cat* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18d07-fba3-4ba3-b726-113560a3c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [ \"non-cat\" for i in range(100) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e79d89-67df-46df-a6a3-f91b1357c4ee",
   "metadata": {},
   "source": [
    "Et mesurons la performance de notre algorithme prédictif grâce au score d’exactitude (*accuracy*) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2588bb-7ae8-4389-ab30-4035f969005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(dataset, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed7767-033d-46ca-b7f1-2aa06a54bde7",
   "metadata": {},
   "source": [
    "**95 % !** Un taux d’exactitude à faire pâlir les diseuses de bonne aventure, non ? Pour cette raison, on ne se satisfera jamais du score d’exactitude, quitte même à s’en méfier dès que les jeux de données sont asymétriques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebdb8d-d01e-4e01-acf7-e16cfa98b8a4",
   "metadata": {},
   "source": [
    "#### Une matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080733f-4bcb-47a2-a1dc-b354a84b6962",
   "metadata": {},
   "source": [
    "La matrice de confusion repose sur un principe simple : compter le nombre de fois où les observations ont été bien ou mal étiquetées. Elle révèle ainsi quatre informations essentielles :\n",
    "- **les vrais positifs** (*true positive*), le classificateur a repéré qu’il s’agissait d’un chat ;\n",
    "- **les vrais négatifs** (*true negative*), le classificateur a repéré qu’il ne s’agissait pas d’un chat ;\n",
    "- **les faux positifs** (*false positive*), le classificateur a cru qu’il s’agissait d’un chat ;\n",
    "- **les faux négatifs** (*false negative*), le classificateur aurait dû voir qu’il s’agissait d’un chat.\n",
    "\n",
    "Préparons un jeu de données aléatoire avec la fonction `make_classification()` du module `sklearn.datasets` et effectuons des prédictions à partir d’un modèle de classification naïve bayésienne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9a4cf-8b14-4789-bb6a-061b459404f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# make dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "# make train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "# model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314b3ce-d27c-437b-8d75-68133df117a3",
   "metadata": {},
   "source": [
    "La matrice de confusion s’obtient avec la fonction `confusion_matrix()` du module `sklearn.metrics` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b262bba-de32-4549-b2be-39668fb6a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44becc-7d76-47bb-ad4b-7f2cbbec5a5b",
   "metadata": {},
   "source": [
    "Chaque ligne correspond à une classe réelle et chaque colonne à une classe prédite avec, sur la première ligne, la classe négative et, sur la seconde, la classe positive, tel que dans le tableau suivant :\n",
    "\n",
    "|prédites/réelles|Classe négative|Classe positive|\n",
    "|-|:-:|:-:|\n",
    "|Classe négative|TN (85)|FP (9)|\n",
    "|Classe positive|FN (3)|TP (103)|\n",
    "\n",
    "Une méthode `.ravel()` permet de récupérer chacun de ces indicateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddfdcf-b7ef-4ce1-88b5-b2e774a36792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c0098-a1b0-4d75-badc-38ab598805d5",
   "metadata": {},
   "source": [
    "Notons également l’existence d’une classe `ConfusionMatrixDisplay` pour afficher la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbe01a-5242-4857-8164-954d3a739fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels=model.classes_)\n",
    "_ = display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce4532-1644-42e9-b9c2-14f63c7b5823",
   "metadata": {},
   "source": [
    "En guise de conclusion, signalons que cette matrice de confusion détermine plusieurs scores : la précision, la sensibilité (ou rappel) et le score $F_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608cdd3-4ab0-485a-bfed-24987699aa9d",
   "metadata": {},
   "source": [
    "### Classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6461376-be1f-4428-999f-577fb70123d9",
   "metadata": {},
   "source": [
    "### Classification multi-classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

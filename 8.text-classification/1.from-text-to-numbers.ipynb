{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb0ca4d-4c09-4e2d-97aa-c16b4a0b8fba",
   "metadata": {},
   "source": [
    "# La vectorisation, ou comment aider les machines à nous comprendre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b661e9c-ca3b-4048-95f5-4d467f5c013f",
   "metadata": {},
   "source": [
    "Le changement de paradigme s’est opéré sans le voir, dans un certain enthousiasme pour le progrès technologique : alors que nous avions créé les machines pour nous alléger la tâche, nous voilà maintenant à les servir dans le même but.\n",
    "\n",
    "Avec le développement du *machine learning* et ses prouesses que l’on vante quotidiennement dans la presse, les réseaux autorisés ou encore dans les cercles d’amis, la notion d’entraînement supervisé ou non-supervisé nous est devenue familière : pour qu’un algorithme d’apprentissage fonctionne bien, il faut le nourrir avec une quantité proverbiale de données.\n",
    "\n",
    "Comment, alors, procéder ? Pour qu’une machine comprenne le sens d’un texte, suffit-il de lui dire qu’elle peut trouver ici un verbe conjugué, là le sujet, et que tout le reste sert à définir des circonstances ? Comment parvenir à lui dire que *un chat* et *un miaou* désignent la même réalité, et en même temps pas tout à fait ?\n",
    "\n",
    "Le procédé repose sur la nécessité de présenter à la machine un objet sous forme numérique. L’idée n’est pas nouvelle. Déjà au IVe siècle avant J.-C, l’inscription « Que nul n’entre ici s’il n’est géomètre » que Platon aurait fait graver au fronton de l’Académie d’Athènes, supposait l’ambition de faire la synthèse entre le monde sensible et celui des idées pures. Plus tard, Galilée affirmait que « la nature est un livre écrit en langage mathématique ». Que dire aussi de la méthode de Descartes ou encore de l’*Éthique* de Spinoza, qui emprunte sa forme à la déduction mathématique ?\n",
    "\n",
    "Et c’est bien un objet issu des mathématiques, le vecteur, qui sert aujourd’hui de support à la traduction des mots dans le langage des machines. Il a cela de commode qu’il se laisse facilement manipuler par des opérations pour le comparer à d’autres vecteurs, autorisant de répondre à une question aussi essentielle que par exemple calculer la distance entre le vecteur $chat$ et le vecteur $miaou$.\n",
    "\n",
    "Les outils mathématiques existant déjà, tout l’art est celui de l’alchimiste : découvrir le nombre secret de l’objet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a72e85-e5ec-4e24-8cb8-4326ee32ef42",
   "metadata": {},
   "source": [
    "## Une approche naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd39421-1de2-4ee1-a405-cc98f46e369c",
   "metadata": {},
   "source": [
    "Quel nombre attribuer à un texte ou, plus spécifiquement, à son composant premier et emblématique, le mot ?\n",
    "\n",
    "Considérons deux phrases qui forment notre corpus :\n",
    "\n",
    "```txt\n",
    "(a) Le petit chat boit du lait.\n",
    "(b) Le petit chien boit de l’eau.\n",
    "```\n",
    "\n",
    "Après la phase de segmentation en mots, elles peuvent se représenter dans deux vecteurs $\\vec{A}$  et $\\vec{B}$ :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    \\text{le} \\\\\n",
    "    \\text{petit} \\\\\n",
    "    \\text{chat} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{lait}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    \\text{le} \\\\\n",
    "    \\text{petit} \\\\\n",
    "    \\text{chien} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{eau}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "De cette opération, il devient facile de constituer le vocabulaire de notre corpus dans une suite de mots :\n",
    "\n",
    "$$\n",
    "S = \\text{boit}, \\text{chat}, \\text{chien}, \\text{de}, \\text{du}, \\text{eau}, \\text{lait}, \\text{l}, \\text{le}, \\text{petit}\n",
    "$$\n",
    "\n",
    "De telle manière que :\n",
    "\n",
    "$$\n",
    "S_0 = \\text{boit}\n",
    "$$\n",
    "\n",
    "Une piste possible serait d’attribuer à chaque élément des vecteurs, les mots, leur numéro d’ordre dans le vocabulaire :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    S_8 \\\\\n",
    "    S_9 \\\\\n",
    "    S_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    S_6\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    S_8 \\\\\n",
    "    S_9 \\\\\n",
    "    S_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    S_5\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Puis, calculer la distance qui les sépare en faisant la somme de leurs différences ou, en langage mathématique, définir **la norme du vecteur** :\n",
    "\n",
    "$$\n",
    "\\| \\vec{AB} \\| = \\sqrt{(A_1 - B_1)^2 + (A_2 - B_2)^2 \\dots (A_n - B_n)^2}\n",
    "$$\n",
    "\n",
    "En Python, nous pourrions formaliser les instructions suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e30f74-8455-4915-b679-3688c97c129e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distance(*, a:list, b:list) -> float:\n",
    "    \"\"\"Euclidean distance between two vectors.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    a -- first vector\n",
    "    b -- second vector\n",
    "    \"\"\"\n",
    "    # difference between indices\n",
    "    coords = [\n",
    "        (x - y) ** 2\n",
    "        for x, y in zip(a, b)\n",
    "    ]\n",
    "    # distance = square root of the sum of coords\n",
    "    return sum(coords) ** .5\n",
    "\n",
    "# sentences into tokens\n",
    "a = [\"le\", \"petit\", \"chat\", \"boit\", \"du\", \"lait\"]\n",
    "b = [\"le\", \"petit\", \"chien\", \"boit\", \"de\", \"l\", \"eau\"]\n",
    "\n",
    "# corpus vocabulary\n",
    "S = [\"boit\", \"chat\", \"chien\", \"de\", \"du\", \"eau\", \"lait\", \"l\", \"le\", \"petit\"]\n",
    "\n",
    "# indices of elements rather than values\n",
    "A = [ S.index(x) for x in a ]\n",
    "B = [ S.index(y) for y in b ]\n",
    "\n",
    "print(f\"La distance entre les deux phrases est de {distance(a=A, b=B) :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd074f7-b61f-42a8-9e9f-1a0cef88cecb",
   "metadata": {},
   "source": [
    "## L’approche fréquentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b6aed-e88d-4196-8577-165d1ddce207",
   "metadata": {},
   "source": [
    "En vérité, les opérations réalisées n’ont pas beaucoup de sens. Dire qu’il existe « une distance de 1.7321 », ça signifie quoi exactement ? Ça semble peu, mais par rapport à quoi ? Quelle est l’unité de référence ? Tout ça sans oublier que, en plus, les vecteurs ne sont pas de tailles égales : une phrase qui ne comporterait qu’un seul mot se révélerait extrêmement proche de toute autre phrase qui commencerait par ce même mot. Nous pourrions refaire le calcul de distance en supprimant le dernier élément de `B` afin d’obtenir des vecteurs de tailles égales que nous obtiendrions le même résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cab05-5db9-4468-8345-877bd585b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"La distance entre les deux phrases est toujours de {distance(a=A, b=B[:-1]) :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc06b8-39ff-449f-b28d-81f61505afd5",
   "metadata": {},
   "source": [
    "Le nombre secret d’un mot ne semble donc pas être sa position dans un vocabulaire de référence. La première méthode véritable consiste à opter pour une approche fréquentielle : du corpus on obtient un sac de mots puis, pour chaque texte, on génère un vecteur de la longueur du corpus dont les dimensions sont le nombre d’occurrences de chaque mot.\n",
    "\n",
    "Considérons les deux textes suivants :\n",
    "\n",
    "```txt\n",
    "(a) Le petit chat boit du lait. Le lait n’est pas bon pour les chats.\n",
    "(b) Les petits chiens boivent de l’eau. L’eau irait aussi aux chats.\n",
    "```\n",
    "\n",
    "La construction d’un sac de mots (*bag of words*, BOW) implique d’autres méthodes que simplement la segmentation en mots. Il s’agit aussi de ne conserver que les mots signifiants et de les lemmatiser à partir, souvent, de leur étiquette morpho-syntaxique.\n",
    "\n",
    "De notre corpus, nous obtiendrions le sac de mots suivant, constitué de 9 éléments :\n",
    "\n",
    "$$\n",
    "\\text{BOW} = \\text{aller}, \\text{aussi}, \\text{boire}, \\text{bon}, \\text{chat}, \\text{chien}, \\text{eau}, \\text{lait}, \\text{petit}\n",
    "$$\n",
    "\n",
    "L’étape suivante est de compter, pour chacun des textes, le nombre d’occurrences des éléments du sac de mots :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    \\text{(aller :) } &0 \\\\\n",
    "    \\text{(aussi :) } &0 \\\\\n",
    "    \\text{(boire :) } &1 \\\\\n",
    "    \\text{(bon :) } &1 \\\\\n",
    "    \\text{(chat :) } &2 \\\\\n",
    "    \\text{(chien :) } &0 \\\\\n",
    "    \\text{(eau :) } &0 \\\\\n",
    "    \\text{(lait :) } &2 \\\\\n",
    "    \\text{(petit :) } &1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\hspace{5em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    \\text{(aller :) } &1 \\\\\n",
    "    \\text{(aussi :) } &1 \\\\\n",
    "    \\text{(boire :) } &1 \\\\\n",
    "    \\text{(bon :) } &0 \\\\\n",
    "    \\text{(chat :) } &1 \\\\\n",
    "    \\text{(chien :) } &1 \\\\\n",
    "    \\text{(eau :) } &2 \\\\\n",
    "    \\text{(lait :) } &0 \\\\\n",
    "    \\text{(petit :) } &1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les vecteurs étant de même longueur, il est cette fois-ci pertinent de normaliser le vecteur $\\vec{AB}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c2d05-f626-4b83-9f51-af3f23f7a39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = [0, 0, 1, 1, 2, 0, 0, 2, 1]\n",
    "B = [1, 1, 1, 0, 1, 1, 2, 0, 1]\n",
    "\n",
    "print(f\"La distance entre les deux phrases est de {distance(a=A, b=B) :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b7050-ac57-47a6-bb00-a084ddc8eb65",
   "metadata": {},
   "source": [
    "Le résultat n’a pas forcément plus de sens que lors de notre approche naïve, mais il permettrait d'effectuer un classement de proximité entre plusieurs phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57903bd-e1ed-4a8e-9888-947c5208e24a",
   "metadata": {},
   "source": [
    "## L’encodage 1 parmi *n*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819bf90-2fc5-4720-b0d0-31316092a331",
   "metadata": {},
   "source": [
    "Plus connue sous sa dénomination anglaise, *one-hot encoding*, et parfois appelée \"encodage à chaud\", cette méthode consiste à recueillir toutes les catégories possibles dans un vecteur et à les transformer en autant de classificateurs binaires ($0$ ou $1$).\n",
    "\n",
    "Considérons deux phrases étiquetées en parties du discours :\n",
    "\n",
    "```txt\n",
    "(a) Le/DET petit/ADJ chat/N boit/V du/DET lait/N ./PONCT\n",
    "(b) Le/DET petit/ADJ chien/N boit/V de/DET l’/DET eau/N ./PONCT\n",
    "```\n",
    "\n",
    "Après la phase de pré-traitement, nous obtenons le sac de mots ci-dessous :\n",
    "\n",
    "$$\n",
    "\\text{BOW} = (\\text{boire}, \\text{V}), (\\text{chat}, \\text{N}), (\\text{chien}, \\text{N}), (\\text{eau}, \\text{N}), (\\text{lait}, \\text{N}), (\\text{petit}, \\text{ADJ})\n",
    "$$\n",
    "\n",
    "Notons bien que chaque élément de la série $BOW$ peut se matérialiser comme un vecteur de longueur 2 :\n",
    "\n",
    "$$\n",
    "\\vec{BOW_1} = \\begin{pmatrix}\n",
    "    \\text{boire} \\\\\n",
    "    \\text{V}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "L’encodage *one-hot* détermine tout d’abord, à partir du sac de mots, les catégories à transformer en classificateurs. Chaqué élément de la série $BOW$ étant de longueur 2, deux ensembles de catégories seront créés : la première pour les lemmes, la seconde pour les étiquettes.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= \\text{boire}, \\text{chat}, \\text{chien}, \\text{eau}, \\text{lait}, \\text{petit} \\\\\n",
    "B &= \\text{ADJ}, \\text{N}, \\text{V}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les deux séries sont ensuite réunies en une seule :\n",
    "\n",
    "$$\n",
    "C = \\text{boire}, \\text{chat}, \\text{chien}, \\text{eau}, \\text{lait}, \\text{petit}, \\text{ADJ}, \\text{N}, \\text{V}\n",
    "$$\n",
    "\n",
    "Et, enfin, chaque élément du sac de mots est envoyé à la liste des classificateurs, puis transformé en un vecteur creux de $0$ et de $1$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{BOW_1} = \\begin{pmatrix}\n",
    "    \\text{(boire :) } &1 \\\\\n",
    "    \\text{(chat :) } &0 \\\\\n",
    "    \\text{(chien :) } &0 \\\\\n",
    "    \\text{(eau :) } &0 \\\\\n",
    "    \\text{(lait :) } &0 \\\\\n",
    "    \\text{(petit :) } &0 \\\\\n",
    "    \\text{(ADJ :) } &0 \\\\\n",
    "    \\text{(N :) } &0 \\\\\n",
    "    \\text{(V :) } &1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{BOW_2} = \\begin{pmatrix}\n",
    "    \\text{(boire :) } &0 \\\\\n",
    "    \\text{(chat :) } &1 \\\\\n",
    "    \\text{(chien :) } &0 \\\\\n",
    "    \\text{(eau :) } &0 \\\\\n",
    "    \\text{(lait :) } &0 \\\\\n",
    "    \\text{(petit :) } &0 \\\\\n",
    "    \\text{(ADJ :) } &0 \\\\\n",
    "    \\text{(N :) } &1 \\\\\n",
    "    \\text{(V :) } &0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{BOW_3} = \\begin{pmatrix}\n",
    "    \\text{(boire :) } &0 \\\\\n",
    "    \\text{(chat :) } &0 \\\\\n",
    "    \\text{(chien :) } &1 \\\\\n",
    "    \\text{(eau :) } &0 \\\\\n",
    "    \\text{(lait :) } &0 \\\\\n",
    "    \\text{(petit :) } &0 \\\\\n",
    "    \\text{(ADJ :) } &0 \\\\\n",
    "    \\text{(N :) } &1 \\\\\n",
    "    \\text{(V :) } &0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les vecteurs étant de longueurs égales, il est désormais pertinent de les normaliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ea722-982f-440b-94dc-c500f6b9ed41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = [1, 0, 0, 0, 0, 0, 0, 0, 1] # boire, V\n",
    "B = [0, 1, 0, 0, 0, 0, 0, 1, 0] # chat, N\n",
    "C = [0, 0, 1, 0, 0, 0, 0, 1, 0] # chien, N\n",
    "\n",
    "print(\n",
    "    f'La distance entre \"boire\" et \"chat\" est de {distance(a=A, b=B) :.4f}',\n",
    "    f'La distance entre \"boire\" et \"chien\" est de {distance(a=A, b=C) :.4f}',\n",
    "    f'La distance entre \"chat\" et \"chien\" est de {distance(a=B, b=C) :.4f}',\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46b86c-9e47-411b-9b2d-7a2fbfa7e02d",
   "metadata": {},
   "source": [
    "Les résultats s’interprètent facilement :\n",
    "\n",
    "- les lemmes *chat* et *chien* sont, dans l’espace vectoriel du corpus, équidistants du lemme *boire* ;\n",
    "- les lemmes *chat* et *chien* sont, dans l’espace vectoriel du corpus, plus proches l’un de l’autre que du lemme *boire*.\n",
    "\n",
    "En termes de géométrie, le triangle $ABC$ serait isocèle en $A$ avec :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\vec{AB} &= \\vec{AC} = 2 \\\\\n",
    "    \\vec{BC} &= 1,4142\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "![Distance entre les vecteurs](./images/distance-between-vectors.png)\n",
    "\n",
    "L’encodage *one-hot* est souvent un point de départ pour nourrir d’autres algorithmes comme dans le cas du *word embedding* avec l’aide de *Word2Vec*, des algorithmes mieux à même d’extraire des caractéristiques des vecteurs pour en dégager une similarité lexicale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70005212-935e-4adf-b2b2-ae3b49a0ebbb",
   "metadata": {},
   "source": [
    "## Évaluer l’importance d’un terme dans un document (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05807efa-fb68-4833-bd34-2d3bd04e77b3",
   "metadata": {},
   "source": [
    "Nous l’avons vu, certains termes dans un corpus sont plus importants que d’autres pour caractériser un texte par rapport à un autre, et leur importance n’est souvent pas proportionnelle à leur fréquence. De là découle la nécessité de repérer les mots vides de sens, les *stopwords*, pour les retirer du sac de mots (BOW) qui le représente. Pour autant, la méthode BOW se contente d’une mesure fréquentielle sans établir de rapport d’importance entre les termes.\n",
    "\n",
    "Une autre approche, largement répandue dans le traitement automatique du langage naturel, parvient à inférer, de l’analyse fréquentielle, une certaine valeur d’importance aux termes contenus dans le sac de mots. Cette approche repose sur deux principes : la fréquence du terme (TF) et la fréquence du terme dans le corpus (IDF) qui prêtent une signification à la rareté d’un terme.\n",
    "\n",
    "Sans parler de robustesse, la justification de cette approche repose sur la loi de Zipf qui prévoit que la fréquence d’un terme dans un texte est liée à son rang dans l’ordre des fréquences : le mot le plus fréquent apparaîtrait dix fois plus souvent que le dixième mot le plus fréquent, cent fois plus que le centième etc. En grande partie pour cette raison, la méthode TF-IDF ne souffre pas de la présence des mots vides dans le sac de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4a3bd-9516-465a-a451-ad066202b334",
   "metadata": {},
   "source": [
    "### La fréquence du terme (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a12a8-1250-4d16-8306-bb25564a6455",
   "metadata": {},
   "source": [
    "De l’anglais *term frequency*, la fréquence du terme établit un rapport entre le nombre d’occurrences d’un mot ($w$) dans un document et le nombre total de mots dans ce document ($n$) :\n",
    "\n",
    "$$\n",
    "\\text{TF}(w, n) = \\frac{w}{n}\n",
    "$$\n",
    "\n",
    "Prenons un corpus constitué de trois textes :\n",
    "\n",
    "```txt\n",
    "(A) Le petit chat boit du lait. Le lait n’est pas bon pour les chats.\n",
    "(B) Les petits chiens boivent de l’eau. L’eau irait aussi aux chats.\n",
    "(C) À partir d’un moment, eau ou lait, ils peuvent bien boire ce qu’ils veulent.\n",
    "```\n",
    "\n",
    "La taille en mots du document *A* est de 15, quand elle est de 13 pour le document *B* et de 16 pour le document *C*. Intéressons-nous au mot *lait* ($1$) qui apparaît deux fois dans *A* et une fois dans *C*, mais jamais dans *B*. Ses fréquences seront :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{TF}_{1, A} &= \\frac{2}{15} = 0,1333 \\\\\n",
    "    \\text{TF}_{1, B} &= \\frac{0}{13} = 0 \\\\\n",
    "    \\text{TF}_{1, C} &= \\frac{1}{16} = 0,0625 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea747b2-85e3-402f-96eb-9828283bec17",
   "metadata": {},
   "source": [
    "### La fréquence inverse de document (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225e66a-feb3-43c2-8e33-31cf7e72e908",
   "metadata": {},
   "source": [
    "Quand la mesure TF s’attachait au terme dans un document, la mesure IDF (*inverse document frequency*) va s’intéresser à la présence du terme dans le corpus entier selon la relation suivante où $d$ représente le nombre de documents où le terme apparaît et $N$ le nombre total de documents :\n",
    "\n",
    "$$\n",
    "\\text{IDF}(d, N) = \\ln{\\frac{N}{d}}\n",
    "$$\n",
    "\n",
    "Dans notre exemple, la mesure IDF pour le mot *lait* vaut :\n",
    "\n",
    "$$\n",
    "\\text{IDF}_1 = \\ln{\\frac{3}{2}} \\approx 0.4055\n",
    "$$\n",
    "\n",
    "Le calcul du logarithme permet de pondérer le rapport entre $N$ et $d$ dans la mesure où, lors de l’obtention de TF, le résultat était situé dans un intervalle $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ec78b-49b2-4f45-a9d9-c559f9e6f0bb",
   "metadata": {},
   "source": [
    "### La mesure TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa408bd-e023-410e-b62e-b24ff3dbc530",
   "metadata": {},
   "source": [
    "Au final, la formule TF-IDF est un produit entre TF et IDF. Pour notre exemple avec le mot *lait* :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{TFIDF}_{1, A} &= \\frac{2}{15} \\cdot ln \\frac{3}{2} \\approx 0,0541 \\\\\n",
    "    \\text{TFIDF}_{1, B} &= \\frac{0}{13} \\cdot ln \\frac{3}{2} = 0 \\\\\n",
    "    \\text{TFIDF}_{1, C} &= \\frac{1}{16} \\cdot ln \\frac{3}{2} \\approx 0,0253 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Au regard du mot *lait*, le document *A* apparaît ainsi comme plus pertinent.\n",
    "\n",
    "Avec Python, nous avons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7f6bd-a586-4d8f-aacb-71631d2916c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import log as ln\n",
    "\n",
    "def TF_IDF(*, t: int, w: int, d: int, n: int) -> float:\n",
    "    \"\"\"Term frequency–inverse document frequency.\n",
    "\n",
    "    Keyword arguments:\n",
    "    t -- term frequency in the document\n",
    "    w -- number of words in the document\n",
    "    d -- document frequency\n",
    "    n -- number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    tf = t / w\n",
    "\n",
    "    idf = ln(n / d)\n",
    "\n",
    "    return tf * idf\n",
    "\n",
    "print(\n",
    "    f'La mesure TF-IDF du mot \"lait\" dans le document A est de : {TF_IDF(t=2, w=15, d=2, n=3) :.4f}',\n",
    "    f'La mesure TF-IDF du mot \"lait\" dans le document B est de : {TF_IDF(t=0, w=13, d=2, n=3) :.4f}',\n",
    "    f'La mesure TF-IDF du mot \"lait\" dans le document C est de : {TF_IDF(t=1, w=16, d=2, n=3) :.4f}',\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e925e88-66e9-418b-9100-3fe8c5b5c1bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Limitations du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2706f-b233-4297-9de0-d973ca38ecd6",
   "metadata": {},
   "source": [
    "Quoique très largement utilisé pour sa facilité de mise en œuvre en dépit du coût en termes de calcul machine, la mesure TF-IDF souffre principalement de son incapacité à traiter la sémantique du terme.\n",
    "\n",
    "Gardons par ailleurs à l’esprit que la formule accordera mécaniquement davantage d’importance aux documents très volumineux, aussi faudra-t-il penser à les pénaliser ou bien à ne travailler que sur des corpus équilibrés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6710dea-c176-4049-b2c4-849830a82b6c",
   "metadata": {},
   "source": [
    "## Opérations sur les vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b8209-a2b4-4e2e-87c8-2754374b6174",
   "metadata": {},
   "source": [
    "La représentation mathématique du monde associe aux objets des propriétés et leur assigne des méthodes, comme la couleur du crayon et la fonction de dessiner. Pour un mot, on pourrait lister parmi ses propriétés son genre ou encore sa catégorie grammaticale, et, parmi ses méthodes, sa capacité à qualifier, à signifier ou à se décliner. La nature des objets matérialise parfois une barrière infrangible, tel qu’il est impossible de conjuguer un nombre ou de diviser des lettres.\n",
    "\n",
    "Une fois sous forme numérique, les mots, textes ou tokens héritent des opérations arithmétiques réservées aux nombres. Il devient possible de les soustraire, de les multiplier ou encore d’obtenir une moyenne, d’en extraire une racine ou de les comparer entre eux.\n",
    "\n",
    "S’agissant de vecteurs à *n* dimensions, on retiendra classiquement deux opérations :\n",
    "\n",
    "- la distance qui les sépare dans l’espace vectoriel du corpus ;\n",
    "- le cosinus de leur angle pour déterminer leur similarité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323724e-1db7-4c88-b4ae-7b5a5731f128",
   "metadata": {},
   "source": [
    "### Calculer la distance entre deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4a48a-12aa-43ad-b0b8-bcfd0b271d6c",
   "metadata": {},
   "source": [
    "Nous l’avons vu, la distance qui sépare deux vecteurs n’a de sens que dans un espace vectoriel borné. L’exemple du sac de mots d’un corpus permet en effet de situer les mots les uns par rapport aux autres dans cet espace sans ne rien présumer de leur relation dans un autre espace. Retirez ou ajoutez un texte et la mesure varierait.\n",
    "\n",
    "En plus de cette notion de relativité, rappelons qu’on ne peut comparer que des vecteurs de dimensions égales et qu’un vecteur de *n* dimensions ne peut se concevoir que dans un espace à *n* dimensions. Si deux vecteurs $\\vec{A}$ et $\\vec{B}$ sont chacun décrits par huit composantes, ils seront considérés dans un espace à huit dimensions.\n",
    "\n",
    "En règle générale, les mots sont convertis en des vecteurs de dimension très élevée, mais la grande majorité de ces dimensions est établie à 0. On parle alors de vecteur creux.\n",
    "\n",
    "La représentation d’un vecteur de dimension élevée étant malaisée, une opération courante consiste à le convertir dans une dimension inférieure. En un sens, lorsque l’on supprime les signes de ponctuation et les mots vides de sens du sac de mots d’un corpus, on réduit déjà sa dimensionnalité. Nous aborderons cette approche dans un autre calepin pour ne considérer ici que les opérations nécessaires au calcul de la distance entre deux vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf97a2f-e900-4617-9789-8388495fe6dc",
   "metadata": {},
   "source": [
    "#### La norme d’un vecteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa038f4c-f5da-47ee-825d-c259b68b53e1",
   "metadata": {},
   "source": [
    "Avant toute chose, un vecteur est décrit par trois caractéristiques : sa norme, sa direction et son sens. Nous ne nous intéresserons qu’à la première.\n",
    "\n",
    "Prenons un vecteur $\\vec{A}$ de dimension 4 :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    3 \\\\\n",
    "    12 \\\\\n",
    "    9 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Le calcul de sa norme est régi par l’extraction de la racine carrée du produit scalaire avec lui-même :\n",
    "\n",
    "$$\n",
    "\\| \\vec{A} \\| = \\sqrt{\\vec{A} \\cdot \\vec{A}}\n",
    "$$\n",
    "\n",
    "Grâce au développement de la formule, nous retrouvons le théorème de Pythagore :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\| \\vec{A} \\| &= \\sqrt{(3 \\times 3) + (12 \\times 12) + (9 \\times 9) + (0 \\times 0)} \\\\\n",
    "\\| \\vec{A} \\| &= \\sqrt{3^2 + 12^2 + 9^2 + 0^2} \\\\\n",
    "\\| \\vec{A} \\| &= \\sqrt{234} \\\\\n",
    "\\| \\vec{A} \\| &\\approx 15,2971\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8c18f-b16b-484b-bb2c-d5be1cb813f1",
   "metadata": {},
   "source": [
    "#### Normaliser un vecteur passant par deux points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaded41-3f1d-4b7d-b112-c447422b9024",
   "metadata": {},
   "source": [
    "Reprenons la formule que nous avons déjà abordée plus haut et qui permet de mesurer la distance entre deux vecteurs $\\vec{A}$ et $\\vec{B}$ :\n",
    "\n",
    "$$\n",
    "\\| \\vec{AB} \\| = \\sqrt{(A_1 - B_1)^2 + (A_2 - B_2)^2 \\dots (A_n - B_n)^2}\n",
    "$$\n",
    "\n",
    "Puis mobilisons deux vecteurs de dimension 3 :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    7 \\\\\n",
    "    32 \\\\\n",
    "    10\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    11 \\\\\n",
    "    4 \\\\\n",
    "    8\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Avant de leur appliquer la formule :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{(7 - 11)^2 + (32 - 4)^2 + (10 - 8)^2} \\\\\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{-4^2 + 28^2 + 2^2} \\\\\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{16 + 784 + 4} \\\\\n",
    "    \\| \\vec{AB} \\| &\\approx 28,3549\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "En détail, l’opération consiste à trouver un vecteur $\\vec{AB}$ qui passe par $A$ et par $B$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\vec{AB} &= \\begin{pmatrix}\n",
    "        B_1 - A_1 \\\\\n",
    "        B_2 - A_2 \\\\\n",
    "        B_3 - A_3\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\vec{AB} &= \\begin{pmatrix}\n",
    "        11 - 7 \\\\\n",
    "        4 - 32 \\\\\n",
    "        8 - 10\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\vec{AB} &= \\begin{pmatrix}\n",
    "        4   \\\\\n",
    "        -28 \\\\\n",
    "        -2\n",
    "    \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Sa norme s’obtient classiquement par la racine carrée de son produit scalaire avec lui-même :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{\\vec{AB} \\cdot \\vec{AB}} \\\\\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{4^2 + (-28)^2 + (-2)^2} \\\\\n",
    "    \\| \\vec{AB} \\| &= \\sqrt{16 + 784 + 4} \\\\\n",
    "    \\| \\vec{AB} \\| &\\approx 28,3549\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e93fb-babb-4388-9912-2a0e1b3000db",
   "metadata": {},
   "source": [
    "### Évaluer la similarité de deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e82af-9014-4834-b733-d69333cc3280",
   "metadata": {},
   "source": [
    "#### La similarité cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199db836-7e8d-42f3-9894-73b869425d71",
   "metadata": {},
   "source": [
    "Lorsque l’on parle de similarité de deux vecteurs en traitement automatique du langage naturel, on désigne la **similarité cosinus**, une mesure par laquelle on détermine le cosinus de leur angle dans un intervalle $[-1 , 1]$, où -1 qualifie deux vecteurs opposés, 0 deux vecteurs indépendants et 1 deux vecteurs colinéaires (il serait possible de tracer une droite pour les relier).\n",
    "\n",
    "La formule pour définir la similarité cosinus vaut :\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{\\vec{A} \\cdot \\vec{B}}{\\| \\vec{A} \\| \\| \\vec{B} \\|}\n",
    "$$\n",
    "\n",
    "Reprenons les vecteurs $\\vec{A}$ et $\\vec{B}$ décrits plus haut et calculons leur similarité :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cos \\theta &= \\frac{ (A_1 \\cdot B_1) + (A_2 \\cdot B_2) + (A_3 \\cdot B_3) }{ (\\sqrt{\\vec{A} \\cdot \\vec{A}}) \\times (\\sqrt{\\vec{B} \\cdot \\vec{B}}) } \\\\\n",
    "    \\cos \\theta &= \\frac{ (7 \\times 11) + (32 \\times 4) + (10 \\times 8) }{ (\\sqrt{7^2 + 32^2 + 10^2}) \\times (\\sqrt{11^2 + 4^2 + 8^2}) } \\\\\n",
    "    \\cos \\theta &= \\frac{77 + 128 + 80}{ 34,2491 \\times 14,1774 } \\\\\n",
    "    \\cos \\theta &= \\frac{285}{ 485,5632 } \\\\\n",
    "    \\cos \\theta &\\approx 0,5869\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Selon la nature de $\\vec{A}$ et $\\vec{B}$, nous pourrions avancer que 0,5869 est la mesure de l’indice de ressemblance entre les mots *A* et *B*, ou entre les documents *A* et *B* etc. Une application pratique serait de résoudre par exemple une tâche de classification en effectuant des regroupements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1adbbb-39fe-4ef0-a054-dfe22d65c9c1",
   "metadata": {},
   "source": [
    "#### Vérification dans le triangle quelconque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c57519-af68-4c53-a8e7-efa1f514f931",
   "metadata": {},
   "source": [
    "Considérons à présent $\\vec{A}$ et $\\vec{B}$ comme des points dans un espace vectoriel reliés à l’origine tel que :\n",
    "\n",
    "$$\n",
    "\\vec{O} = \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Nous pouvons estimer la distance de vecteurs passant par $\\vec{OA}$, $\\vec{OB}$ et $\\vec{AB}$ grâce au théorème de Pythagore :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\vec{OA} &= \\sqrt{(O_1 - A_1)^2 + (O_2 - A_2)^2 + (O_3 - A_3)^2} \\\\\n",
    "    &= \\sqrt{(0 - 7)^2 + (0 - 32)^2 + (0 - 10)^2} \\\\\n",
    "    &= \\sqrt{49 + 1024 + 100} \\\\\n",
    "    &= \\sqrt{1173} \\\\\n",
    "    &\\approx 34,2491 \\\\\n",
    "    \\vec{OB} &=\\sqrt{(0 - 11)^2 + (0 - 4)^2 + (0 - 8)^2} \\\\\n",
    "    &= \\sqrt{121 + 16 + 64} \\\\\n",
    "    &\\approx 14,1774 \\\\\n",
    "    \\vec{AB} &=\\sqrt{(7 - 11)^2 + (32 - 4)^2 + (10 - 8)^2} \\\\\n",
    "    &= \\sqrt{16 + 784 + 4} \\\\\n",
    "    &\\approx 28,3549 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "À partir de là, pour trouver $\\cos(O)$, nous pouvons invoquer la loi des cosinus dans un triangle quelconque, de telle manière que :\n",
    "\n",
    "$$\n",
    "\\cos(c) = \\frac{a^2 + b^2 - c^2}{2ab}\n",
    "$$\n",
    "\n",
    "Appliquée à notre exemple, la formule nous donne :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cos(O) &= \\frac{\\vec{OA}^2 + \\vec{OB}^2 - \\vec{AB}^2}{2 \\cdot \\vec{OA} \\cdot \\vec{OB}} \\\\\n",
    "    &= \\frac{34,2491^2 + 14,1774^2 - 28,3549^2}{2 \\cdot 34,2491 \\cdot 14,1774} \\\\\n",
    "    &= \\frac{1173 + 201 - 804}{2 \\cdot 485,5631} \\\\\n",
    "    &= \\frac{570}{971,1262} \\\\\n",
    "    &\\approx 0,5869\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c093c-8995-4d40-b334-d5a3ff7c6681",
   "metadata": {},
   "source": [
    "#### Similarité de vecteurs binaires : l’indice de Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e380e-3c9f-4b20-a47f-8b36cdb68e96",
   "metadata": {},
   "source": [
    "Dans l’exemple précédent, les attributs des vecteurs $\\vec{A}$ et $\\vec{B}$ étaient représentés par des nombres pouvant prendre leur valeur dans l’ensemble $\\mathbb{R}$. Et si les possibilités pour les valeurs se limitaient à *0* et *1*, comme avec un encodage *one-hot* ? Dans ce cas particulier, la similarité peut se calculer avec l’indice de Jaccard :\n",
    "\n",
    "$$\n",
    "J = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\n",
    "$$\n",
    "\n",
    "Ou, sachant que *n* est le nombre d’attributs :\n",
    "\n",
    "$$\n",
    "J = \\frac{M_{11}}{n - M_{00}}\n",
    "$$\n",
    "\n",
    "Dans ces formules, les quantités valent :\n",
    "\n",
    "- $M_{00}$ pour le nombre d’attributs qui valent $0$ dans $\\vec{A}$ et $0$ dans $\\vec{B}$ ;\n",
    "- $M_{01}$ pour le nombre d’attributs qui valent $0$ dans $\\vec{A}$ et $1$ dans $\\vec{B}$ ;\n",
    "- $M_{10}$ pour le nombre d’attributs qui valent $1$ dans $\\vec{A}$ et $0$ dans $\\vec{B}$ ;\n",
    "- $M_{11}$ pour le nombre d’attributs qui valent $1$ dans $\\vec{A}$ et $1$ dans $\\vec{B}$.\n",
    "\n",
    "Prenons les deux vecteurs suivants :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Après application de la formule, nous obtenons :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J &= \\frac{1}{1 + 2 + 1} \\\\\n",
    "    J &= \\frac{1}{4} \\\\\n",
    "    J &\\approx 0,25\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

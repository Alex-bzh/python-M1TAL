{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac67fe56-e2ca-40cf-a906-26f24e0750ae",
   "metadata": {},
   "source": [
    "# L’analyse en composantes principales (ACP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af09f9-3c4f-499b-a96b-b4e295fdc45c",
   "metadata": {},
   "source": [
    "Le principe derrière l’ACP (ou PCA en anglais pour *principal component analysis*) serait de résumer l’information contenue dans une base de données grâce à des variables synthétiques appelées **composantes principales**. Grâce à des méthodes statistiques, les variables retenues pour l’analyse sont combinées entre elles afin de mettre en évidence des traits qui rassemblent le maximum de la variance des données.\n",
    "\n",
    "Nous parlons bien d’un maximum sans garantir l’intégralité. Toute compression implique inévitablement une perte de l’information, mais elle se fait au profit d’un gain de performance et d’acquisition des données.\n",
    "\n",
    "Comme lors d’autres méthodes d’analyse des données, nous passerons par une étape incontournable de préparation puis de construction d’une matrice propice à la décomposition – afin d’en révéler les composantes principales, avant de projeter les données dans un sous-espace.\n",
    "\n",
    "Si les outils informatiques permettent d’obtenir rapidement une ACP, il est important d’en comprendre les étapes afin, peut-être, d’améliorer les paramètres de l’algorithme. Nous allons dans un premier temps effectuer l’analyse à la main avant de mobiliser des outils informatiques.\n",
    "\n",
    "Commençons par charger les bibliothèques logicielles nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272f4eb-c848-462c-94c3-102d0618558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169137f-f87d-4cbd-a252-9fa7fa8743dd",
   "metadata": {},
   "source": [
    "## Une ACP à la main sur une sélection des indicateurs du vivre mieux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fecfe-f9cb-4955-b04c-e27558bb0be0",
   "metadata": {},
   "source": [
    "### Présentation de l’enquête"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea554d-d689-43f3-a2f0-7f740309fe94",
   "metadata": {},
   "source": [
    "Chargeons une enquête, limitée aux données concernant les femmes des pays de l’OCDE et augmentée de la Russie et de l’Afrique du Sud, sur l’indicateur du vivre de mieux et affichons les premières lignes afin de prendre connaissance des informations enregistrées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345116b-46da-40fd-a25f-ef9360ba3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/better-life-index-women-2021.csv\")\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444c76c-3731-4588-a9f8-53795c22f397",
   "metadata": {},
   "source": [
    "L’enquête comporte un certain nombre de variables numériques expliquées ci-dessous :\n",
    "\n",
    "|Variable|Signification|\n",
    "|:-:|-|\n",
    "|*code du pays*|Pays|\n",
    "|*country*|Pays|\n",
    "|*PS_FSAFEN*|Se sentir en sécurité quand on marche seul la nuit|\n",
    "|*JE_EMPL*|Taux d’emploi|\n",
    "|*JE_LTUR*|Taux de chômage de longue durée|\n",
    "|*SC_SNTWS*|Qualité du réseau social|\n",
    "|*ES_EDUA*|Niveau d’instruction|\n",
    "|*ES_STCS*|Compétences des élèves|\n",
    "|*ES_EDUEX*|Années de scolarité|\n",
    "|*EQ_WATER*|Qualité de l’eau|\n",
    "|*HS_LEB*|Espérance de vie|\n",
    "|*HS_SFRH*|Auto-évaluation de l’état de santé|\n",
    "|*SW_LIFS*|Satisfaction à l’égard de la vie|\n",
    "|*PS_REPH*|Taux d’homicides|\n",
    "|*WL_EWLH*|Horaires de travail lourds|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a50f04-eeae-4d9a-ba4a-7ab0a747c530",
   "metadata": {},
   "source": [
    "### Centrer et réduire les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba0fb8-1e94-4c08-84b4-b13817f5694c",
   "metadata": {},
   "source": [
    "On le remarque aisément, toutes les variables ne sont pas codées sur la même échelle : parfois nous avons des pourcentages, d’autres un nombre d’années, ou encore une évaluation de 0 à 10. Afin de limiter l’influence d’une variable sur l’autre, le premier réflexe est de centrer les valeurs autour de la moyenne, voire de les réduire. Pour centrer une variable, il suffit d’appliquer la formule $x = X - \\mu$ et, pour la réduire, il faut ensuite la diviser par l’écart-type :\n",
    "\n",
    "$$\n",
    "x = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Le principal avantage d’une variable centrée est qu’elle dispose d’une espérance nulle (la moyenne de ses observations sera proche de zéro). Si elle est en plus réduite, elle deviendra indépendante de l’unité et son écart-type comme sa variance égaleront 1.\n",
    "\n",
    "Préparons une copie des données numériques en éliminant les deux observations qui comportent des valeurs vides (Slovénie et Afrique du Sud) et centrons la première variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27905e9a-0caf-400f-9e1a-a56814bb12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working copy\n",
    "data = df.copy()\n",
    "\n",
    "# drop rows with NA values\n",
    "data = data.dropna(ignore_index=True)\n",
    "\n",
    "# keep track of index\n",
    "index = data.code\n",
    "\n",
    "# select only numerical columns\n",
    "data = data.drop(columns=['code', 'country'])\n",
    "\n",
    "# centering first feature\n",
    "data.PS_FSAFEN = data.PS_FSAFEN - data.PS_FSAFEN.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7753d-cecf-462f-b93b-d03d7e9f1701",
   "metadata": {},
   "source": [
    "La moyenne de la variable `PS_FSAFEN` devrait maintenant avoisiner 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5349684-1fab-496d-8800-d06a4180b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.PS_FSAFEN.mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c393d-e20b-42e2-8a20-2c3d8dff49b2",
   "metadata": {},
   "source": [
    "Si ensuite nous la réduisons, son écart-type est fixé autour de 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0e9b1-0ef0-474f-bfe2-4dc8109f65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling first feature\n",
    "data.PS_FSAFEN = data.PS_FSAFEN / data.PS_FSAFEN.std()\n",
    "\n",
    "data.PS_FSAFEN.std().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956ecf4-b935-476e-bccb-631344087214",
   "metadata": {},
   "source": [
    "Il reste à appliquer le même traitement à toutes les colonnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c73d1a-e85f-4ba7-8dd4-b21545198b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centering and scaling all features\n",
    "data_scaled = (data - data.mean(axis=0)) / data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1537c7-38a4-4ce8-81aa-ae21c614498f",
   "metadata": {},
   "source": [
    "Un coup d’œil rapide à nos données nous prouve que la transformation a bien été appliquée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f0328-1a25-4932-b020-446ba29627a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174e4c8-fecf-410f-87d3-e2d50e4c8e12",
   "metadata": {},
   "source": [
    "### Préservation de la variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42125d1-83cc-4d68-8498-77097fb432e2",
   "metadata": {},
   "source": [
    "Nous avons insisté sur ce point : toute compression implique une perte d’information qu’il convient de minimiser en préservant la variance du jeu de données.\n",
    "\n",
    "La variance peut se comprendre comme l’éloignement des valeurs d’une variable aléatoire par rapport à leur moyenne. En langage mathématique, elle exprime la moyenne des carrés des écarts à la moyenne et s’obtient, par la formule suivante :\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n - 1}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $\\sigma^2$ est le carré de l’écart-type ;\n",
    "- $n - 1$ correspond à l’estimateur non biaisé ;\n",
    "\n",
    "Si nous nous attachons à la première variable, sa variance vaut ainsi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f82978-39cf-4c15-ba64-fc9c55d913e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ps_fsafen = sum((data_scaled.PS_FSAFEN - data_scaled.PS_FSAFEN.mean()) ** 2) / (len(data_scaled) - 1)\n",
    "\n",
    "print(var_ps_fsafen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e88a2-e652-4ca0-a0f5-8dfe0425f494",
   "metadata": {},
   "source": [
    "Selon la définition, nous nous attendions à ce que la variance d’une variable centrée-réduite soit de 1. Si la variance de toutes les variables dans notre jeu de données sont égales à 1, qu’en est-il de leur covariance ?\n",
    "\n",
    "La covariance entre deux variables mesure le produit de leurs écarts par rapport à leur moyenne respective. Elle se calcule par la relation suivante :\n",
    "\n",
    "$$\n",
    "\\text{Cov}_{x,y} = \\frac{\\sum{(x_i - \\bar x)(y_i - \\bar y)}}{n - 1}\n",
    "$$\n",
    "\n",
    "Pour la covariance des deux premières variables de notre enquête, nous aurions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139c184-2d6f-452f-876c-0b3c065e724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_ps_fsafen_je_empl = sum((data_scaled.PS_FSAFEN - data_scaled.PS_FSAFEN.mean()) * (data_scaled.JE_EMPL - data_scaled.JE_EMPL.mean())) / (len(data_scaled) - 1)\n",
    "\n",
    "print(round(cov_ps_fsafen_je_empl, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0693f3-3bc9-4695-b034-0764db0ab1c4",
   "metadata": {},
   "source": [
    "Nous pouvons ainsi dresser le tableau suivant :\n",
    "\n",
    "|Variable|PS_FSAFEN|JE_EMPL|\n",
    "|:-|:-:|:-:\n",
    "|PS_FSAFEN|1|0.6466|\n",
    "|JE_EMPL|0.6466|1|\n",
    "\n",
    "Et produire la matrice carrée correspondante ci-dessous, réputée diagonale et symétrique :\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\vec{X}_{j_1, j_2}) = \\begin{bmatrix}\n",
    "    1 & 0.6466 \\\\\n",
    "    0.6466 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Si nous rajoutons la variable *JE_LTUR*, nous obtenons :\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\vec{X}_{j_1, j_2, j_3}) = \\begin{bmatrix}\n",
    "    1 & 0.6466 & -0.1181 \\\\\n",
    "    0.6466 & 1 & -0.2278 \\\\\n",
    "    -0.1181 & -0.2278 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7934a-36ce-4dd8-a94b-2643e1cb94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_ps_fsafen_je_empl_je_ltur = sum(\n",
    "        (data_scaled.JE_EMPL - data_scaled.JE_EMPL.mean()) *\n",
    "        (data_scaled.JE_LTUR - data_scaled.JE_LTUR.mean())\n",
    "    ) / (len(data_scaled) - 1)\n",
    "\n",
    "print(round(cov_ps_fsafen_je_empl_je_ltur, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48825f8-8e1f-429d-bee5-fe70663ae0e5",
   "metadata": {},
   "source": [
    "### Décomposition en valeurs propres et vecteurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cda6e4-cd51-4478-9a5f-b30b8b2dce3f",
   "metadata": {},
   "source": [
    "Peu importe le nombre de variables de notre jeu de données, avec la matrice de covariance nous aurons toujours une matrice carrée qu’il sera possible de décomposer en valeurs propres (*eigenvalues*) et vecteurs propres (*eigenvectors*).\n",
    "\n",
    "En algèbre linéaire, un vecteur propre « correspond à l'étude des axes privilégiés selon lesquels l’application [linéaire] se comporte comme une dilatation, multipliant les vecteurs par une même constante. » ([*Wikipédia*](https://fr.wikipedia.org/wiki/Valeur_propre,_vecteur_propre_et_espace_propre))\n",
    "\n",
    "![Équation d’une valeur propre](./images/eigenvalue-equation.svg)\n",
    "\n",
    "La constante $A$ étire dans la figure le vecteur $x$ sans modifier son orientation. $x$ est réputé être le vecteur propre de $A$ pour la valeur propre $\\lambda$.\n",
    "\n",
    "**Crédits :** Lyudmil Antonov Lantonov. – *Eigenvalue equation*. – CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c06b44-1fe0-496c-be51-e59f0a9e7be3",
   "metadata": {},
   "source": [
    "#### Calculer les valeurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ff1f9-56c3-4cf0-816e-6f0bc7da5469",
   "metadata": {},
   "source": [
    "Maintenant que nous disposons d’une matrice carrée, il est possible de la décomposer en valeurs propres et vecteurs propres. Les valeurs propres d’une matrice seront les racines de son polynôme caractéristique, sachant qu’une matrice d’ordre $(2, 2)$ sera représentée par un polynôme de degré 2, une matrice $(3, 3)$ par un polynôme de degré 3 et ainsi de suite. La formule vaut :\n",
    "\n",
    "$$\n",
    "P_M(x) = \\text{det}[M - x.I_n]\n",
    "$$\n",
    "\n",
    "Le polynôme caractéristique de notre matrice de covariance incluant uniquement les variables *PS_FSAFEN* et *JE_EMPL* serait :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    M - x.I_2 &= \\begin{bmatrix}\n",
    "        1 & 0.6466 \\\\\n",
    "        0.6466 & 1\n",
    "    \\end{bmatrix} - x \\cdot \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        1 & 0.6466 \\\\\n",
    "        0.6466 & 1\n",
    "    \\end{bmatrix} -\n",
    "    \\begin{bmatrix}\n",
    "        x & 0 \\\\\n",
    "        0 & x\n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        -x + 1 & 0.6466 \\\\\n",
    "        0.6466 & -x + 1\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Puis, pour le déterminant :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{det} \\begin{bmatrix}\n",
    "        -x + 1 & 0.6466 \\\\\n",
    "        0.6466 & -x + 1\n",
    "    \\end{bmatrix} &= (-x + 1)^2 - 0.6466^2 \\\\\n",
    "    &= x^2 - 2x + 0.5819\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Une fois en possession d’un polynôme de degré 2, la formule quadratique nous permet de révéler ses racines :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Delta &= (-2)^2 - 4 \\times 0.5819\\\\\n",
    "    \\Delta &= 4 - 4 * 0.5819\\\\\n",
    "    \\Delta &= 1.6724\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\Delta$ étant strictement supérieur à 0, il existe deux racines à notre polynôme. Calculons-les :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_1 &= \\frac{2 + \\sqrt{1.6724}}{2} \\\\\n",
    "    x_1 &= 1.6466 \\\\\n",
    "    x_2 &= \\frac{2 - \\sqrt{1.6724}}{2} \\\\\n",
    "    x_2 &= 0.3534\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Les valeurs propres sont ainsi $\\lambda_1 = 1.6466$ et $\\lambda_2 = 0.3534$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d49f9-21bf-4edd-b800-6e3164c7b6a1",
   "metadata": {},
   "source": [
    "#### Calculer les vecteurs propres associés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdb5fc-f6f1-4d9b-a877-006c96dc76de",
   "metadata": {},
   "source": [
    "Les vecteurs propres associés à $\\lambda_1$ et $\\lambda_2$ correspondent au système d’équation :\n",
    "\n",
    "$$\n",
    "(M − \\lambda I_n)\\vec{X} = \\vec{0}\n",
    "$$\n",
    "\n",
    "Pour $\\lambda_1 = 1.6466$, le vecteur propre est :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\left( \\begin{bmatrix}\n",
    "        1 & 0.6466 \\\\\n",
    "        0.6466 & 1\n",
    "    \\end{bmatrix} -\n",
    "    1.6466 \\cdot \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix} \\right) \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix} &= \\vec{0} \\\\\n",
    "    \\left( \\begin{bmatrix}\n",
    "        1 & 0.6466 \\\\\n",
    "        0.6466 & 1\n",
    "    \\end{bmatrix} -\n",
    "    \\begin{bmatrix}\n",
    "        1.6466 & 0 \\\\\n",
    "        0 & 1.6466\n",
    "    \\end{bmatrix} \\right) \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix} &= \\vec{0} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        -0.6466 & 0.6466 \\\\\n",
    "        0.6466 & -0.6466\n",
    "    \\end{bmatrix} \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix} &= \\vec{0} \\\\\n",
    "    \\left \\{\n",
    "    \\begin{array}{c @{=} c}\n",
    "        -0.6466 x_1 + 0.6466 x_2 &= 0 \\\\\n",
    "        0.6466 x_1 - 0.6466 x_2 &= 0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Nous pouvons conclure que le vecteur propre associé à $\\lambda_1$ est : $\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$\n",
    "\n",
    "Et pour $\\lambda_2 = 0.3534$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\left( \\begin{bmatrix}\n",
    "        1 & 0.6466 \\\\\n",
    "        0.6466 & 1\n",
    "    \\end{bmatrix} -\n",
    "    \\begin{bmatrix}\n",
    "        0.3534 & 0 \\\\\n",
    "        0 & 0.3534\n",
    "    \\end{bmatrix} \\right) \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix} &= \\vec{0} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        0.6466 & 0.6466 \\\\\n",
    "        0.6466 & 0.6466\n",
    "    \\end{bmatrix} \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix} &= \\vec{0} \\\\\n",
    "    \\left \\{\n",
    "    \\begin{array}{c @{=} c}\n",
    "        0.6466 x_1 + 0.6466 x_2 &= 0 \\\\\n",
    "        0.6466 x_1 + 0.6466 x_2 &= 0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ainsi, le vecteur propre associé à $\\lambda_2$ est $\\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}$\n",
    "\n",
    "Vérifions le calcul avec *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0959a-3813-4284-92fd-4b4eaa380fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, 0.6466],\n",
    "    [0.6466, 1]\n",
    "])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "for i, eigenvalue in enumerate(eigenvalues):\n",
    "    print(f\"Le vecteur propre de {eigenvalue} est : {eigenvectors[:, i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d2f17-531c-417b-a28c-48fef0bc4669",
   "metadata": {},
   "source": [
    "**Remarque :** *Numpy* rajoute une étape de normalisation de telle manière que nous obtenons bien les vecteurs suivants :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\vec{v_1} &= \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\\\\n",
    "    \\vec{v_1} &= \\begin{bmatrix} 0.70710678 \\\\ 0.70710678 \\end{bmatrix} \\\\\n",
    "    \\vec{v_2} &= \\begin{bmatrix} \\frac{-1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\\\\n",
    "    \\vec{v_2} &= \\begin{bmatrix} -0.70710678 \\\\ 0.70710678 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd28dc-b3b7-4819-ae9d-19590be301e7",
   "metadata": {},
   "source": [
    "### Charger la matrice des composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e749992-9eb3-4c16-98bf-b216f7600114",
   "metadata": {},
   "source": [
    "Les étapes précédentes nous ont permis d’obtenir les composantes principales de nos données pour les variables *PS_FSAFEN* et *JE_EMPL* uniquement. Il est bon de signaler que le résultat aurait été tout autre si nous avions effectué les calculs avec d’autres variables.\n",
    "\n",
    "Les composantes principales de ce sous-ensemble de notre jeu de données correspondent simplement aux vecteurs $\\vec{v_1}$ et $\\vec{v_2}$ ! Cette nouvelle matrice vaut :\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    0.70710678 & −0.70710678 \\\\\n",
    "    0.70710678 & 0.70710678\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Considérons à présent la matrice $M$ comme le sous-ensemble de notre jeu de données correspondant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e3764-2b53-4de9-912f-531636cbbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata = pd.DataFrame(data=data_scaled, columns=['PS_FSAFEN', 'JE_EMPL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb55b0c-a1df-4e17-b6be-23fc7ae09b6a",
   "metadata": {},
   "source": [
    "Et zoomons sur les cinq premières observations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287b867-0228-4ca9-8e20-3b3b7aa1e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(subdata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367d0ea-bd3b-487e-b969-e9296584ad01",
   "metadata": {},
   "source": [
    "En effectuant le produit matriciel entre $M$ et $A$, nous obtiendrons une projection de nos données sur les axes de nos composantes principales :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    -0.767841 & 0.538239 \\\\\n",
    "    1.168137 & 0.444417 \\\\\n",
    "    -1.251835 & -0.212333 \\\\\n",
    "    0.131006 & 0.350596 \\\\\n",
    "    0.131006 & 0.350596\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "    0.70710678 & −0.70710678 \\\\\n",
    "    0.70710678 & 0.70710678\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    -0.16235313 & 0.92353802  \\\\\n",
    "    1.14024787 & -0.51174732 \\\\\n",
    "    -1.03532312 & 0.73503891 \\\\\n",
    "    0.34054404 & 0.15527358 \\\\\n",
    "    0.34054404 & 0.15527358\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Le calcul se vérifie facilement avec *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d5e9c-5478-4077-a7a8-816921a3c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.70710678, -0.70710678],\n",
    "    [0.70710678, 0.70710678]\n",
    "])\n",
    "data_projected = np.dot(subdata.values, A)\n",
    "\n",
    "projected_df = pd.DataFrame(data_projected, columns=[\"PC1\", \"PC2\"], index=index)\n",
    "\n",
    "# five first rows\n",
    "display(projected_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba54bb4-2f41-47af-a72d-1995416a62e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Projeter visuellement les composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28009efa-a0ce-4f7e-a6ec-539ffd58b2c9",
   "metadata": {},
   "source": [
    "À l’aide d’un graphique, nous pouvons qualifier l’impact de la transformation sur les données initiales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107287be-caea-409d-86ec-da36d0a3b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "\n",
    "_ = sns.scatterplot(data=df, x=\"PS_FSAFEN\", y=\"JE_EMPL\", ax=axes[0])\n",
    "_ = sns.scatterplot(data=projected_df, x=\"PC1\", y=\"PC2\", ax=axes[1])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc43b2-9f2c-44a5-8dc5-ece095695e51",
   "metadata": {},
   "source": [
    "Après une observation attentive, on note bien une rotation des points sur des axes qui semblent orthogonaux, mais sans gain évident pour notre étude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa83056-d97c-487c-99b6-6ffb87eb638c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expliquer la variance par les valeurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae7771-4a27-4195-962d-44890debad0c",
   "metadata": {},
   "source": [
    "L’objectif de notre étude était de réduire la dimensionnalité de nos données. Or, nous avons jusqu’ici simplement décomposé un sous-ensemble de deux variables en autant de composantes principales, une opération nulle en somme. Il est par conséquent l’heure de nous séparer de l’une des deux composantes, mais comment faire le bon choix ?\n",
    "\n",
    "Si nous calculons les variances de nos composantes principales, nous retrouvons des données connues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0fc0b-81c7-4857-9e72-129c8fdbe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_pc1 = sum((projected_df.PC1 - projected_df.PC1.mean()) ** 2) / (len(projected_df) - 1)\n",
    "var_pc2 = sum((projected_df.PC2 - projected_df.PC2.mean()) ** 2) / (len(projected_df) - 1)\n",
    "\n",
    "print(\n",
    "    f\"La variance de PC1 est égale à lambda 1 : {var_pc1:.5f}\",\n",
    "    f\"La variance de PC2 est égale à lambda 2 : {var_pc2:.5f}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862b11e-3f36-4f6a-b57d-3551767d3b18",
   "metadata": {},
   "source": [
    "Si nos deux composantes expliquent 100 % de la variance de nos données, ce qui est le cas, il devient aisé d’estimer la part expliquée par chacune d’elles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae856b1b-5476-4e92-8e9e-48c3dad4cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"PC1 explique{var_pc1 / (var_pc1 + var_pc2): .2%} de la variance des données.\",\n",
    "    f\"PC2 explique{var_pc2 / (var_pc1 + var_pc2): .2%} de la variance des données.\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465f2aa-ed49-4cb0-96b5-e85a2a94686e",
   "metadata": {},
   "source": [
    "En conclusion, nous pouvons nous séparer de la deuxième composante principale tout en préservant un maximum la variance de nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fec05-340e-4c43-8b4b-32c28fa4c305",
   "metadata": {},
   "source": [
    "## Réaliser une ACP guidée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f2f3e-3584-4c82-b4f7-96179113a060",
   "metadata": {},
   "source": [
    "Voyons à présent comment réaliser cette analyse sur l’ensemble des indicateurs du vivre mieux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546e9b4-b00f-48c1-958f-3e102bb38cca",
   "metadata": {},
   "source": [
    "### Étape 1 : préparer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c78e4-7c63-4006-a943-48add4d1bdfe",
   "metadata": {},
   "source": [
    "Commençons par préparer les données de la même façon mais en se posant des questions un peu différentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4dc3c-e173-4871-b4ba-5e746ca0b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what features should be retained for the APC?\n",
    "features = df.columns.drop(labels=['code', 'country'])\n",
    "\n",
    "# drop NA\n",
    "X = df[features].dropna(ignore_index=True)\n",
    "index = df.dropna(ignore_index=True).code\n",
    "\n",
    "# restoring index\n",
    "X.index = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59912775-95e3-4398-8fa3-166f786438c8",
   "metadata": {},
   "source": [
    "### Étape 2 : centrer-réduire les variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4450e19-a5ca-498b-a41e-6a09ab0f973b",
   "metadata": {},
   "source": [
    "Normalisons les données afin d’obtenir des variables centrées-réduites :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca9f9f-7378-4dbd-afd9-9fd33ddca526",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42caca25-5fb3-4f82-aa67-627d7780214c",
   "metadata": {},
   "source": [
    "### Étape 3 : établir une matrice de covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336d17e-0560-4c55-ba40-0684b81b07e8",
   "metadata": {},
   "source": [
    "Mettons à profit la méthode `.cov()` de la bibliothèque *Numpy* pour calculer la matrice de covariance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1feac-b9b2-4639-b98a-bc72b4a65f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(X_scaled, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27772dd-ecde-4e19-83f7-38a3bbf46ba9",
   "metadata": {},
   "source": [
    "Le paramètre `rowvar`, fixé à `True` par défaut, doit être mis à `False` afin de préciser que les variables sont dans les colonnes et les observations sur les lignes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d843a4-883a-49e9-874a-46f1a0f146c8",
   "metadata": {},
   "source": [
    "### Étape 4 : décomposition de la matrice de covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77e3a8-6580-47c5-b1ab-093548b8fec1",
   "metadata": {},
   "source": [
    "Nous le savons, les composantes principales sont les vecteurs propres de la matrice de covariance, que nous pouvons obtenir directement avec la méthode `.linalg.eig()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d65bd4-6564-4833-859e-1f2ab20202f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091fbe1-2136-49a1-90b1-c919319c574a",
   "metadata": {},
   "source": [
    "Les vecteurs sont déjà empilés en colonne, aussi $\\vec{v_1}$ peut-il être consulté avec la syntaxe suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797294d-f10d-42fe-9d6c-a7ccc64eecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dba51-4994-449c-b452-12a5f0379f3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Étape 5 : choisir le nombre de facteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b72548-e05e-44c8-9ed2-e02d02b2ebe8",
   "metadata": {},
   "source": [
    "Cette étape est cruciale pour réduire la dimensionnalité de notre enquête. Sur la totalité des composantes principales calculées, une matrice d’ordre $(n, m)$ ayant maximum $m$ composantes principales, certaines n’expliquent que très peu la variance des données.\n",
    "\n",
    "Par définition, nous savons que la variance de chaque composante principale est expliquée par leur valeur propre associée. De là, nous pouvons établir que :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28404fd4-32fd-4b9c-84e1-17ae7951da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, eigenvalue in enumerate(eigenvalues):\n",
    "    print(f\"PC{n + 1} explique{eigenvalue / sum(eigenvalues): .2%} de la variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0baa3-50d9-4ca6-a88b-0f37a4ff577f",
   "metadata": {},
   "source": [
    "En jetant un œil plus attentif aux taux de variance expliquée, nous remarquons que la 8e composante est moins déterminante que la 9e, d’où la nécessité de trier les valeurs propres par score décroissant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846bacb-a457-4814-9b23-2082ebbdc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(eigenvalues, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379e87d-95e4-4020-8910-dd1c80b8440e",
   "metadata": {},
   "source": [
    "**Attention !** Si nous modifions l’ordre des valeurs propres, il convient de reporter la mutation sur les vecteurs propres. La fonction native `sorted()` ne nous permettra malheureusement pas de suivre les changements sauf à regrouper vecteurs et valeurs dans une structure commune. *Numpy* nous sauve une fois de plus grâce à la méthode `.argsort()` qui renvoie un numéro d’indice permettant de trier une matrice :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde09bc-5c3a-401a-b899-1c72ddd274e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = eigenvalues.argsort()[::-1]\n",
    "\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0d8c3-3916-413e-9e14-849f6be9b7f1",
   "metadata": {},
   "source": [
    "À présent, posons-nous la question essentielle pour notre objectif de réduction de la dimensionnalité : combien de composantes principales conserver pour expliquer un maximum de la variance des données ? Nous retenons un seuil de 70 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7c966-ce05-43ad-a8cc-adc01d8d2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = 0\n",
    "n_components = 0\n",
    "\n",
    "for n, eigenvalue in enumerate(eigenvalues):\n",
    "    variance += eigenvalue / sum(eigenvalues)\n",
    "    n_components += 1\n",
    "    if variance > .7: break\n",
    "\n",
    "print(f\"Les {n_components} premières composantes principales expliquent {variance:.2%} de la variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44310148-8c2e-4571-8370-fdfe088d5c06",
   "metadata": {},
   "source": [
    "Une autre méthode consiste à afficher un graphique du nombre de composantes principales en fonction des valeurs propres, appelé diagramme d’éboulis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e7de1-9bf8-434e-a020-077dc2068a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lineplot(x=range(1, len(eigenvalues) + 1), y=eigenvalues)\n",
    "\n",
    "plt.title(\"Diagramme d’éboulis\")\n",
    "plt.xlabel(\"Numéro de la composante principale\")\n",
    "plt.ylabel(\"Explication de la variance\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec825a-9480-429e-82c6-3bddc4715a0f",
   "metadata": {},
   "source": [
    "Avec la méthode dite « du coude », nous serions plus tenté·es de ne sélectionner que les deux premières composantes principales, ce qui impliquerait de ne conserver que 53 % de la variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e303d-99d7-4dcb-b091-23e52c9f9d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Étape 5 : charger la matrice des composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b163efd-a589-4ebf-a4b4-db2639ddd4e1",
   "metadata": {},
   "source": [
    "Si nous transformons nos données à l’aide des quatre premières composantes principales, nous obtenons la projection suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5acdf6-7e07-4f41-96f7-999aac919d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = np.dot(X_scaled, eigenvectors[:, :n_components])\n",
    "\n",
    "# into dataframe\n",
    "pca = pd.DataFrame(\n",
    "    data=X_projected,\n",
    "    columns=[ f\"PC{n}\" for n in range(1, n_components + 1) ],\n",
    "    index=index\n",
    ")\n",
    "\n",
    "display(pca.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc48894-4f41-4f95-8553-ef36528081de",
   "metadata": {},
   "source": [
    "### Étape 7 : analyser le lien entre les variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e222f-add8-41c4-8fe2-8860c8312d09",
   "metadata": {},
   "source": [
    "Un enjeu fort de l’ACP consiste à étudier le lien entre les variables pour :\n",
    "\n",
    "- regrouper les variables fortement corrélées en variables synthétiques ;\n",
    "- supprimer des variables mal représentées afin de réduire la dimensionnalité du jeu de données.\n",
    "\n",
    "Un outil très largement utilisé pour réaliser cette analyse est le cercle des corrélations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424021a-9e06-414d-afcc-a14effd7830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a variable factor map for the first two dimensions\n",
    "(fig, ax) = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# for each variable (PS_FSAFEN, JE_EMPL…)\n",
    "for i in range(0, len(X_scaled.columns)):\n",
    "    ax.arrow(\n",
    "        0, 0, # start the arrow at the origin     \n",
    "        eigenvectors[i, 0],\n",
    "        eigenvectors[i, 1],\n",
    "        head_width=0.1,\n",
    "        head_length=0.1\n",
    "    )\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.text(\n",
    "        eigenvectors[i, 0] + 0.05,\n",
    "        eigenvectors[i, 1] + 0.05,\n",
    "        X_scaled.columns.values[i],\n",
    "        ha='center', va='center'\n",
    "    )\n",
    "\n",
    "# a unit circle\n",
    "circle = np.linspace(0, 2 * np.pi, 500)\n",
    "plt.plot(np.cos(circle), np.sin(circle), color=\"tab:blue\", linestyle='dashed', alpha=0.7)\n",
    "\n",
    "# axis delimiters\n",
    "plt.plot(np.linspace(-1,1), np.linspace(0,0), color=\"tab:gray\", linestyle='dashed', alpha=.5)\n",
    "plt.plot(np.linspace(0,0), np.linspace(-1,1), color=\"tab:gray\", linestyle='dashed', alpha=.5)\n",
    "\n",
    "ax.set_title('Cercle des corrélations')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f864b0-857a-4e08-bd47-e30c768ea350",
   "metadata": {},
   "source": [
    "La représentation du cercle des corrélations nous montre que certaines variables sont corrélées positivement ou négativement à l’un des deux axes d’inertie projetés.\n",
    "\n",
    "Par exemple, les variables *PS_REPH*, *JE_LTUR* et *WL_EWLH* sont corrélées positivement à la première composante principale quand les variables *ES_STCS*, *EQ_WATER*, *HS_LEB* ou encore *SC_SNTWS* lui sont corrélées négativement. Sur le second axe, *PC2*, on remarque que les variables *WL_EWLH*, *PS_REPH* et *EQ_WATER* lui sont corrélées positivement, alors que les variables *ES_EDUA* et *ES_STCS* ou *JE_LTUR* lui sont corrélées négativement.\n",
    "\n",
    "À ce stade, il est important d’analyser les corrélations entre les variables en se posant des questions comme : qu’est-ce qui relie le taux d’homicides (*PS_REPH*), des horaires de travail lourds (*WL_EWLH*) et le taux de chômage de longue durée (*JE_LTUR*) ? La notion derrière serait peut-être **la qualité de vie en société**. On comprend alors facilement la corrélation négative qui la lie avec le sentiment de sécurité (*PS_FSAFEN*), la qualité de l’eau (*EQ_WATER*), la satisfaction à l’égard de la vie (*SW_LIFS*) ou encore la qualité du réseau social (*SC_SNTWS*) : si la qualité de vie en société augmente, les indicateurs de sa détérioriation diminueront.\n",
    "\n",
    "Pour la seconde composante principale (PC2), la relation entre le niveau d’instruction (*ES_EDUA*) et les compétences des élèves (*ES_SCTS*) est manifeste. Cette composante semble principalement refléter la **performance du système éducatif**. En intégrant le taux de chômage de longue durée (*JE_LTUR*) dans notre analyse, on observe également une dimension socio-économique, bien que son impact sur PC2 soit moins marqué. Cette interprétation est confirmée par la contribution positive d’autres variables, telles que *WL_EWLH*, *PS_REPH* et *EQ_WATER* à PC2, qui indiquent que des conditions sociales ou économiques moins favorables sont associées à des valeurs élevées de cette composante.\n",
    "\n",
    "Si nous devions résumer le cercle des corrélations des deux premières composantes principales, on pourrait dire que les pays de l’OCDE se distinguent principalement selon un critère de **qualité de vie en société**, fortement influencé par les conditions de travail et le sentiment de sécurité. Une seconde tendance émerge également : **la réussite scolaire** semble être un moteur de la réussite sociale, contribuant à se prémunir contre l'insécurité et les mauvaises conditions de vie.\n",
    "\n",
    "On peut également exprimer les relations entre les variables sous forme numérique. Les composantes principales étant les vecteurs propres calculés, regardons le premier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2a205-ef8b-4d74-883d-4af268ffe33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc9dbf-0188-4198-a7bb-f3c5c626c4c4",
   "metadata": {},
   "source": [
    "De là, nous pouvons en déduire que PC1 est une combinaison de :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece597fc-d8ed-40d8-ba6c-d57cecb96948",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eigenvector in enumerate(eigenvectors[:, 0]):\n",
    "    print(f\"{'' if str(eigenvector).startswith('-') else '+'}{round(eigenvector, 2)} * {X_scaled.columns[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65fa53-01cf-4ec2-aeb3-e8df830278c1",
   "metadata": {},
   "source": [
    "Avantage non négligeable, d’un coup d’œil nous repérons les variables qui ont le plus d’impact sur nos composantes principales !\n",
    "\n",
    "Pour aller plus loin, devrions nous étudier également les corrélations entre PC2 et PC3, puis entre PC3 et PC4 ? Et peut-être même entre PC1 et PC3, puis PC4 ? Pas nécessairement : moins les composantes expliquent la variance, plus il sera difficile d’en tirer une explication valable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7494e8-8f76-4e3a-838f-8fabc1644f60",
   "metadata": {},
   "source": [
    "### Étape 8 : analyser la variabilité des observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd15a32-a127-40f3-b8fa-a913b5914681",
   "metadata": {},
   "source": [
    "Le second objectif de l’ACP est d’analyser la manière dont les observations varient en fonction des facteurs. Pour ce faire, nous projetons un nuage de points sur les deux premiers axes d’inertie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff7e4d-3c74-45d7-a8e8-cd4a681aad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delimiters are considered right included 'right=True':\n",
    "# (76-81] (81-84] (84-88]\n",
    "count, mean, std, minimum, first, second, third, maximum = X.HS_LEB.describe()\n",
    "bins = [minimum, first, second, maximum]\n",
    "labels = [\"-81 ans\", \"81-84 ans\", \"+84 ans\"]\n",
    "\n",
    "# segmentation\n",
    "pca[\"HS_LEB_cat\"] = pd.cut(X.HS_LEB, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "palette = ['tab:orange', 'tab:olive', 'tab:blue']\n",
    "fig = sns.scatterplot(data=pca, x=\"PC1\", y=\"PC2\", hue=\"HS_LEB_cat\", palette=palette)\n",
    "fig.set_title(\"Projection des pays de l’OCDE groupés selon l’espérance de vie\")\n",
    "fig.set_xlabel(\"PC1 (39,03 %)\")\n",
    "fig.set_ylabel(\"PC2 (14.26 %)\")\n",
    "\n",
    "for i in range(0, len(pca)):\n",
    "    plt.text(\n",
    "        pca.PC1.iloc[i] + .05,\n",
    "        pca.PC2.iloc[i] + .05,\n",
    "        pca.index[i]\n",
    "    )\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e440fe5-53f3-4729-979d-adb0e9a290a4",
   "metadata": {},
   "source": [
    "Pour construire le graphique, nous avons recodé la variable *HS_LEB* qui, dans les données d’origine, enregistre en nombre d’années l’espérance de vie à la naissance en variable catégorielle afin de regrouper les pays dans trois catégories définies en fonction des quartiles de la série. Il en ressort que la projection sur les deux premiers axes d’inertie cartographie les pays à plus faible espérance de vie dans la partie droite de l’axe des abscisses. Pour interpréter ce classement, il convient de se référer au cercle des corrélations et à l’analyse que nous avons fournie.\n",
    "\n",
    "D’après les deux graphiques, on s’attend à ce que la Russie, la Turquie, le Brésil, la Colombie et le Mexique présentent des scores élevés en *PS_REPH* (taux d’homicide) et *WL_EWLH* (horaires de travail lourds) et de mauvais en *PS_FSAFEN* (se sentir en sécurité la nuit quand on marche seul), *EQ_WATER* (qualité de l’eau) et *SW_LIFS* (satisfaction à l’égard de la vie) comme ces variables sont fortement corrélées positivement à PC1. À l’inverse, on s’attend à ce que la Norvège, la Suède, la Finlande et l’Islande, situées du côté opposé sur l’axe des abscisses, affichent de meilleurs résultats sur ces indicateurs (c’est-à-dire des scores plus faibles).\n",
    "\n",
    "Pour expliquer maintenant la variabilité sur l’axe des ordonnées, prenons deux pays avec une abscisse similaire, la Lituanie et les États-Unis, et demandons-nous ce qui les différencie. Là encore, le cercle des corrélations nous apprend que la Lituanie, dans la partie inférieure de l’axe délimité par PC2, devrait avoir un bon score en *ES_EDUA* et *ES_STCS* comme ces indicateurs lui sont corrélés négativement (plus leur valeur augmente, plus PC2 diminue), et un faible en *EQ_WATER*, *WL_EWLH* ou *PS_REPH* quand ce serait l’opposé pour les États-Unis.\n",
    "\n",
    "Les données pour les deux pays coïncident avec notre analyse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de50b98-df21-43f2-930b-0362220506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[\"ES_EDUA\", \"ES_STCS\", \"EQ_WATER\", \"WL_EWLH\", \"PS_REPH\"]][(X.index == \"LTU\") | (X.index == \"USA\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddf483-aab2-452a-8fd6-616de273559c",
   "metadata": {},
   "source": [
    "Attention toutefois, quand on se focalise sur une observation particulière, les données peuvent différer de la tendance générale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55335a2f-0d4d-49a3-bff1-f2c09efb6557",
   "metadata": {},
   "source": [
    "## Alternative à la décomposition en éléments propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc1187-19dd-4881-8946-7b175288b9a3",
   "metadata": {},
   "source": [
    "Même s’il est fréquent de calculer les vecteurs propres à partir de la diagonalisation de la matrice de covariance, il peut être plus simple de le faire directement à partir d’une décomposition en valeurs singulières (SVD pour *Singular Value Decomposition*) de la matrice de données. En plus de supprimer une étape intermédiaire, cette méthode peut être plus robuste mathématiquement : dans le cas d’un ensemble de données volumineux ou de variables fortement corrélées, les erreurs d’arrondi dans la matrice de covariance peuvent affecter la précision de l’analyse.\n",
    "\n",
    "En travaillant à partir des données normalisées, nous pouvons appliquer la méthode `.linalg.svd()` de *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f9fe3-e3a2-468d-b9a8-048b94693a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = np.linalg.svd(X_scaled.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b38c1-1749-4f91-8119-0707084455c9",
   "metadata": {},
   "source": [
    "Cette méthode permet de récupérer les vecteurs propres organisés en colonnes dans `U`, les valeurs singulières dans `S` et `VT` la transposée de la matrice des vecteurs propres. À partir de là, nous pouvons calculer la variance expliquée selon la formule :\n",
    "\n",
    "$$\n",
    "\\text{variance}_i = \\frac{S^2_i}{\\sum S^2_j}\n",
    "$$\n",
    "\n",
    "Où $S_i$ est la *i*-ème valeur singulière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d47681-8acb-4f58-93e3-5b73378a30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = (S ** 2) / np.sum(S ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43948b3b-91fe-473e-8acf-1515da609edf",
   "metadata": {},
   "source": [
    "Autre bénéfice de la SVD, les valeurs singulières sont déjà retournées par ordre décroissant, aussi aucun besoin de les trier ! On ne sera pas non plus déstabilisés de remarquer que la somme des taux de variance est égale à 100 % :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf75a3-fe63-43a2-bb86-6a6961130055",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c41f2-ee48-4258-ac01-2dae0ccad66d",
   "metadata": {},
   "source": [
    "On peut ensuite calculer la variance cumulative grâce à la méthode `.cumsum()` de *Numpy* afin de calculer le nombre de composantes à conserver :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39c706-e85b-4f6a-a033-52e71ec4b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(variance)\n",
    "threshold = 0.70\n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1 # because index starts at 0\n",
    "\n",
    "print(f\"Les {n_components} premières composantes principales expliquent {cumulative_variance[n_components - 1]:.2%} de la variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231569b-a6ef-4d9d-828b-799ff7ecdc1b",
   "metadata": {},
   "source": [
    "Il nous reste à récupérer les quatre premières lignes de la matrice `VT`, qui correspondent aux vecteurs propres des quatre premières composantes principales, puis à projeter les données sur ces composantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659a2da-cfe9-48c3-bb3a-8c9cee955eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "VT_k = VT[:n_components, :]\n",
    "\n",
    "pca = X_scaled @ VT_k.T # matrix product\n",
    "pca.columns = [ f\"PC{n}\" for n in range(1, n_components + 1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24577c-3182-4612-af28-2a4ed8eb4fe9",
   "metadata": {},
   "source": [
    "Les graphiques se construisent de la même façon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4ea74-44df-49b0-949b-48757739c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recoding HS_LEB\n",
    "pca[\"HS_LEB_cat\"] = pd.cut(X.HS_LEB, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# a figure with 2 subplots\n",
    "(fig, axes) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "######################\n",
    "# Correlation circle #\n",
    "######################\n",
    "for i in range(0, len(X_scaled.columns)):\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].arrow(\n",
    "        0, 0,\n",
    "        VT.T[i, 0],  # PC1\n",
    "        VT.T[i, 1],  # PC2\n",
    "        head_width=0.1,\n",
    "        head_length=0.1,\n",
    "    )\n",
    "\n",
    "    # column names\n",
    "    axes[0].text(\n",
    "        VT.T[i, 0] + 0.05,\n",
    "        VT.T[i, 1] + 0.05,\n",
    "        X_scaled.columns.values[i],\n",
    "        ha='center', va='center'\n",
    "    )\n",
    "\n",
    "# unit circle\n",
    "circle = np.linspace(0, 2 * np.pi, 500)\n",
    "axes[0].plot(np.cos(circle), np.sin(circle), color=\"tab:blue\", linestyle='dashed', alpha=0.7)\n",
    "\n",
    "# axes\n",
    "axes[0].plot(np.linspace(-1, 1), np.linspace(0, 0), color=\"tab:gray\", linestyle='dashed', alpha=.5)\n",
    "axes[0].plot(np.linspace(0, 0), np.linspace(-1, 1), color=\"tab:gray\", linestyle='dashed', alpha=.5)\n",
    "\n",
    "# titles and labels\n",
    "axes[0].set_title('Cercle des corrélations')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "##################\n",
    "# OCDE countries #\n",
    "##################\n",
    "palette = ['tab:orange', 'tab:olive', 'tab:blue']\n",
    "sns.scatterplot(data=pca, x=\"PC1\", y=\"PC2\", hue=\"HS_LEB_cat\", palette=palette, ax=axes[1])\n",
    "\n",
    "# titles and labels\n",
    "axes[1].set_title(\"Projection des pays de l’OCDE groupés selon l’espérance de vie\")\n",
    "axes[1].set_xlabel(\"PC1 (39,03 %)\")\n",
    "axes[1].set_ylabel(\"PC2 (14.26 %)\")\n",
    "\n",
    "# country names\n",
    "for i in range(0, len(pca)):\n",
    "    axes[1].text(\n",
    "        pca.PC1.iloc[i] + .05,\n",
    "        pca.PC2.iloc[i] + .05,\n",
    "        pca.index[i]\n",
    "    )\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "# plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a018b0-1af6-4977-b185-eb8aaf1d486a",
   "metadata": {},
   "source": [
    "## Réaliser une ACP automatiquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ec8d5-e041-4ef1-a248-36b98af4321f",
   "metadata": {},
   "source": [
    "La librairie *Scikit-Learn* permet de réaliser facilement une ACP à partir de données normalisées. La classe `PCA` du module `.decomposition` se charge de tous ces aspects :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc6a88-16b8-40b8-abcb-05c55bf1b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean copy of data\n",
    "data_b = pd.DataFrame.copy(df).dropna(ignore_index=True)\n",
    "#data_b = data_b.reset_index(drop=True)\n",
    "\n",
    "# steps in the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=4))\n",
    "])\n",
    "\n",
    "# do the job!\n",
    "_ = pipe.fit_transform(data_b[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e503024-c8da-47bf-b225-a4f2aecf24f3",
   "metadata": {},
   "source": [
    "Le paramètre `n_components` permet de limiter le nombre de facteurs à 4 comme dans notre ACP semi-guidée. Pour les révéler, il convient d’appeler l’attribut spécial `.components_`. Si l’on compare le premier facteur calculé par *Scikit-Learn* et notre premier vecteur propre calculé à la main, nous observons une forte correspondance en termes de direction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ef458-4e64-4249-94b1-6b49d1e2612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    abs(pipe[\"pca\"].components_[0].round(10)) == abs(eigenvectors[:, 0].round(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e4e21-4288-4e91-9d81-331c014ac337",
   "metadata": {},
   "source": [
    "Si nous comparons les valeurs absolues des vecteurs, c’est parce qu’ils sont opposés mais colinéaires, ce qui reflète bien la propriété des vecteurs propres : ils sont définis à une constante multiplicative près.\n",
    "\n",
    "Un autre attribut intéressant, `.explained_variance_`, permet de ressortir le total de variance expliqué par chaque composante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eca448-d6a7-4c15-81f2-334e6b553016",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[\"pca\"].explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a10c60-b944-4395-9d56-fa3a668f852d",
   "metadata": {},
   "source": [
    "Un résultat similaire à ce que nous avions calculé à la main :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0341869-4ee7-4d4b-80f1-24c9354dc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95e7ff-6606-45ab-a3e0-3f017d67ac21",
   "metadata": {},
   "source": [
    "Aucun mystère à ce que les ratios soient également identiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707182b-d323-4152-9da6-c43532e483dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pipe[\"pca\"].explained_variance_ratio_.tolist(),\n",
    "    [ eigenvalue / sum(eigenvalues) for eigenvalue in eigenvalues ][:4],\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac5a75-fb13-4c5d-88ce-a6999301d8e3",
   "metadata": {},
   "source": [
    "En quelques manipulations, nous avons réalisé une ACP qui nous a permis de réduire la représentation de nos données d’une matrice à 13 dimensions à une matrice à seulement 4 dimensions !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8cb5cf-d21b-48eb-96b3-94a83dcd6cae",
   "metadata": {},
   "source": [
    "# Gérer des données numériques dans un projet d’apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c599a71-0737-4bf5-b599-0f68a79536a5",
   "metadata": {},
   "source": [
    "C’est bien connu, les machines adorent les nombres. Ils ont cela de commode qu’ils se prêtent mieux aux calculs que des symboles comme *pomme* ou *Pommes*. Dans un projet de *machine learning*, les données fournies aux algorithmes d’apprentissage devront toutes être représentées sous forme numérique et, avant d’aborder la manière de vectoriser des chaînes de caractère, il est plus sage de commencer par la manipulations des nombres, qu’ils soient entiers ou décimaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb74d3b-bb60-4a1d-b5c6-9943a3062e69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Identifier une variable aléatoire quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914a9e5-8f59-4917-b881-a3bcb9fb5e77",
   "metadata": {},
   "source": [
    "En statistiques, une variable aléatoire est l’une des caractéristiques d’une observation. Elle peut se représenter de manière rudimentaire sous forme de tableau à deux dimensions :\n",
    "\n",
    "|Sexe|Taille|\n",
    "|-|:-:|\n",
    "|F|180|\n",
    "|M|172|\n",
    "|M|167|\n",
    "\n",
    "Le tableau étant composé de trois lignes et de deux colonnes, il est réputé présenter deux caractéristiques pour trois observations dans une structure de dimensions $3\\times 2$.\n",
    "\n",
    "Sans se tromper, la variable aléatoire *Sexe* n’est pas de type numérique quand la variable *Taille*, elle, l’est.\n",
    "\n",
    "Comment s’en assurer avec Python ? La propriété `dtypes` d’un *data frame* permet de s’en assurer rapidement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e080e-3ebd-4453-bd64-835eae5c87dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'gender': ['F', 'M', 'M'],\n",
    "    'height': [180, 172, 167]\n",
    "})\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f499d-2e93-4a5c-9025-ba2231da5390",
   "metadata": {},
   "source": [
    "La variable `height` est bien de type numérique. Pour autant, une variable aléatoire représentée sous forme numérique est-elle systématiquement quantitative, au sens statistique du terme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dcd1b-f766-465c-a4a6-40f2088a79e2",
   "metadata": {},
   "source": [
    "### Numérique ≠ quantitatif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e745fc2-561b-4507-b85d-aabe1af9fd3d",
   "metadata": {},
   "source": [
    "Pour qu’une variable numérique soit considérée comme quantitative, elle est censée pouvoir exprimer une quantité. Après avoir ajouté les années de naissance des individus, la propriété `dtypes` signale que la variable `birth`, conformément à l’intuition, est bien de type numérique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb769a-5803-40d6-8350-5952dbfdda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"birth\"] = [1983, 2001, 2011]\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d9f5f-3a97-43d9-855a-3d543111cc28",
   "metadata": {},
   "source": [
    "La variable `birth` est-elle quantitative ? Pour le déterminer facilement, il faut se poser la question de savoir si cela fait sens de cumuler les valeurs consignées. Quand les tailles des individus peuvent former une somme pour obtenir ensuite une moyenne, est-ce raisonnable d’additionner des années de naissance ? Le calcul de la moyenne arithmétique des années de naissance donne pour résultat : $1998,\\overline{3}$. Est-il logique de poser ce genre de question ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1d256-fe66-456a-a2a4-0193ecd3ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"birth\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6df12-cc12-4407-815b-b43f696385b5",
   "metadata": {},
   "source": [
    "Eh bien, en fait, oui. Si la moyenne ici n’est pas à proprement parler intéressante, il est légitime de se demander quelle est la médiane des années de naissance des individus interrogés, elle pourrait expliquer certains résultats. En revanche, il n’en serait pas de même des codes postaux de leur lieu d’habitation ou de leurs numéros de sécurité sociale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f7b6d-2f20-45fd-a3c7-a4b031918926",
   "metadata": {},
   "source": [
    "### Quantitative discrète ou continue ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f12fa-2005-4607-83e5-8c0be8fbdd53",
   "metadata": {},
   "source": [
    "La différence est encore parfois plus subtile entre les variables aléatoires quantitatives discrètes et continues. Dans l’exemple avec les années, la coutume est de considérer leur valeur comme une discrétion du temps qui, lui, est continu ; d’un autre côté, si le jeu de données comporte une représentation décimale des années, où $1998,55$ équivaudrait au 15 juin 1998, rien n’interdit de considérer qu’il existe alors une continuité.\n",
    "\n",
    "Parfois, une distinction commode véhiculée par certaines sources consiste à considérer une donnée représentée par un élément de l’ensemble $\\mathbb{Z}$ comme discrète, mais appliquer cette préconisation sans réflexion peut amener à des erreurs d’interprétation. L’âge, par exemple, est souvent noté sous forme d’entiers naturels. Il s’agit d’une convention : personne ne passe réellement de $x$ ans à $x+1$ ans sans vivre les intervalles, si ? Pour s’en assurer tout à fait, demandons-nous si le calcul de l’âge moyen des individus d’une enquête ferait sens.\n",
    "\n",
    "En revanche, si au moment de la préparation des données de l’enquête, on établissait des classes d’âge (moins de 18 ans, plus de 35 ans etc.), la variable deviendrait discrète, et qualitative. D’autres données posent les mêmes difficultés, comme la taille, ou le poids, qui, comme elles sont exprimées avec une unité et ne peuvent prendre qu’une valeur isolée, sont cataloguées généralement comme valeurs discrètes. Pourtant, la taille et le poids **d’un individu** peuvent prendre, si mesurés précisément, n’importe quelle valeur dans un intervalle (p. ex. : de 0 à 300 cm) et exprimer ainsi une continuité.\n",
    "\n",
    "Comment, alors, être sûr·es de faire le bon choix ? Dans le doute, une bonne option est de se reposer sur la représentation graphique de la variable en ballottage :\n",
    "- Un diagrammes en barres pour une quantitative discrète ;\n",
    "- un histogramme pour une quantitative continue.\n",
    "\n",
    "Dans notre petit jeu de données, nous avons catalogué la variable *année* comme une quantitative discrète. Représentons-là avec un diagramme en barres, puis un histogramme :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02e5ed-7d35-4514-9ae9-c03f1f287760",
   "metadata": {},
   "source": [
    "![Représentation des années avec un diagramme en barres](./images/birth-barplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4918e0-de70-495a-835a-cbccb557016c",
   "metadata": {},
   "source": [
    "![Représentation des années avec un histogramme](./images/birth-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8876458-428a-4f4f-b19e-77cc430d9561",
   "metadata": {},
   "source": [
    "Quand le premier graphique parle de lui-même, le second peine à convaincre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c9a52-6afb-40fb-a897-a49a4f65d13e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## La délicate question du pré-traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132fc43-5d40-4152-abfe-7d435e3d9921",
   "metadata": {},
   "source": [
    "Un algorithme de *machine learning* est grandement dépendant de la qualité des données sur lesquelles il est entraîné. Pour cette raison, la phase de pré-traitement (*pre-processing*) est cruciale. Il s’agira de ne laisser aucune donnée manquante dans le jeu de données et d’harmoniser les grandeurs des variables numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729fb68-fe6c-46ef-81a5-804b0776d115",
   "metadata": {
    "tags": []
   },
   "source": [
    "### La chasse aux données manquantes…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf0aba-018d-46ff-8689-4f7870783483",
   "metadata": {},
   "source": [
    "Comment repérer les données manquantes dans un *dataset* et, surtout, comment les gérer ? Pour une seule variable sans valeur, faut-il supprimer toute l’observation ? Et s’il est question de la remplacer, quelle valeur choisir ?\n",
    "\n",
    "Commençons par charger le jeu de données sur le recensement des manchots de l’Antarctique et affichons un résumé du *data frame* obtenu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bb4a7-55ab-45e7-96c7-5e3c2597885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/penguin-census-numerical-features.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f3de5-6f37-4d9e-a771-55efc54ab1e3",
   "metadata": {},
   "source": [
    "Il en ressort que les 344 observations ne sont pas toutes complètes : pour deux d’entre elles il manque les caractéristiques physiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbcee8-a2e2-4225-9965-ae260869b307",
   "metadata": {},
   "source": [
    "#### Supprimer les observations avec données manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8cae9-1900-4e81-b8ba-e175ba8a8b96",
   "metadata": {},
   "source": [
    "La première stratégie consiste à supprimer les observations concernées. Après tout, deux observations sur 344 ne représente qu’une perte de 0,5 % de l’ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66b937-5a60-4cd1-81ec-5b411c737f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863d69d-6eca-41ad-9029-ee644cfa579d",
   "metadata": {},
   "source": [
    "Dans ce cas de figure, l’argument se tient, parce que les valeurs manquaient toutes pour les deux mêmes observations. Mais si elles avaient concerné des manchots différents, on aurait alors supprimé 8 observations, soit 2 % de l’ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaaa82-7de9-45fd-b3f4-bb38cacb6789",
   "metadata": {},
   "source": [
    "#### Remplacer par une valeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d024fe1-db93-4f5d-9238-e90f445d716b",
   "metadata": {},
   "source": [
    "Plusieurs options se présentent : remplacer par des zéros, par une valeur fixe, par la moyenne, par la médiane ou encore par la valeur la plus représentée. Chacune de ces options a ses avantages et ses inconvénients.\n",
    "\n",
    "*Scikit-Learn* dispose d’une classe `SimpleImputer` pour réaliser n’importe laquelle de ces options. Elle prend un paramètre `strategy`, dont les valeurs sont à choisir parmi : `mean` (option par défaut), `median`, `most_frequent`, `constant`. Si la stratégie `constant` est sélectionnée, il faut indiquer la valeur dans un paramètre `fill_value`.\n",
    "\n",
    "Remplaçons dans un premier temps les valeurs manquantes par une valeur fixe, le zéro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a0cfa-35fd-4f03-ac6e-db48594c6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# copy of the data frame\n",
    "data = pd.DataFrame.copy(df)\n",
    "# new instance\n",
    "imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "# fit the imputer to data\n",
    "imputer.fit(data)\n",
    "# create a matrice\n",
    "X = imputer.transform(data)\n",
    "\n",
    "# missing values for the 4th sample are now fixed to 0\n",
    "X[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31af55d-7d37-406d-9ba4-b5a36b2c5d90",
   "metadata": {},
   "source": [
    "Remplaçons maintenant par la valeur médiane :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56765-cb5f-443c-a0a9-b452639857bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "data = pd.DataFrame.copy(df)\n",
    "# only numerical features\n",
    "data = data.drop(columns=\"species\")\n",
    "# an imputer with median strategy\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# shortcut for fit then transform\n",
    "X = imputer.fit_transform(data)\n",
    "\n",
    "# values for the 4th sample\n",
    "X[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9cc8f-b093-405b-85d8-bdf70c42a927",
   "metadata": {
    "tags": []
   },
   "source": [
    "### … et aux données aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324792f-6317-41a3-aab6-613eed13ed84",
   "metadata": {},
   "source": [
    "Les mêmes stratégies peuvent s’appliquer aux données aberrantes, les valeurs extrêmes pouvant affecter négativement certaines mesures. Il s’agit parfois d’un zéro surnuméraire ou du déplacement de la virgule dans la notation décimale d’une quantité. La moyenne arithmétique est par exemple est très sensible à ces erreurs. Et il en va de même des algorithmes d’apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736fd39-6991-45ea-be26-4f67c818f055",
   "metadata": {},
   "source": [
    "### Mise à l’échelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01460e-6dfd-42e3-a94b-c069b0a5fe2e",
   "metadata": {},
   "source": [
    "Les données d’une observation font rarement toutes référence à une échelle commune. L’âge d’un individu sera compris entre 0 et 100, sa taille entre 0 et 200, son score de satisfaction entre 0 et 10, la numération de ses globules rouges entre 3 000 000 et 6 000 000 etc. Il faut savoir que les algorithmes d’apprentissage sont sensibles à la différence entre les grandeurs et fourniront des prédictions de mauvaise qualité si certaines variables sont réparties dans un espace bien plus vaste que les autres.\n",
    "\n",
    "La mise à l’échelle consiste alors à réduire leur variance ou leur valeur absolue. Plusieurs méthodes existent et, parmi les plus utilisées, citons la standardisation et la normalisation.\n",
    "\n",
    "Avant toutes choses, récupérons une variable descriptive des manchots, la longueur du bec, en remplaçant les valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5089a7e-f8a3-4449-90d2-a4eb051720c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_length_median = df[\"bill_length_mm\"].median()\n",
    "bill_length = df[\"bill_length_mm\"].fillna(bill_length_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98912bc7-c7b1-41f5-a5f9-2e9ea51771f3",
   "metadata": {},
   "source": [
    "#### La standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac119f-a851-431b-b26c-949ed2f5a1ab",
   "metadata": {},
   "source": [
    "La standardisation (*Z score normalization*) consiste à centrer la variable autour de 0 de telle manière que son écart-type soit égal à 1. La formule donne avec $\\mu$ pour la moyenne et $\\sigma$ pour l’écart-type :\n",
    "\n",
    "$$f(x) = \\frac{x − \\mu}{\\sigma}$$\n",
    "\n",
    "Avant de centrer-réduire la variable *bill_length_mm*, l’affichage de sa moyenne et de son écart-type donne 43,92 pour la première et 5,44 pour la seconde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b02eb-d85b-4bb5-a0c1-e97d03754ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Mean value: { bill_length.mean().round(2) }\",\n",
    "    f\"Standard deviation: { bill_length.std().round(2) }\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7021e34-c512-4eab-bbd6-7e2d4774ef8a",
   "metadata": {},
   "source": [
    "Concrètement, l’opération de standardisation va d’abord soustraire la moyenne puis diviser ensuite le résultat par l’écart-type. Si l’on effectue ce calcul à la main, on obtient bien une moyenne à 0 et un écart-type de 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6101999-82f4-491f-96ee-40a8a2f09c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_length_scaled = list()\n",
    "\n",
    "[\n",
    "    bill_length_scaled.append(\n",
    "        (n - bill_length.mean()) / bill_length.std()\n",
    "    )\n",
    "    for n in bill_length\n",
    "]\n",
    "bill_length_scaled = pd.Series( (value for value in bill_length_scaled) )\n",
    "\n",
    "print(\n",
    "    f\"Mean value: { bill_length_scaled.mean().round(2) }\",\n",
    "    f\"Standard deviation: { bill_length_scaled.std().round(2) }\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eae630-d67f-45a5-8d69-3c10da031725",
   "metadata": {},
   "source": [
    "Il existe heureusement une classe `StandardScaler` dans *Scikit-Learn* pour effectuer l’opération plus simplement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da43185-4b4c-4208-affc-f7c4852838f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# reshape Serie to match 2d array\n",
    "bill_length_scaled = scaler.fit_transform(bill_length.values.reshape(-1, 1))\n",
    "\n",
    "print(\n",
    "    f\"Mean value: { bill_length_scaled.mean().round(2) }\",\n",
    "    f\"Standard deviation: { bill_length_scaled.std().round(2) }\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93cd73-a75b-48d3-92a2-dbcf1639e019",
   "metadata": {},
   "source": [
    "#### La normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297986b4-6260-466a-8c10-d9559a108d55",
   "metadata": {},
   "source": [
    "Plus simple à appréhender, la normalisation Min-Max (*Min-Max normalization*) est une méthode qui va soustraire à chaque valeur la minimale puis la diviser ensuite par l’écart maximal de la série. Comme la formule est basée sur les extrêmes, elle est particulièrement sensible aux données aberrantes.\n",
    "\n",
    "$$f(x) = \\frac{x − min(x)}{max(x) − min(x)}$$\n",
    "\n",
    "Le résultat n’est plus une variable centrée réduite, mais une variable dont les valeurs seront contenues dans un intervalle $[0, 1]$.\n",
    "\n",
    "En reprenant l’exemple précédent sur la longueur du bec des manchots, nous appliquons cette fois-ci une classe `MinMaxScaler` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c16b3b-5d41-4a2e-9169-edc937fd2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "bill_length_scaled = scaler.fit_transform(bill_length.values.reshape(-1, 1))\n",
    "\n",
    "bill_length_scaled[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4612cc-a51a-4686-8bdd-4921ae068a4b",
   "metadata": {},
   "source": [
    "# Les modèles pour l’apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef28a8-6424-4372-adf1-169784168bf4",
   "metadata": {},
   "source": [
    "Pour mémoire, les modèles pour l’apprentissage supervisé sont classés en fonction de la tâche à réaliser :\n",
    "- La **régression**, quand la prévision porte sur une valeur continue comme le salaire d’un·e étudiant·e en fin de formation ;\n",
    "- ou la **classification** lorsqu’il s’agit de prédire l’appartenance à une classe.\n",
    "\n",
    "Certains modèles sont réservés à l’une ou l’autre de ces tâches quand les autres peuvent servir aux deux. Dans ce calepin, nous parcourons les plus populaires sans pour autant rentrer dans les détails.\n",
    "\n",
    "Avant de commencer, chargeons les librairies nécessaires et constituons des ensembles de données pour les différentes tâches à traiter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e8a9c-91d7-4179-b4df-46a44552d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_selector as col_selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "###################\n",
    "# Regression task #\n",
    "###################\n",
    "\n",
    "# for deterministic purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# hundred points\n",
    "X = np.random.rand(100, 1)\n",
    "\n",
    "# polynomial function: y = 2 - 3x + 3x^2 + 5x^3\n",
    "y = 2 - 3 * X + 3 * X ** 2 + 5 * X ** 3 + np.random.rand(100, 1)\n",
    "\n",
    "#######################\n",
    "# Classification task #\n",
    "#######################\n",
    "\n",
    "# penguin census\n",
    "df = pd.read_csv(\"./data/penguin-census.csv\").dropna()\n",
    "\n",
    "# variables\n",
    "target = \"species\"\n",
    "features = [\"flipper_length_mm\", \"body_mass_g\", \"sex\"]\n",
    "\n",
    "# dataset\n",
    "X_b = df[features]\n",
    "y_b = df[target]\n",
    "\n",
    "# selectors\n",
    "num_col_selector = col_selector(dtype_exclude=object)\n",
    "cat_col_selector = col_selector(dtype_include=object)\n",
    "\n",
    "# num & cat cols\n",
    "num_cols = num_col_selector(X_b)\n",
    "cat_cols = cat_col_selector(X_b)\n",
    "\n",
    "# preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num-transformers\", StandardScaler(), num_cols),\n",
    "        (\"cat-transformers\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b22ba-a7f8-4426-a1cc-9cd959540bf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Avant-propos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274c526-70ac-46cd-b73e-f098b91dc2a3",
   "metadata": {},
   "source": [
    "Les modèles d’apprentissage de *Scikit-Learn*, programmés en différentes classes, sont conçus autour du même esprit de cohésion. Chaque classe expose notamment les mêmes méthodes suivantes :\n",
    "- `fit()` pour ajuster le modèle sur des données distribuées dans deux paramètres `x` et `y` ;\n",
    "- `predict()` pour effectuer des prédictions sur des données ;\n",
    "- `score()` pour obtenir une évaluation des prédictions obtenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ece45f-2309-48ce-9dbf-7e26a904a2ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les modèles pour les tâches de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6edcf5-f27c-4c3b-839d-68143ffc91e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### La régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d9ba4-d87a-4a46-a36a-abec3fed6c06",
   "metadata": {},
   "source": [
    "Sans doute la méthode la plus populaire en apprentissage supervisé, la régression linéaire consiste à trouver la fonction affine $y$ d’une variable explicative $x$. Elle permet de comprendre rapidement la relation entre deux variables. Pour autant, sa sensibilité aux données aberrantes et aux *outliers* peut l’empêcher d’observer une corrélation ; elle se révèle alors une hypothèse trop simple pour les données. On parle alors de sous-ajustement (*underfitting*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670852c-b4ce-4213-90f3-0533fa9d8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "_ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e66302-3bd8-47b1-9a5c-ad401163c5fa",
   "metadata": {},
   "source": [
    "Une fois le modèle entraîné, il est possible d’accéder au coefficient directeur de la droite et à son ordonnée à l’origine grâce aux attributs `coef_` et `intercept_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13bbf1-5d8b-4130-a7c2-036508f1cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774622f-b818-44eb-ac1b-07d411f01c2d",
   "metadata": {},
   "source": [
    "Tout comme en connaître la précision, ici grâce au coefficient de détermination $R^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118f17a-4b58-457c-8e59-5b7cdf625657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e50bb-c0ed-49c9-ab51-db94f3fb775e",
   "metadata": {},
   "source": [
    "L’interprétation des résultats d’une régression linéaire est extrêmement simple et généralement immédiate :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739d511-ff94-4f9f-85c0-c9a7852ad630",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.regplot(x=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daace4c-d1d2-47f9-b4a1-3e0991e71bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### La régression polynomiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da7a82-8758-4500-b52d-d6d0c7dd882e",
   "metadata": {},
   "source": [
    "Le modèle de régression linéaire par ajustement affine ne s’est pas révélé une bonne hypothèse pour nos données d’entrée. Pour rappel, nous avions utilisé un polynôme de degré 3 pour générer les coordonnées en $y$.\n",
    "\n",
    "Lorsque, en visualisant les données, on observe une relation non-linéaire, l’hypothèse est qu’il doit exister d’autres caractéristiques pour l’expliquer que simplement $X$. *Scikit-Learn* met à disposition une classe `PolynomialFeatures` pour les ajouter à $X$ en fonction du polynôme considéré dans un paramètre `degree` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e059b-45e0-4ed5-b345-50e394673837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    LinearRegression(),\n",
    ")\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332a3c0-9851-4bfe-ab67-86019fef0a29",
   "metadata": {},
   "source": [
    "Le modèle s’ajuste bien mieux aux données que la précédente régression linéaire, ce qui se traduit immédiatement dans une augmentation de son *$R^2$ score* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca798f64-dd40-4fe4-8023-c0a50263828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X[:, 0], y=y[:, 0])\n",
    "ax.plot(sorted(X), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cac8f-6539-4894-8ec6-7eba7ab941b5",
   "metadata": {},
   "source": [
    "Il serait tentant de chercher à améliorer le *$R^2$ score* en augmentant le degré du polynôme, mais le risque serait d’ajuster trop bien le modèle (*overfitting*) aux données d’entraînement qu’il ne parviendrait plus à obtenir de bonnes prédictions sur des données nouvelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a512563-eabb-4038-9e36-2ae1faeee123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=15),\n",
    "    LinearRegression(),\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "ax = sns.scatterplot(x=X[:, 0], y=y[:, 0])\n",
    "ax.plot(sorted(X), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653146f-47d3-4011-9d5c-bc92e7d2005d",
   "metadata": {},
   "source": [
    "La courbe semble plus fortement attirée par les données avec un polynôme de très haut degré, ce qui, en quelque sorte, la rend dépendante vis-à-vis d’elles. Il faut s’attendre à voir le *$R^2$ score* diminuer sensiblement sur des données nouvelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57c0d7-2e02-4cc4-8fab-dd4354cfb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X_prime = np.random.rand(100, 1)\n",
    "y_prime = 2 - 3 * X_prime + 3 * X_prime ** 2 + 5 * X_prime ** 3 + np.random.rand(100, 1)\n",
    "\n",
    "y_pred = model.predict(X_prime)\n",
    "\n",
    "ax = sns.scatterplot(x=X_prime[:, 0], y=y_prime[:, 0])\n",
    "ax.plot(sorted(X_prime), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X_prime, y_prime):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36631ab0-f477-4508-9c5b-3b283522e12b",
   "metadata": {},
   "source": [
    "## Les modèles pour les tâches de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff34f8-f9c0-4f97-84c2-e726b5d864a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Les K plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6511452-6aea-4f51-ac9f-1541c19cb979",
   "metadata": {},
   "source": [
    "Le modèle de recherche des $k$ plus proches voisins (*K-Nearest Neighbors*) consiste à rechercher, pour toute nouvelle entrée, les $k$ entrées les plus proches dans la base d’apprentissage. Il est par ailleurs possible de préciser $k$ grâce à un paramètre `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42daf523-5379-48c0-95e2-7bdfbecfb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors=3))\n",
    "_ = model.fit(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2c43a-6101-44f3-96fe-0c039cab5b45",
   "metadata": {},
   "source": [
    "Parmi les attributs intéressants de la classe, citons `classes_` et `feature_names_in_` qui permettent de garder une trace des différentes modalités ainsi que des variables explicatives utilisées dans le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07dec6-c11e-443d-b938-747827cf093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Modalités : {model.classes_}\",\n",
    "    f\"Variables explicatives : {model.feature_names_in_}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643591d0-cbeb-484d-8cbc-4c9658f65675",
   "metadata": {},
   "source": [
    "Le score obtenu est le taux de prédictions exactes sur l’ensemble du jeu d’apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5f629-9263-4ccf-a14b-3bcd8f8ebe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8b6ee-c7fb-4a4a-945b-3872e483ab57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### La régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b882bd-a492-4292-bbcc-c69818372541",
   "metadata": {},
   "source": [
    "L’algorithme de régression logistique est très souvent employé dans les tâches de classification car il peut estimer rapidement la probabilité qu’une observation appartienne à une modalité particulière. Un paramètre `class_weight` permet de transmettre un dictionnaire de poids pour les modalités afin de pondérer la classification, ce qui peut se révéler utile pour corriger un biais de représentativité. Dans le cas contraire, toutes les étiquettes sont réputées avoir un poids de 1. Il est également possible de modifier l’algorithme utilisé par défaut pour l’optimisation de la régression avec un paramètre `solver`. Notons également un paramètre `max_iter` fixé par défaut à 100, qu’il peut être utilse d’augmenter afin de faire disparaître un avertissement `ConvergenceWarning` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bdad6-323c-4e91-8d92-e5524e9e69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "weights = {\n",
    "    \"Adelie\": .4,\n",
    "    \"Gentoo\": .2,\n",
    "    \"Chinstrap\": .4\n",
    "}\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(\n",
    "        max_iter=100,\n",
    "        class_weight=weights,\n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efb32b-8a57-4ff1-95e3-b124fba0a2e3",
   "metadata": {},
   "source": [
    "### Les machines à vecteurs de support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291114d5-a77a-4649-8276-70ab98f3f965",
   "metadata": {},
   "source": [
    "### La classification naïve bayésienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b94cad-9b7f-4101-a272-61fe4d954326",
   "metadata": {},
   "source": [
    "## Les modèles mixtes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d1ce4-3b26-4d22-9cdb-da1b361de213",
   "metadata": {},
   "source": [
    "### Les arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19d37f-622d-4212-9da1-678ce83d5aba",
   "metadata": {},
   "source": [
    "### Les forêts d’arbres décisionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9c0c1-ddd6-42d5-b4e8-5d8f8f1235b6",
   "metadata": {},
   "source": [
    "### Les réseaux de neurones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

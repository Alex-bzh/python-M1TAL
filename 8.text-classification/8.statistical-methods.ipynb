{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb27f0db-cae5-4f6d-8e87-e9ebf37919a2",
   "metadata": {},
   "source": [
    "# Aperçu de méthodes statistiques dans l’attribution de la paternité ou de la maternité d’une œuvre littéraire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413224b2-13bc-4c21-9b95-297596f6dda4",
   "metadata": {},
   "source": [
    "Avant l’avènement des techniques d’apprentissage automatique, les tâches d’attribution de la paternité ou de la maternité d’une œuvre littéraire se résolvaient grâce à des méthodes statistiques qui ont encore cours aujourd’hui au point d’alimenter parfois les premières.\n",
    "\n",
    "L’idée générale est de comparer la fréquence de traits linguistiques (mots, mots outils, *n*-grammes, structures syntaxiques…) entre un ou plusieurs corpus aux auteurs avérés avec un texte anonyme ou à l’attribution contestée. Les tâches concernées par ces méthodes peuvent aller de la simple vérification d’une attribution à la classification (décider s’il s’agit d’un roman policier ou de science-fiction) en passant par l’arbitrage entre deux ou plusieurs auteur·rices.\n",
    "\n",
    "Après avoir abordé rapidement quelques notions fondamentales sur les tests statistiques, nous appliquerons certains d’entre eux à des cas précis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced76df-8b17-473c-bd51-ad1714f6ae1b",
   "metadata": {},
   "source": [
    "## Des hypothèses statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b04f10-6ee2-42a4-ace4-3650ed8ae75a",
   "metadata": {},
   "source": [
    "Communément, un test statistique sert à vérifier qu’une hypothèse est vraie : c’est *l’hypothèse nulle*, notée $H_0$. Dans le cas contraire, *l’hypothèse alternative*, notée $H_1$, est retenue car considérée comme statistiquement significative. Pour juger de la significativité du test, le résultat de l’expérience est confronté à une loi de probabilité avec un seuil de risque $\\alpha$ généralement établi à 0,05 (5 % de risque d’erreur). Si la probabilité d’obtenir une valeur aussi extrême que celle observée, la valeur-p (ou *p-value* en anglais), est inférieure à ce seuil, de fortes présomptions pèsent contre l’hypothèse nulle que l’on vient de tester. À l’inverse, si la valeur-p est supérieure à ce seuil, on conclut simplement à l’échec du test, sans ne rien dire des hypothèses formulées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db3c8b2-8234-4b9a-8068-8dd3445fdb7b",
   "metadata": {},
   "source": [
    "## Les courbes caractéristiques de composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2c30f-803a-4a79-94bd-f7f353b5839d",
   "metadata": {},
   "source": [
    "En 1887, Thomas Corwin Mendenhall publie un article dans *Science*, \"[The Characteristic Curves of Composition](https://www.science.org/doi/10.1126/science.ns-9.214S.237)\", dans lequel il avance que non seulement le style d’un auteur est régulier tout au long d’une de ses œuvres mais qu’il se retrouve en plus dans l’ensemble de ses ouvrages. Afin de le prouver, il retient comme mesure la longueur des mots dont la distribution révèlerait une signature caractéristique.\n",
    "\n",
    "Aujourd’hui dépassée par l’évolution de l’informatique, la méthode de Mendenhall vaut surtout pour l’anecdote historique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5641f8-eef0-4f15-bb6f-bb7d629d3f71",
   "metadata": {},
   "source": [
    "## Le test du $\\chi^2$ d’indépendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca40c0-67b7-481d-995c-8ef3e5d6deb9",
   "metadata": {},
   "source": [
    "### Principe du test d’indépendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139168ae-23c7-4627-9b31-c83bdb920a27",
   "metadata": {},
   "source": [
    "L’une des applications connues du test du $\\chi^2$ intervient dans l’estimation de la liaison entre deux variables qualitatives : soit elles sont indépendantes ($H_0$) soit elles covarient de manière statistiquement significative ($H_1$).\n",
    "\n",
    "Pour parvenir au résultat, la méthode consiste à établir, pour les variables ciblées, la distance entre les valeurs empiriques ($O_{ij}$) et les valeurs théoriques normalement attendues ($E_{ij}$) si elles étaient indépendantes, selon la formule :\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$$\n",
    "\n",
    "La somme obtenue est alors comparée à [la loi du $\\chi^2$](https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2#Table_de_valeurs_des_quantiles) avec $k$ degrés de liberté pour un risque d’erreur de 5 %, sachant que les degrés de liberté s’obtiennent en effectuant le produit entre le nombre de lignes et le nombre de colonnes selon la formule :\n",
    "\n",
    "$$\n",
    "k = (I – 1)(J – 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2782ff-db2a-4f6f-bdf1-aedc4723bfe4",
   "metadata": {},
   "source": [
    "### Comment survivre au naufrage du *Titanic*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2635e02-c263-442b-a353-79e88481b1cc",
   "metadata": {},
   "source": [
    "Émettons l’hypothèse qu’être riche nous aurait fourni plus de chances pour survivre au naufrage du *Titanic*. Mais comment s’en assurer ? Mobilisons [un jeu de données](./data/titanic.csv) qui fournit quelques informations sur les passagers du fameux paquebot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163872e7-9175-4b77-9d49-7a79f363737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730adf1-7e43-4f85-9b30-3e9907181558",
   "metadata": {},
   "source": [
    "Retenons les variables `Pclass` et `Survived`, la première indiquant la classe de transport (1e, 2e ou 3e classe) et la seconde nous apprenant si la personne a survécu ou non (1 ou 0) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2bfd0-cfa0-46f5-bd1a-24e0e3097097",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"Survived\", \"Pclass\"]]\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e6c14-b738-4130-ab39-fd292a51d653",
   "metadata": {},
   "source": [
    "Créons maintenant une table de contingence afin de savoir combien de passagers et passagères ont péri ou survécu en fonction de leurs conditions d’hébergement à bord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d5666-6008-46e6-abc7-941a6e570e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(df.Pclass, df.Survived)\n",
    "\n",
    "display(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056b6cd-3eb7-42c5-906f-fc3724a59ecb",
   "metadata": {},
   "source": [
    "Dans ce tableau, nous lisons par exemple que 80 personnes de la première classe ont péri contre 372 de la seconde classe. À titre informatif, notons qu’il est possible d’obtenir les sommes marginales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c55aa-9899-40e2-abd1-e41df8dbd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Pclass, df.Survived, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531adef-f1d4-4c30-8c4c-c2242f213e8e",
   "metadata": {},
   "source": [
    "De là, nous devons calculer les valeurs théoriques attendues ($E_{ij}$) en cas d’indépendance des variables :\n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{ \\sum_{j=1}^{J}O_{ij} \\cdot \\sum_{i=1}^{I}O_{ij}}{N}\n",
    "$$\n",
    "\n",
    "|Pclass|0|1|\n",
    "|:-:|:-:|:-|\n",
    "|1|(216 * 549) / 891 = **133.09**|(216 * 342) / 891 = **82.91**|\n",
    "|2|(184 * 549) / 891 = **113.37**|(184 * 342) / 891 = **70.63**|\n",
    "|3|(491 * 549) / 891 = **302.54**|(491 * 342) / 891 = **188.46**|\n",
    "\n",
    "La matrice $E_{ij}$ peut s’obtenir directement avec Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b2073-6211-4751-b73c-2b70644dde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = contingency.sum().sum()\n",
    "Oi = contingency.sum(axis=1).values\n",
    "Oj = contingency.sum(axis=0).values\n",
    "\n",
    "# reshape Oi\n",
    "Oi = Oi.reshape(contingency.shape[0], 1)\n",
    "\n",
    "Eij = (Oi * Oj) / N\n",
    "\n",
    "display(Eij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd87a4-4a9b-48b7-9b4b-724b4b25a1de",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à calculer la valeur du $\\chi^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f5171-6b33-40df-a907-ebeca2f26b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = (contingency.values - Eij) ** 2 / Eij\n",
    "\n",
    "display(chi_squared.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb70a0-80b3-4039-bd1d-7790f5e4d969",
   "metadata": {},
   "source": [
    "#### Interprétation avec la table de distribution du $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac497b2-8f95-4b09-9ecd-0faf4e9bb375",
   "metadata": {},
   "source": [
    "La [loi du $\\chi^2$](https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2#Table_de_valeurs_des_quantiles) fournit des valeurs critiques pour chaque niveau de seuil $\\alpha$ avec $k$ degrés de liberté. Dans notre exemple, nous avons retenu un seuil de risque de 0,05 et le nombre de degrés de liberté est de $(3 - 1)(2 - 1) = 2$. La valeur critique donnée par la table est de 5,99. Notre résultat lui étant bien supérieur, nous pouvons rejeter $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619f273-1f15-4366-9dd1-b97860f31746",
   "metadata": {},
   "source": [
    "#### Interprétation avec la valeur-p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86352f-9c48-4083-b4a0-7423c6e2ee6a",
   "metadata": {},
   "source": [
    "Plus communément, l’interprétation d’un test statistique se fait avec la valeur-p qui dans notre exemple exprime la probabilité d’obtenir une valeur aussi extrême que celle observée. Si la valeur-p est inférieure au seuil de 0,05, nous pouvons rejeter l’hypothèse nulle.\n",
    "\n",
    "La méthode `chi2.sf()` de *Scipy* permet de calculer la valeur-p à partir d’un $\\chi^2$ et du nombre de degrés de libertés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b54567-974a-4c40-abc5-6d57473e85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "display(chi2.sf(chi_squared.sum(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dbc99-a951-44af-81cc-d01a96e2587a",
   "metadata": {},
   "source": [
    "À noter l’existence d’une fonction `chi2_contingency()` qui ressort toutes les mesures désirées à partir d’un tableau de contingence sans les sommes marginales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d3f26-23b2-462a-aacd-b25341ee52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, pvalue, degrees, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\n",
    "    f\"Valeur du khi-deux : {chi2:.4f}\",\n",
    "    f\"Probabilité du khi-deux (valeur-p) : {pvalue:.4f}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d2599-631e-445d-b2a9-b3fb65329120",
   "metadata": {},
   "source": [
    "### Test de la distance entre deux vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605faa55-cc6b-4255-b12e-ff06a3bb7bbc",
   "metadata": {},
   "source": [
    "Formulons une nouvelle hypothèse selon laquelle un texte anonyme n’est pas de la main d’un auteur particulier. Pour la tester, nous envisageons de calculer la distance entre la fréquence des occurrences des mots dans le texte anonyme et celle observée dans le corpus de l’auteur. Si à la fin du test la valeur-p est inférieure au seuil de 5 %, nous présumons que notre hypothèse est fausse et retenons l’hypothèse alternative.\n",
    "\n",
    "La méthode consiste à :\n",
    "\n",
    "1. obtenir les vocabulaires du texte anonyme et du corpus de l’auteur ;\n",
    "2. les combiner afin d’obtenir un vocabulaire complet ;\n",
    "3. sélectionner les $n$ mots les plus fréquents du corpus combiné ;\n",
    "4. calculer les valeurs théoriques ($E_{ij}$) dans chacun des deux corpus ;\n",
    "5. calculer la distance avec les valeurs empiriques.\n",
    "\n",
    "Avant de commencer, chargeons toutes les bibliothèques logicielles nécessaires ainsi que les textes à comparer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec5365-de11-4afc-8217-30860e450cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.stats import chi2\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "corpus = [\n",
    "    \"Le petit chat boit du lait.\",\n",
    "    \"Le petit chien boit de l’eau.\",\n",
    "    \"La vache boit de l’eau mais ne boit pas de lait.\"\n",
    "]\n",
    "\n",
    "anonym = \"Le petit oiseau boit de l’eau.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cde16-563e-48df-a0a0-e5f252f7033f",
   "metadata": {},
   "source": [
    "#### Étape 1 : obtenir les vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26abd-3c82-4727-b74d-e812e8cf13b0",
   "metadata": {},
   "source": [
    "La réalisation de cette étape consiste à appliquer toutes les opérations mises en jeu dans une tâche de sac de mots. Nous nous contentons ici d’une banale tokenisation afin d’obtenir au final deux matrices d’occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23614eb7-2323-4979-9727-cb3619647f7f",
   "metadata": {},
   "source": [
    "##### La matrice des occurrences du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ab37a-7958-473e-99d1-00e87af34de8",
   "metadata": {},
   "source": [
    "La segmentation est effectuée pour chaque texte de la variable `corpus` en filtrant sur les mots de deux caractères et moins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b0497-c199-467a-966d-a411f23599a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_corpus = [\n",
    "    word.lower()\n",
    "    for sent in corpus\n",
    "    for word in tokenizer.tokenize(sent)\n",
    "    if len(word) > 2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcd3b4-5144-4c74-acc1-6ce78e554e27",
   "metadata": {},
   "source": [
    "Grâce à un objet de type `Counter()`, il est maintenant aisé d’obtenir les fréquences d’apparition des mots dans le corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1254270-d641-4b8c-9639-63a7954bd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_corpus = Counter(words_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ffa32-0da5-4a8c-8e87-f3dc73592c09",
   "metadata": {},
   "source": [
    "##### La matrice d’occurrences du texte anonyme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda6d54-5e44-4989-8b9b-8aab2b76f6c4",
   "metadata": {},
   "source": [
    "Répétons l’opération pour le texte anonyme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125ecaa-b52e-4208-bd56-372bc29ce3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_anonym = Counter([\n",
    "    word.lower()\n",
    "    for word in tokenizer.tokenize(anonym)\n",
    "    if len(word) > 2\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0458f5c-04d7-4dc4-a0f3-5ba5bf95a406",
   "metadata": {},
   "source": [
    "#### Étape 2 : combiner les deux vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabd7e3-f5a6-4787-8f75-c0d8c0670235",
   "metadata": {},
   "source": [
    "Une opération triviale qui consiste à fusionner les deux objets de type `Counter` en un troisième :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2685b0-4a69-4063-b615-0df438725b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_all = occurrences_corpus + occurrences_anonym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa1100-0442-45ce-8a76-0c698b350aa8",
   "metadata": {},
   "source": [
    "Regardons un aperçu du résultat dans un *data frame* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd2b4f-d0b6-4c72-a345-8394789116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=occurrences_all.values(),\n",
    "    index=occurrences_all.keys(),\n",
    "    columns=[\"All\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed823fb-2452-4a3f-98e3-4f9bb18fc519",
   "metadata": {},
   "source": [
    "Les *data frames* sont en vérité des structures très pratiques pour combiner et afficher des données. Leur compatibilité avec les structures natives de Python et des biblbiothèques de référence comme *Numpy* en font un incontournable de l’analyse de données.\n",
    "\n",
    "Affichons une représentation de la distribution de notre vocabulaire parmi les deux documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e04b9f-e67d-43c1-995f-f043d33c6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[occurrences_corpus, occurrences_anonym],\n",
    "    index=[\"A\", \"X\"]\n",
    ").fillna(0)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea5bef-b592-4aba-9dfe-f9a3da992263",
   "metadata": {},
   "source": [
    "#### Étape 3 : sélectionner les mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f0e6c-d64b-4fd1-a5f7-969f5c3d23e5",
   "metadata": {},
   "source": [
    "Un corpus étant généralement constitué d’une très grande quantité, il conviendra dans la réalité de sélectionner les *n* mots les plus fréquents du vocabulaire combiné. Pour les besoins de l’exercice, nous sélectionnerons les 20 mots les plus fréquents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844b18a-98cb-4de8-930c-c6995944f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = occurrences_all.most_common(20)\n",
    "\n",
    "# remove occurrences to obtain a simple list\n",
    "most_common = list(map(lambda x: x[0], most_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab86250-81ec-4958-a4cd-c02ae5c1b04b",
   "metadata": {},
   "source": [
    "Et filtrons les objets `Counter` afin de ne retenir que les clés désirées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fffa97-c82d-4d4a-a918-1091a3d51991",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_all = Counter({ key: value for key, value in occurrences_all.items() if key in most_common })\n",
    "occurrences_corpus = Counter({ key: value for key, value in occurrences_corpus.items() if key in most_common })\n",
    "occurrences_anonym = Counter({ key: value for key, value in occurrences_anonym.items() if key in most_common })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56ff1f-7f6c-4b08-8fee-a075357c9e5d",
   "metadata": {},
   "source": [
    "L’opération est en fait plus simple lorsqu’elle est appliquée directement au *data frame* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b973a6e-b203-441c-87da-e77def288085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns.intersection(most_common)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6cc2b-b59d-454e-8e6c-519b5ba35122",
   "metadata": {},
   "source": [
    "#### Étape 4 : calculer les valeurs théoriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b3007-1f60-4ca1-8d15-a084ed0b1787",
   "metadata": {},
   "source": [
    "Le tableau se présentant avec des effectifs différents répartis dans des catégories similaires, nous pouvons appliquer le test du $\\chi^2$ d’indépendance et calculer la distribution marginale afin d’obtenir les valeurs théoriques ($E_{ij}$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269aa4e-a8bd-417b-9feb-74702b286b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.sum().sum()\n",
    "Oi = df.sum(axis=1).values\n",
    "Oj = df.sum(axis=0).values\n",
    "\n",
    "# reshape Oi\n",
    "Oi = Oi.reshape(df.shape[0], 1)\n",
    "\n",
    "Eij = (Oi * Oj) / N\n",
    "\n",
    "display(Eij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6228a8e-143a-4205-9820-28cfd5e79f58",
   "metadata": {},
   "source": [
    "#### Étape 5 : calculer la distance entre les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37e217-8dce-4cdb-8888-4cc10ffaf66c",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à appliquer la formule pour trouver la valeur du $\\chi^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9bf83-a451-4e5c-b97d-bf71cc3285e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = (df.values - Eij) ** 2 / Eij\n",
    "\n",
    "display(chi_squared.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8c2b-a2ac-44bd-859d-9b62eade914d",
   "metadata": {},
   "source": [
    "Et pour les degrés de liberté :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9a691-69dd-4922-9f5d-176ce9b3e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (Oi.shape[0] - 1) * (Oj.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad5c51-4a77-4cf5-b7a4-f10111c4c104",
   "metadata": {},
   "source": [
    "#### Interpréter les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fef8c-6f73-4b81-ab2b-bf71fdb9c703",
   "metadata": {},
   "source": [
    "La valeur critique fournie par la loi du $\\chi^2$ pour un risque $\\alpha$ de 0,05 avec 9 degrés de liberté est de 16,92. La valeur que nous avons calculée étant inférieure, nous ne pouvons pas rejeter l’hypothèse nulle et ne pouvons donc pas affirmer que les deux textes sont du même auteur.\n",
    "\n",
    "Le calcul de la valeur-p renvoie à la même conclusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af76670-dc09-4423-80b9-0463116eddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = chi2.sf(chi_squared.sum(), k)\n",
    "\n",
    "print(\n",
    "    f\"Valeur-p : {pvalue}\",\n",
    "    \"La valeur-p étant supérieure au seuil de 0,05, nous ne pouvons pas rejeter l’hypothèse nulle.\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31707b-79d8-4659-9a61-99ea867f7452",
   "metadata": {},
   "source": [
    "### Du malaise dans la statistique textuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef40e72-ccf7-46b0-98bc-989bfca57d4f",
   "metadata": {},
   "source": [
    "Le résultat que nous avons précédemment obtenu provient d’un test statistique durement éprouvé, conçu en 1900 par un mathématicien britannique, Karl Pearson, et diffusé grâce à un article paru dans [*The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science*](https://www.tandfonline.com/doi/abs/10.1080/14786440009463897). Comme nous l’avons vu, l’application de la formule du $\\chi^2$ repose sur des données quantitatives afin de fournir une valeur censée vérifier une hypothèse. Que ce soit dans le premier exemple sur nos chances de survie à bord du *Titanic* ou dans le second sur l’indépendance entre un corpus et un texte anonyme, cette première exigence a été remplie. Pourquoi *première* ? Parce que le recours au test du $\\chi^2$ supppose davantage de prérequis, parmi lesquels :\n",
    "\n",
    "- des effectifs de 5 minimum ;\n",
    "- des variables indépendantes ;\n",
    "- des données aléatoires ;\n",
    "- …\n",
    "\n",
    "Le dernier critère que nous citons est moins anodin qu’il n’y paraît : une variable aléatoire suppose des données aléatoirement distribuées. Autrement dit, la présence des mots dans le corpus ne doit pas être explicable par leur entourage, ce qui est contradictoire avec l’idée même de langage. Rien de plus naturel en effet à trouver en présence du substantif *chat* le déterminant *le*. Pour cette raison, en 2001, Adam Kilgarriff propose une variante du test du $\\chi^2$ d’indépendance qui se débarrasse de l’hypothèse nulle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2b06a-3e74-4af8-a1d6-fefa558b3af9",
   "metadata": {},
   "source": [
    "#### Le $\\chi^2$ de Kilgarriff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f61ed-8a6f-4669-b8d2-ea643946c125",
   "metadata": {},
   "source": [
    "Dans son article [\"Comparing Corpora\"](https://doi.org/10.1075/ijcl.6.1.05kil) paru en 2001 dans le *International Journal of Corpus Linguistics*, Adam Kilgarriff met en lumière les problèmes soulevés par l’application de tests statistiques à la matière textuelle. Non seulement la distribution des mots dans un corpus n’est pas le fruit du hasard, mais cette influence réciproque qu’ils exercent entre eux rend caduque le recours à certaines hypothèses. Franck Owen et Ron Jones font remarquer en 1977 dans [*Statistics*](https://annas-archive.org/md5/30509e973a8e1198e80c20bf1694c8da) qu’à partir d’une certaine taille d’échantillon toutes les hypothèses nulles dans le cadre d’un test du $\\chi^2$ seraient rejetées.\n",
    "\n",
    "Kilgarriff propose une méthode d’évaluation du $\\chi^2$ reposant sur une constitution rigoureuse des corpus à comparer, trop souvent oubliée par la littérature scientifique qui se contente de ne conserver de son article que la formule. Les expériences menées par Kilgarriff, qu’elles portent sur le $\\chi^2$ ou sur d’autres tests statistiques, impliquaient au préalable de constituer un ensemble de *Known-Similarity Corpora* (KSC) à partir de deux textes raisonnablement différents. Dans un ensemble KSC, le premier corpus contient 100 % du premier texte, le second 90 % du premier et 10 % du second etc., ce qui permet d’émettre des affirmations comme : le premier corpus est plus similaire au second qu’au troisième, ou bien le second corpus est plus similaire au troisième qu’il ne l’est au quatrième et ainsi de suite. À partir de là, une bonne mesure serait celle qui validerait toutes les affirmations.\n",
    "\n",
    "Une fois l’ensemble KSC constitué, la méthode de Kilgarriff propose de suivre les étapes suivantes :\n",
    "\n",
    "1. diviser un corpus en tranches (500 mots) ;\n",
    "2. créer deux sous-corpus en affectant aléatoirement la moitié de chaque ;\n",
    "3. mesurer leur similarité ;\n",
    "4. reproduire les étapes 2 et 3 un certain nombre de fois ;\n",
    "5. calculer la moyenne et l’écart-type de toutes les itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1e176-2184-435e-a4a5-21c37c23e7f8",
   "metadata": {},
   "source": [
    "## Tests basés sur les rangs des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327dd35-7209-4a6e-9b11-b0658aa12fa9",
   "metadata": {},
   "source": [
    "Il existe des tests statistiques qui se prêtent mieux à l’étude d’un texte que d’autres. Dans le cas du $\\chi^2$, nous avons notamment relevé qu’il impliquait une distribution aléatoire des mots alors que toute production d’un énoncé un tant soit peu compréhensible suppose le respect d’un modèle de langue.\n",
    "\n",
    "Parmi ces tests, dits non paramétriques, certains prennent pour données de départ le rang d’un item dans un ensemble ordonné plutôt que sa valeur. Il s’agit par exemple du test U de Mann-Whitney ou encore du coefficient de corrélation de rang de Spearman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbfbe0-a952-446e-8c0c-97ddf7b98dd8",
   "metadata": {},
   "source": [
    "### Le test $U$ d’indépendance de Mann-Whitney"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba44b6-a61d-4d55-809f-b899f0f9c328",
   "metadata": {},
   "source": [
    "#### Dans un ensemble d’échantillons de moins de vingt paires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4383e-cd21-4863-8ee7-1f4c784bac11",
   "metadata": {},
   "source": [
    "Sous l’hypothèse nulle, ce test pose la similitude entre deux groupes d’une population. En analyse de textes, il s’agira de tester deux corpus en comparant des échantillons de tailles similaires. On calculera la fréquence d’un mot *w* dans chacun des échantillons et ordonnerons les résultats. Prenons pour exemple huit échantillons de 5000 mots issus de deux textes A et B et regardons la distribution des occurrences du mot *chat* :\n",
    "\n",
    "||||||||||Total|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Fréquence|4|12|12|15|21|27|31|75|–|\n",
    "|Échantillon|B3|B1|A2|B4|A1|A3|A4|B2|–|\n",
    "|Rang (A)|–|–|2,5|–|5|6|7|–|20,5|\n",
    "|Rang (B)|1|2,5|–|4|–|–|–|8|15,5|\n",
    "\n",
    "**Remarque :** en cas d’égalité de rang, on les additionne pour attribuer la moyenne à toutes les valeurs concernées par l’égalité.\n",
    "\n",
    "On note ensuite les valeurs de $U_A$ et $U_B$ :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_A &= n_1 \\cdot n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\\\\n",
    "&= 4 \\cdot 4 + \\frac{4 \\cdot (4 + 1)}{2} - 20,5\\\\\n",
    "&= 16 + \\frac{20}{2} - 20,5\\\\\n",
    "&= 16 + 10 - 20,5\\\\\n",
    "&= 5,5\\\\\n",
    "U_B &= n_1 \\cdot n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\\\\n",
    "&= 16 + 10 - 15,5\\\\\n",
    "&= 10,5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Pour ne retenir que le plus petit score – à savoir ici 5.5 – comme valeur de $U$. Si le nombre d’échantillons n’excède pas une vingtaine de paires, alors le résultat du test est lisible directement dans [une table de Mann-Whitney](images/mann-whitney.png). Dans notre exemple, la valeur critique au seuil $\\alpha$ de 0.05 est de 0. Comme $5 > 0$, nous ne pouvons rejeter l’hypothèse nulle et devons accepter qu’il n'y a pas de différence notable entre les deux ensembles d’échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577672f-642f-4232-b0b1-00343246c825",
   "metadata": {},
   "source": [
    "#### Dans un grand ensemble d’échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af2c56-e36b-421e-a17e-3b2d3f127dba",
   "metadata": {},
   "source": [
    "Si les ensembles que nous comparons sont constitués de plus d’une vingtaine d’échantillons, nous aurons alors besoin de calculer une valeur-z :\n",
    "\n",
    "$$\n",
    "z = \\frac{U - \\mu U}{\\sigma U}\n",
    "$$\n",
    "\n",
    "Où $\\mu U$ représente la valeur attendue de $U$ :\n",
    "\n",
    "$$\n",
    "\\mu U = \\frac{n_1 \\cdot n_2}{2}\n",
    "$$\n",
    "\n",
    "et $\\sigma U$ son écart-type :\n",
    "\n",
    "$$\n",
    "\\sigma U = \\sqrt{\\frac{n_1 \\cdot n_2 \\cdot (n_1 + n_2 + 1)}{12}}\n",
    "$$\n",
    "\n",
    "Considérons un ensemble d’une cinquantaine d’échantillons caractérisé par :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    n_1 &= 25\\\\\n",
    "    n_2 &= 25\\\\\n",
    "    R_1 &= 636\\\\\n",
    "    R_2 &= 639\\\\\n",
    "    U_1 &= 25 \\cdot 25 + \\frac{25 \\cdot (25 + 1)}{2} - 636 = 314\\\\\n",
    "    U_2 &= 25 \\cdot 25 + \\frac{25 \\cdot (25 + 1)}{2} - 639 = 311\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Nous obtenons une espérance de U de :\n",
    "\n",
    "$$\n",
    "\\mu U = 25 \\cdot 25 \\div 2 = 312,5\n",
    "$$\n",
    "\n",
    "Et un écart-type de :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\sigma U &= \\sqrt{\\frac{25 \\cdot 25 \\cdot (25 + 25 + 1)}{12}}\\\\\n",
    "    &= \\sqrt{\\frac{625 \\cdot 51}{12}}\\\\\n",
    "    &= \\sqrt{\\frac{31875}{12}}\\\\\n",
    "    &= 51,5388\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Maintenant que nous avons la moyenne et l’écart-type attendus, nous pouvons calculer la valeur-z :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    z &= \\frac{311 - 312,5}{51,5388}\\\\\n",
    "    &= \\frac{-1.5}{51,5388}\\\\\n",
    "    &\\approx -0,0291\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La valeur-z permet de se repérer dans la table de la loi normale centrée réduite, d’où nous pouvons déduire que, pour une valeur-z approchant de -0,0291, la valeur-p est de 0,977, ce qui ne nous permet pas de rejeter l’hypothèse nulle selon laquelle il n’y a pas de différence entre les deux classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8c2b7-688d-4865-97f5-e43f3118b75d",
   "metadata": {},
   "source": [
    "#### Ampleur de l’effet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c3428-c550-447a-ae9b-371f9ed71e96",
   "metadata": {},
   "source": [
    "Dans l’hypothèse où nous aurions mis en lumière une différence significative entre les deux classes, nous aurions pu également mesurer l’ampleur de l’effet selon la formule :\n",
    "\n",
    "$$\n",
    "r = \\frac{\\lvert z \\rvert}{\\sqrt{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11752b02-df8e-4108-b49a-b5f0e0ac6349",
   "metadata": {},
   "source": [
    "#### Effectuer le test avec Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2ed78-0b02-4601-8f01-38fa215a71e5",
   "metadata": {},
   "source": [
    "La bibliothèque *Scipy* offre une méthode `.mannwhitneyu()` pour passer le test. Reprenons le premier exemple avec la distribution du mot *chat* dans deux corpus divisés en huit échantillons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b67f2-5e66-4971-ba68-de2b92f91769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# occurrences of word 'chat' in two corpora,\n",
    "# divided into eight samples\n",
    "A = [12, 21, 27, 31]\n",
    "B = [4, 12, 15, 75]\n",
    "\n",
    "# method returns the value of U for the first sample,\n",
    "# test is two-sided by default\n",
    "U1, p = mannwhitneyu(A, B, method=\"exact\", alternative='two-sided')\n",
    "\n",
    "# U of the second sample can be easily computed\n",
    "U2 = len(A) * len(B) - U1\n",
    "\n",
    "# value of U\n",
    "U = min(U1, U2)\n",
    "\n",
    "print(f\"La valeur U de Mann-Whitney est de {U} avec une valeur-p de {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b57a3c-9a67-4ae5-8664-484bb0c3b669",
   "metadata": {},
   "source": [
    "La probabilité de rencontrer une distribution au moins aussi extrême du mot *chat* étant supérieure à 5 %, les résultats du test ne sont pas significatifs et nous ne pouvons pas rejeter l’hypothèse nulle d’absence de différence entre les corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fc01d-b462-4184-b262-a31270f7a73f",
   "metadata": {},
   "source": [
    "### Le coefficient de corrélation de rang de Spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff1103-5e65-4545-ab29-14dd82637be6",
   "metadata": {},
   "source": [
    "Une autre mesure basée sur le rang des mots et qui permet d’évaluer la similarité de deux corpus est la corrélation de rang de Spearman :\n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\sum d^2_i}{n \\cdot (n^2 -1)}\n",
    "$$\n",
    "\n",
    "Elle suppose de comparer des objets de dimensions équivalentes et sans égalité de rang où tous les mots d’un corpus seront dans le second corpus. Considérons l’exemple suivant où nous souhaitons étudier deux corpus A et B dont les cinq mots les plus fréquents sont :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    A &= (\\text{chat}, 12), (\\text{eau}, 9), (\\text{lait}, 9), (\\text{chien}, 8), (\\text{boire}, 6)\\\\\n",
    "    B &= (\\text{chat}, 85), (\\text{lait}, 42), (\\text{boire}, 38), (\\text{chien}, 22), (\\text{eau}, 17)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Nous obtenons le tableau suivant :\n",
    "\n",
    "|Mot|Freq A|Rang A|Freq B|Rang B|Différence (absolue)|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|\n",
    "|chat|12|1|85|1|0|\n",
    "|chien|8|4|22|4|0|\n",
    "|eau|9|2|17|5|3|\n",
    "|lait|9|3|42|2|1|\n",
    "|boire|6|5|38|3|2|\n",
    "\n",
    "Remarquons l’égalité de rang dans le corpus A entre les mots *eau* et *lait* qui partagent la même mesure de fréquence et corrigeons le tableau :\n",
    "\n",
    "|Mot|Freq A|Rang A|Freq B|Rang B|Différence (absolue)|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|\n",
    "|chat|12|1|85|1|0|\n",
    "|chien|8|4|22|4|0|\n",
    "|eau|9|2,5|17|5|2,5|\n",
    "|lait|9|2,5|42|2|0,5|\n",
    "|boire|6|5|38|3|2|\n",
    "\n",
    "Le coefficient de Spearman vaut ainsi :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\rho &= 1 - \\frac{6 \\cdot (2,5^2 + 0,5^2)}{5 \\cdot (5^2 -1)}\\\\\n",
    "    &= 1 - \\frac{6 \\cdot 6,50}{5 \\cdot 24}\\\\\n",
    "    &= 1 - \\frac{39}{120}\\\\\n",
    "    &= 0,675\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466957c-bb4a-4b52-9fcd-9345889a1630",
   "metadata": {},
   "source": [
    "## La mesure Delta de Burrows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c090d96-8e68-4535-b2f0-c2fea497ab80",
   "metadata": {},
   "source": [
    "Issue d’un papier [\"‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship\"](https://doi-org.inshs.bib.cnrs.fr/10.1093/llc/17.3.267) paru en 2002 dans la revue *Literary and Linguistic Computing*, la mesure utilise les fréquences relatives des mots les plus courants dans des textes dont les auteurs ou les autrices sont connues afin d’établir une distance avec d’autres textes dont la paternité ou la maternité est disputée.\n",
    "\n",
    "La méthode exposée consiste à :\n",
    "\n",
    "1. créer un sous-corpus par auteur ou autrice qui sera constitué des textes pour lesquels la paternité ou la maternité n’est pas contestée ;\n",
    "2. réunir tous ces textes dans un corpus général ;\n",
    "3. déterminer les *n* mots les plus fréquents de ce corpus ;\n",
    "4. pour chacun de ces *n* mots, calculer sa probabilité d’apparition dans chacun des sous-corpus ;\n",
    "5. établir la moyenne et l’écart-type puis en déduire la valeur-z ;\n",
    "6. répéter les opérations pour les textes anonymes ou dont l’attribution est contestée ;\n",
    "7. calculer la cote $\\Delta$.\n",
    "\n",
    "Pour rappel, la formule de la valeur-z est identique à la variable centrée-réduite :\n",
    "\n",
    "$$\n",
    "Z = \\frac{X-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Pour la mesure du $\\Delta$ de Burrows, utiliser la distance de Manhattan :\n",
    "\n",
    "$$\n",
    "\\Delta (C, T) = \\sum_{i=1}^n \\lvert Z_i(C) - Z_i(T) \\rvert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c469b5-1050-4821-8d70-907797e00ba3",
   "metadata": {},
   "source": [
    "### Pour aller plus loin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb65aa-035a-433b-9ccd-91a4ad6c17f2",
   "metadata": {},
   "source": [
    "Suite à la proposition de John Burrows, des améliorations ont ensuite été proposées, comme le $\\Delta$ quadratique ([Argamon, 2007](https://doi.org/10.1093/llc/fqn003)) ou encore le $\\Delta \\cos$ ([Smith and Aldridge, 2011](https://doi.org/10.1080/09296174.2011.533591)) qui calcule la similarité cosinus à partir des valeurs-z afin d’obtenir une mesure angulaire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

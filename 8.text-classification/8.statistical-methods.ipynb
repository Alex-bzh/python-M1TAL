{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb27f0db-cae5-4f6d-8e87-e9ebf37919a2",
   "metadata": {},
   "source": [
    "# Aperçu de méthodes statistiques dans l’attribution de la paternité ou de la maternité d’une œuvre littéraire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413224b2-13bc-4c21-9b95-297596f6dda4",
   "metadata": {},
   "source": [
    "Avant l’avènement des techniques d’apprentissage automatique, les tâches d’attribution de la paternité ou de la maternité d’une œuvre littéraire se résolvaient grâce à des méthodes statistiques qui ont encore cours aujourd’hui au point d’alimenter parfois les premières.\n",
    "\n",
    "L’idée générale est de comparer la fréquence de traits linguistiques (mots, mots outils, *n*-grammes, structures syntaxiques…) entre un ou plusieurs corpus aux auteurs avérés avec un texte anonyme ou à l’attribution contestée. Les tâches concernées par ces méthodes peuvent aller de la simple vérification d’une attribution à la classification (décider s’il s’agit d’un roman policier ou de science-fiction) en passant par l’arbitrage entre deux ou plusieurs auteur·rices.\n",
    "\n",
    "Après avoir abordé rapidement quelques notions fondamentales sur les tests statistiques, nous appliquerons certains d’entre eux à des cas précis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced76df-8b17-473c-bd51-ad1714f6ae1b",
   "metadata": {},
   "source": [
    "## Des hypothèses statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b04f10-6ee2-42a4-ace4-3650ed8ae75a",
   "metadata": {},
   "source": [
    "Communément, un test statistique sert à vérifier qu’une hypothèse est vraie : c’est *l’hypothèse nulle*, notée $H_0$. Dans le cas contraire, *l’hypothèse alternative*, notée $H_1$, est retenue car considérée comme statistiquement significative. Pour juger de la significativité du test, le résultat de l’expérience est confronté à une loi de probabilité avec un seuil de risque $\\alpha$ généralement établi à 0,05 (5 % de risque d’erreur). Si la probabilité d’obtenir une valeur aussi extrême que celle observée, la valeur-p (ou *p-value* en anglais), est inférieure à ce seuil, de fortes présomptions pèsent contre l’hypothèse nulle que l’on vient de tester. À l’inverse, si la valeur-p est supérieure à ce seuil, on conclut simplement à l’échec du test, sans ne rien dire des hypothèses formulées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5641f8-eef0-4f15-bb6f-bb7d629d3f71",
   "metadata": {},
   "source": [
    "## Le test du $\\chi^2$ d’indépendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca40c0-67b7-481d-995c-8ef3e5d6deb9",
   "metadata": {},
   "source": [
    "### Principe du test d’indépendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139168ae-23c7-4627-9b31-c83bdb920a27",
   "metadata": {},
   "source": [
    "L’une des applications connues du test du $\\chi^2$ intervient dans l’estimation de la liaison entre deux variables qualitatives : soit elles sont indépendantes ($H_0$) soit elles covarient de manière statistiquement significative ($H_1$).\n",
    "\n",
    "Pour parvenir au résultat, la méthode consiste à établir, pour les variables ciblées, la distance entre les valeurs empiriques ($O_{ij}$) et les valeurs théoriques normalement attendues ($E_{ij}$) si elles étaient indépendantes, selon la formule :\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n",
    "$$\n",
    "\n",
    "La somme obtenue est alors comparée à [la loi du $\\chi^2$](https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2#Table_de_valeurs_des_quantiles) avec $k$ degrés de liberté pour un risque d’erreur de 5 %, sachant que les degrés de liberté s’obtiennent en effectuant le produit entre le nombre de lignes et le nombre de colonnes selon la formule :\n",
    "\n",
    "$$\n",
    "k = (I – 1)(J – 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2782ff-db2a-4f6f-bdf1-aedc4723bfe4",
   "metadata": {},
   "source": [
    "### Comment survivre au naufrage du *Titanic*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2635e02-c263-442b-a353-79e88481b1cc",
   "metadata": {},
   "source": [
    "Émettons l’hypothèse qu’être riche nous aurait fourni plus de chances pour survivre au naufrage du *Titanic*. Mais comment s’en assurer ? Mobilisons [un jeu de données](./data/titanic.csv) qui fournit quelques informations sur les passagers du fameux paquebot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163872e7-9175-4b77-9d49-7a79f363737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730adf1-7e43-4f85-9b30-3e9907181558",
   "metadata": {},
   "source": [
    "Retenons les variables `Pclass` et `Survived`, la première indiquant la classe de transport (1e, 2e ou 3e classe) et la seconde nous apprenant si la personne a survécu ou non (1 ou 0) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2bfd0-cfa0-46f5-bd1a-24e0e3097097",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"Survived\", \"Pclass\"]]\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e6c14-b738-4130-ab39-fd292a51d653",
   "metadata": {},
   "source": [
    "Créons maintenant une table de contingence afin de savoir combien de passagers et passagères ont péri ou survécu en fonction de leurs conditions d’hébergement à bord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d5666-6008-46e6-abc7-941a6e570e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(df.Pclass, df.Survived)\n",
    "\n",
    "display(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056b6cd-3eb7-42c5-906f-fc3724a59ecb",
   "metadata": {},
   "source": [
    "Dans ce tableau, nous lisons par exemple que 80 personnes de la première classe ont péri contre 372 de la seconde classe. À titre informatif, notons qu’il est possible d’obtenir les sommes marginales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c55aa-9899-40e2-abd1-e41df8dbd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Pclass, df.Survived, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531adef-f1d4-4c30-8c4c-c2242f213e8e",
   "metadata": {},
   "source": [
    "De là, nous devons calculer les valeurs théoriques attendues ($E_{ij}$) en cas d’indépendance des variables :\n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{ \\sum_{j=1}^{J}O_{ij} \\cdot \\sum_{i=1}^{I}O_{ij}}{N}\n",
    "$$\n",
    "\n",
    "|Pclass|0|1|\n",
    "|:-:|:-:|:-|\n",
    "|1|(216 * 549) / 891 = **133.09**|(216 * 342) / 891 = **82.91**|\n",
    "|2|(184 * 549) / 891 = **113.37**|(184 * 342) / 891 = **70.63**|\n",
    "|3|(491 * 549) / 891 = **302.54**|(491 * 342) / 891 = **188.46**|\n",
    "\n",
    "La matrice $E_{ij}$ peut s’obtenir directement avec Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b2073-6211-4751-b73c-2b70644dde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = contingency.sum().sum()\n",
    "Oi = contingency.sum(axis=1).values\n",
    "Oj = contingency.sum(axis=0).values\n",
    "\n",
    "# reshape Oi\n",
    "Oi = Oi.reshape(contingency.shape[0], 1)\n",
    "\n",
    "Eij = (Oi * Oj) / N\n",
    "\n",
    "display(Eij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd87a4-4a9b-48b7-9b4b-724b4b25a1de",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à calculer la valeur du $\\chi^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f5171-6b33-40df-a907-ebeca2f26b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = (contingency.values - Eij) ** 2 / Eij\n",
    "\n",
    "display(chi_squared.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb70a0-80b3-4039-bd1d-7790f5e4d969",
   "metadata": {},
   "source": [
    "#### Interprétation avec la table de distribution du $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac497b2-8f95-4b09-9ecd-0faf4e9bb375",
   "metadata": {},
   "source": [
    "La [loi du $\\chi^2$](https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2#Table_de_valeurs_des_quantiles) fournit des valeurs critiques pour chaque niveau de seuil $\\alpha$ avec $k$ degrés de liberté. Dans notre exemple, nous avons retenu un seuil de risque de 0,05 et le nombre de degrés de liberté est de $(3 - 1)(2 - 1) = 2$. La valeur critique donnée par la table est de 5,99. Notre résultat lui étant bien supérieur, nous pouvons rejeter $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619f273-1f15-4366-9dd1-b97860f31746",
   "metadata": {},
   "source": [
    "#### Interprétation avec la valeur-p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86352f-9c48-4083-b4a0-7423c6e2ee6a",
   "metadata": {},
   "source": [
    "Plus communément, l’interprétation d’un test statistique se fait avec la valeur-p qui dans notre exemple exprime la probabilité d’obtenir une valeur aussi extrême que celle observée. Si la valeur-p est inférieure au seuil de 0,05, nous pouvons rejeter l’hypothèse nulle.\n",
    "\n",
    "La méthode `chi2.sf()` de *Scipy* permet de calculer la valeur-p à partir d’un $\\chi^2$ et du nombre de degrés de libertés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b54567-974a-4c40-abc5-6d57473e85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "display(chi2.sf(chi_squared.sum(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dbc99-a951-44af-81cc-d01a96e2587a",
   "metadata": {},
   "source": [
    "À noter l’existence d’une fonction `chi2_contingency()` qui ressort toutes les mesures désirées à partir d’un tableau de contingence sans les sommes marginales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d3f26-23b2-462a-aacd-b25341ee52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, pvalue, degrees, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\n",
    "    f\"Valeur du khi-deux : {chi2:.4f}\",\n",
    "    f\"Probabilité du khi-deux (valeur-p) : {pvalue:.4f}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d2599-631e-445d-b2a9-b3fb65329120",
   "metadata": {},
   "source": [
    "### Test de la distance entre deux vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605faa55-cc6b-4255-b12e-ff06a3bb7bbc",
   "metadata": {},
   "source": [
    "Formulons une nouvelle hypothèse selon laquelle un texte anonyme n’est pas de la main d’un auteur particulier. Pour la tester, nous envisageons de calculer la distance entre la fréquence des occurrences des mots dans le texte anonyme et celle observée dans le corpus de l’auteur. Si à la fin du test la valeur-p est inférieure au seuil de 5 %, nous présumons que notre hypothèse est fausse et retenons l’hypothèse alternative.\n",
    "\n",
    "La méthode consiste à :\n",
    "\n",
    "1. obtenir les vocabulaires du texte anonyme et du corpus de l’auteur ;\n",
    "2. les combiner afin d’obtenir un vocabulaire complet ;\n",
    "3. sélectionner les $n$ mots les plus fréquents du corpus combiné ;\n",
    "4. calculer les valeurs théoriques ($E_{ij}$) dans chacun des deux corpus ;\n",
    "5. calculer la distance avec les valeurs empiriques.\n",
    "\n",
    "Avant de commencer, chargeons toutes les bibliothèques logicielles nécessaires ainsi que les textes à comparer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec5365-de11-4afc-8217-30860e450cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.stats import chi2\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "corpus = [\n",
    "    \"Le petit chat boit du lait.\",\n",
    "    \"Le petit chien boit de l’eau.\",\n",
    "    \"La vache boit de l’eau mais ne boit pas de lait.\"\n",
    "]\n",
    "\n",
    "anonym = \"Le petit oiseau boit de l’eau.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cde16-563e-48df-a0a0-e5f252f7033f",
   "metadata": {},
   "source": [
    "#### Étape 1 : obtenir les vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26abd-3c82-4727-b74d-e812e8cf13b0",
   "metadata": {},
   "source": [
    "La réalisation de cette étape consiste à appliquer toutes les opérations mises en jeu dans une tâche de sac de mots. Nous nous contentons ici d’une banale tokenisation afin d’obtenir au final deux matrices d’occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23614eb7-2323-4979-9727-cb3619647f7f",
   "metadata": {},
   "source": [
    "##### La matrice des occurrences du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ab37a-7958-473e-99d1-00e87af34de8",
   "metadata": {},
   "source": [
    "La segmentation est effectuée pour chaque texte de la variable `corpus` en filtrant sur les mots de deux caractères et moins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b0497-c199-467a-966d-a411f23599a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_corpus = [\n",
    "    word.lower()\n",
    "    for sent in corpus\n",
    "    for word in tokenizer.tokenize(sent)\n",
    "    if len(word) > 2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcd3b4-5144-4c74-acc1-6ce78e554e27",
   "metadata": {},
   "source": [
    "Grâce à un objet de type `Counter()`, il est maintenant aisé d’obtenir les fréquences d’apparition des mots dans le corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1254270-d641-4b8c-9639-63a7954bd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_corpus = Counter(words_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ffa32-0da5-4a8c-8e87-f3dc73592c09",
   "metadata": {},
   "source": [
    "##### La matrice d’occurrences du texte anonyme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda6d54-5e44-4989-8b9b-8aab2b76f6c4",
   "metadata": {},
   "source": [
    "Répétons l’opération pour le texte anonyme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125ecaa-b52e-4208-bd56-372bc29ce3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_anonym = Counter([\n",
    "    word.lower()\n",
    "    for word in tokenizer.tokenize(anonym)\n",
    "    if len(word) > 2\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0458f5c-04d7-4dc4-a0f3-5ba5bf95a406",
   "metadata": {},
   "source": [
    "#### Étape 2 : combiner les deux vocabulaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabd7e3-f5a6-4787-8f75-c0d8c0670235",
   "metadata": {},
   "source": [
    "Une opération triviale qui consiste à fusionner les deux objets de type `Counter` en un troisième :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2685b0-4a69-4063-b615-0df438725b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_all = occurrences_corpus + occurrences_anonym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa1100-0442-45ce-8a76-0c698b350aa8",
   "metadata": {},
   "source": [
    "Regardons un aperçu du résultat dans un *data frame* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd2b4f-d0b6-4c72-a345-8394789116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=occurrences_all.values(),\n",
    "    index=occurrences_all.keys(),\n",
    "    columns=[\"All\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed823fb-2452-4a3f-98e3-4f9bb18fc519",
   "metadata": {},
   "source": [
    "Les *data frames* sont en vérité des structures très pratiques pour combiner et afficher des données. Leur compatibilité avec les structures natives de Python et des biblbiothèques de référence comme *Numpy* en font un incontournable de l’analyse de données.\n",
    "\n",
    "Affichons une représentation de la distribution de notre vocabulaire parmi les deux documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e04b9f-e67d-43c1-995f-f043d33c6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[occurrences_corpus, occurrences_anonym],\n",
    "    index=[\"A\", \"X\"]\n",
    ").fillna(0)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea5bef-b592-4aba-9dfe-f9a3da992263",
   "metadata": {},
   "source": [
    "#### Étape 3 : sélectionner les mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f0e6c-d64b-4fd1-a5f7-969f5c3d23e5",
   "metadata": {},
   "source": [
    "Un corpus étant généralement constitué d’une très grande quantité, il conviendra dans la réalité de sélectionner les *n* mots les plus fréquents du vocabulaire combiné. Pour les besoins de l’exercice, nous sélectionnerons les 20 mots les plus fréquents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844b18a-98cb-4de8-930c-c6995944f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = occurrences_all.most_common(20)\n",
    "\n",
    "# remove occurrences to obtain a simple list\n",
    "most_common = list(map(lambda x: x[0], most_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab86250-81ec-4958-a4cd-c02ae5c1b04b",
   "metadata": {},
   "source": [
    "Et filtrons les objets `Counter` afin de ne retenir que les clés désirées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fffa97-c82d-4d4a-a918-1091a3d51991",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_all = Counter({ key: value for key, value in occurrences_all.items() if key in most_common })\n",
    "occurrences_corpus = Counter({ key: value for key, value in occurrences_corpus.items() if key in most_common })\n",
    "occurrences_anonym = Counter({ key: value for key, value in occurrences_anonym.items() if key in most_common })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56ff1f-7f6c-4b08-8fee-a075357c9e5d",
   "metadata": {},
   "source": [
    "L’opération est en fait plus simple lorsqu’elle est appliquée directement au *data frame* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b973a6e-b203-441c-87da-e77def288085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns.intersection(most_common)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6cc2b-b59d-454e-8e6c-519b5ba35122",
   "metadata": {},
   "source": [
    "#### Étape 4 : calculer les valeurs théoriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b3007-1f60-4ca1-8d15-a084ed0b1787",
   "metadata": {},
   "source": [
    "Le tableau se présentant avec des effectifs différents répartis dans des catégories similaires, nous pouvons appliquer le test du $\\chi^2$ d’indépendance et calculer la distribution marginale afin d’obtenir les valeurs théoriques ($E_{ij}$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269aa4e-a8bd-417b-9feb-74702b286b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.sum().sum()\n",
    "Oi = df.sum(axis=1).values\n",
    "Oj = df.sum(axis=0).values\n",
    "\n",
    "# reshape Oi\n",
    "Oi = Oi.reshape(df.shape[0], 1)\n",
    "\n",
    "Eij = (Oi * Oj) / N\n",
    "\n",
    "display(Eij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6228a8e-143a-4205-9820-28cfd5e79f58",
   "metadata": {},
   "source": [
    "#### Étape 5 : calculer la distance entre les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37e217-8dce-4cdb-8888-4cc10ffaf66c",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à appliquer la formule pour trouver la valeur du $\\chi^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9bf83-a451-4e5c-b97d-bf71cc3285e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = (df.values - Eij) ** 2 / Eij\n",
    "\n",
    "display(chi_squared.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8c2b-a2ac-44bd-859d-9b62eade914d",
   "metadata": {},
   "source": [
    "Et pour les degrés de liberté :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9a691-69dd-4922-9f5d-176ce9b3e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (Oi.shape[0] - 1) * (Oj.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad5c51-4a77-4cf5-b7a4-f10111c4c104",
   "metadata": {},
   "source": [
    "#### Interpréter les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fef8c-6f73-4b81-ab2b-bf71fdb9c703",
   "metadata": {},
   "source": [
    "La valeur critique fournie par la loi du $\\chi^2$ pour un risque $\\alpha$ de 0,05 avec 9 degrés de liberté est de 16,92. La valeur que nous avons calculée étant inférieure, nous ne pouvons pas rejeter l’hypothèse nulle et ne pouvons donc pas affirmer que les deux textes sont du même auteur.\n",
    "\n",
    "Le calcul de la valeur-p renvoie à la même conclusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af76670-dc09-4423-80b9-0463116eddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = chi2.sf(chi_squared.sum(), k)\n",
    "\n",
    "print(\n",
    "    f\"Valeur-p : {pvalue}\",\n",
    "    \"La valeur-p étant supérieure au seuil de 0,05, nous ne pouvons pas rejeter l’hypothèse nulle.\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

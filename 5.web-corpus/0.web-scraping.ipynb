{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Définition\n",
    "\n",
    "Le *Web Scraping* est une technique d’exploration du Web visant à en extraire du contenu. Comme les pages Web sont un minimum structurées grâce à un langage de balisage, le HTML, il est possible de repérer dans le code source des segments de texte placés entre des marqueurs :\n",
    "\n",
    "```html\n",
    "<a href=\"http://www.lemonde.fr\">Le Monde.fr</a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans l’exemple ci-dessus, les balises `<a>` et `</a>` encadrent le texte *Le Monde.fr*. Si notre ambition est d’étudier la sémantique des liens hypertextes, on peut facilement adresser une requête à un outil d’analyse du code HTML de la forme :\n",
    "\n",
    "> Récupèrer tous les segments encadrés par les marqueurs de l’élément HTML `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pré-requis\n",
    "\n",
    "Cette facilité est mise en œuvre par deux modules de Python :\n",
    "- *urllib.request*\n",
    "- *BeautifulSoup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "# Modules\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extraire du code HTML\n",
    "\n",
    "Le premier module, *urllib.request*, est chargé de récupérer le document HTML à analyser. Il a juste besoin d’une adresse URL pour fonctionner, mais nous lui fournissons en prime, par politesse, un *User-agent*. Comme la visite du programme que nous construisons aura un impact sur la performance du site Web visité, il est de bon ton d’expliquer rapidement notre volonté et de laisser une adresse mail pour nous contacter au besoin. Cette convenance nous évitera peut-être un fichage sur un fichier `robots.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# URL\n",
    "url = 'http://www.llf.cnrs.fr'\n",
    "\n",
    "# Additional headers\n",
    "headers = { 'User-agent' : 'HyperText extractor (Alexandre Roulois)' }\n",
    "\n",
    "# HTTP Request\n",
    "request = urllib.request.Request(url, headers=headers)\n",
    "\n",
    "# Load HTML document\n",
    "with urllib.request.urlopen(request) as webpage:\n",
    "    # Get the html content\n",
    "    html = webpage.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyser du code HTML\n",
    "\n",
    "Le module *BeautifulSoup* permet d’extraire des données depuis un document structuré aux formats XML ou HTML. Il met à disposition un ensemble de méthodes et de stratégies pour les exploiter simplement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# New instance of BeautifulSoup\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Find all anchors in the HTML page\n",
    "anchors = soup.find_all('a')\n",
    "\n",
    "# For each anchor found…\n",
    "for anchor in anchors:\n",
    "    # … print the textual content\n",
    "    print(anchor.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constituer un corpus à partir du Web\n",
    "\n",
    "Grâce à ces techniques, il devient très abordable avec Python de constituer son propre corpus à partir de données sur le Web. Dans notre démonstration, nous souhaitons récupérer les commentaires des spectateurs de films de science-fiction sur le site *Allociné*. Du point de vue légal, les droits demeurent ceux de la plateforme et même si au final nous concevons un nouveau produit intellectuel (un corpus de données), il nous est interdit de diffuser ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Méthodologie\n",
    "\n",
    "Pour réaliser notre objectif, nous mettons au point une méthodologie en sept étapes, représentée par le schéma ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. cibler une page du site *Allociné* qui recense [les films de SF de la décennie 2010-2019](http://www.allocine.fr/films/notes/genre-13021/decennie-2010/) ;\n",
    "2. analyser le code HTML afin de récupérer, dans la liste des films, leurs identifiants uniques ;\n",
    "3. enregistrer les identifiants dans un fichier plat ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. reconstruire les URLs de chaque film à partir des identifiants ;\n",
    "5. interroger les pages Web de chaque film ;\n",
    "6. identifier la section qui recueille les commentaires ;\n",
    "7. extraire les commentaires et les enregistrer dans un fichier texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Méthode pour extraire des critiques de spectateurs](images/0-fig1.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

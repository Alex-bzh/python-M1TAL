{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa672c4f-4c19-47ef-9afb-6fc6d57f9dfd",
   "metadata": {},
   "source": [
    "# Vectorisation avec Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2f853-e559-4616-b8c9-f390595b2a31",
   "metadata": {},
   "source": [
    "## Opérations sur des vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef144c7-257d-4823-b816-31bed506e89b",
   "metadata": {},
   "source": [
    "Python fournit heureusement des outils pour effectuer les opérations sur les vecteurs. Parmi les librairies utiles, citons *math*, *Numpy*, *Scipy* et *Scikit-Learn*.\n",
    "\n",
    "Avant de les charger en fonction de nos besoins, reprenons quelques vecteurs vus dans le précédent calepin :\n",
    "\n",
    "$$\n",
    "\\vec{A} = \\begin{pmatrix}\n",
    "    3  \\\\\n",
    "    12 \\\\\n",
    "    9  \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{B} = \\begin{pmatrix}\n",
    "    7  \\\\\n",
    "    32 \\\\\n",
    "    10\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{C} = \\begin{pmatrix}\n",
    "    11 \\\\\n",
    "    4  \\\\\n",
    "    8\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d555e43-c455-4e4d-8c93-cc6edae5a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [3, 12, 9, 0]\n",
    "B = [7, 32, 10]\n",
    "C = [11, 4, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae2c48-3317-498a-9ecd-a657727188d5",
   "metadata": {},
   "source": [
    "### Le produit scalaire de deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c6356-87ae-44be-9820-788f6adcf704",
   "metadata": {},
   "source": [
    "La méthode `.dot()` permet d’obtenir le produit de deux vecteurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9235d87-b58c-44b1-82e8-c97db789800b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.297058540778355"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.dot(A, A) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127cfb2-b65f-48fe-b172-b444ce2c24d2",
   "metadata": {},
   "source": [
    "### La norme d’un vecteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52adfea-a667-4d87-b6d4-34bb4cebff78",
   "metadata": {},
   "source": [
    "Nous avions établi précédemment la norme du vecteur $\\vec{A}$ approximativement à $15,2971$. Vérifions notre calcul avec la fonction d’algèbre linéaire `norm()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b903a8-a201-41e7-baff-16de20091c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.297058540778355"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc76c93-c86e-4663-8f14-c99a7aaf24d8",
   "metadata": {},
   "source": [
    "### La distance entre deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad49ce-b9e5-4f4c-8329-5da703faa602",
   "metadata": {},
   "source": [
    "La distance euclidienne entre deux vecteurs a cet avantage qu’elle peut se trouver dans un espace de n’importe quel dimension. L’opération revient à soustraire deux vecteurs puis à obtenir la norme de ce nouveau vecteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82b4072-b3ab-4831-898b-8a69b7ca3a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.35489375751565"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array(C) - np.array(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22098222-1a3f-4179-8a31-b2653361e52d",
   "metadata": {},
   "source": [
    "**Remarque :** notons que nous avons dû convertir explicitement nos variables `B` et `C`, de type `list` en tableaux *Numpy*. À titre anecdotique, le module `math` se passe de la conversion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c82011-a761-4c89-be16-169592385727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.35489375751565"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import dist\n",
    "\n",
    "dist(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01ce0b-cee6-4f69-b38a-a07f2fe0055c",
   "metadata": {},
   "source": [
    "## Métriques d’évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad5585-5a0b-4a40-a07f-68a677d32a0e",
   "metadata": {},
   "source": [
    "### La similarité cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87200a11-cd32-428e-90b2-5166f305daa7",
   "metadata": {},
   "source": [
    "Avec *Numpy*, il est nécessaire de déplier la formule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efe4490-c097-4d7c-b8f0-2f2bb33c0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité cosinus des vecteurs B et C est évaluée à 0.5869455647701575.\n"
     ]
    }
   ],
   "source": [
    "cos_BC = np.dot(B, C) / np.dot(np.linalg.norm(B), np.linalg.norm(C))\n",
    "\n",
    "print(f\"La similarité cosinus des vecteurs B et C est évaluée à {cos_BC}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97e7fd-e90b-491d-8902-905660aa40c6",
   "metadata": {},
   "source": [
    "Une fonction adéquate existe dans la librairie *Scikit-Learn*, `cosine_similarity`. Il convient toutefois de lui transmettre une matrice aux bonnes dimensions (nombre de vecteurs, nombre d’attributs). Dans notre exemple, la dimension serait (1, 3) parce que nous lui envoyons un vecteur constitué de trois composantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0ac1d9-9258-4d63-9eb0-f23ba4a8bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité cosinus des vecteurs B et C est évaluée à 0.5869455647701576.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_BC = cosine_similarity(\n",
    "    np.array(B).reshape(1, -1),\n",
    "    np.array(C).reshape(1, -1)\n",
    ")\n",
    "\n",
    "print(f\"La similarité cosinus des vecteurs B et C est évaluée à {cos_BC.ravel().tolist()[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141b1a-c32e-422c-bf34-dc8ea116ac78",
   "metadata": {},
   "source": [
    "### L’indice de Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663dac-27b3-4a52-b0ed-8fbae9173e5a",
   "metadata": {},
   "source": [
    "Une fois encore, *Scikit-Learn* permet d’obtenir directement la métrique en comparant deux vecteurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebe407c-41d9-48bd-ae04-2157166effa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les vecteurs A et B sont similaires à 25.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "A = [0, 0, 1, 1, 1]\n",
    "B = [1, 0, 1, 0, 0]\n",
    "\n",
    "print(f\"Les vecteurs A et B sont similaires à {jaccard_score(A, B):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d0906-7895-4c0b-a3df-0b26eb8c39ce",
   "metadata": {},
   "source": [
    "## Vectoriser un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ceeae5-2201-44b2-bcd3-84a09910e81b",
   "metadata": {},
   "source": [
    "Considérons un corpus restreint de trois phrases :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65196ccf-514b-412c-9824-72e0efe952fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Le petit chat boit du lait.\",\n",
    "    \"Le petit chien boit de l’eau.\",\n",
    "    \"La vache boit de l’eau mais ne boit pas de lait.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae44794-b45f-41b3-8b96-47ba481dcf0a",
   "metadata": {},
   "source": [
    "### L’approche fréquentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fdcb8-af6d-4e29-8ad9-9d73a8460b31",
   "metadata": {},
   "source": [
    "#### Obtenir une matrice d’occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10217ff-e530-40cf-a21d-26914c71bbec",
   "metadata": {},
   "source": [
    "Le module *Scikit-Learn* dispose d’outils pour l’extraction de caractéristiques dans un texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417e299a-b9d2-4687-b1cf-f3996b678cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27baaf-3116-4958-861f-f286d90acc0f",
   "metadata": {},
   "source": [
    "Le résultat produit un vocabulaire, que l’on peut récupérer avec la méthode `.get_feature_names_out()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4232ef1-9ded-46e6-a6c7-bb0f26683d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['boit', 'chat', 'chien', 'de', 'du', 'eau', 'la', 'lait', 'le',\n",
       "       'mais', 'ne', 'pas', 'petit', 'vache'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f065fc5-5f25-4107-ad1d-c24c419075ed",
   "metadata": {},
   "source": [
    "La méthode `.toarray()` quant à elle permet de représenter la matrice creuse du corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf2088a-a760-4468-9563-69ddf0c58dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [2, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6072dc-0dc9-49ea-b019-02e4a749cc4d",
   "metadata": {},
   "source": [
    "Associée à *Pandas*, il est possible de révéler le nom des colonnes afin de nous apprendre que dans la dernière phrase le mot *de* apparaît deux fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36d80cc-7c79-405b-b13c-fad9788becfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   boit  chat  chien  de  du  eau  la  lait  le  mais  ne  pas  petit  vache\n",
      "0     1     1      0   0   1    0   0     1   1     0   0    0      1      0\n",
      "1     1     0      1   1   0    1   0     0   1     0   0    0      1      0\n",
      "2     2     0      0   2   0    1   1     1   0     1   1    1      0      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5f70c-4099-41e1-8127-fd0e2eca41b0",
   "metadata": {},
   "source": [
    "Il est également possible de transmettre directement un vocabulaire au constructeur plutôt que de lui laisser la charge de le construire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f08010ed-f2bd-4f8b-bd7e-54ec12e68ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0],\n",
       "       [2, 0, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = ['boit', 'chat', 'chien', 'eau', 'lait', 'petit', 'vache']\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756826b-fa50-4541-819f-66326e276110",
   "metadata": {},
   "source": [
    "#### Calculer une matrice de cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026c275-2a84-4b98-bdde-7c21cdbb1e5e",
   "metadata": {},
   "source": [
    "Une matrice de cooccurrences relève la fréquence d’apparition d’un terme en compagnie de chacun des autres termes du corpus. Elle se calcule simplement en algèbre linéaire grâce au produit de la transposée de la matrice d’occurrences avec elle-même :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175f6aa7-2cf2-476c-bdbf-715dba004d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boit</th>\n",
       "      <th>chat</th>\n",
       "      <th>chien</th>\n",
       "      <th>eau</th>\n",
       "      <th>lait</th>\n",
       "      <th>petit</th>\n",
       "      <th>vache</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>boit</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chien</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eau</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lait</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petit</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vache</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       boit  chat  chien  eau  lait  petit  vache\n",
       "boit      0     1      1    3     3      2      2\n",
       "chat      1     0      0    0     1      1      0\n",
       "chien     1     0      0    1     0      1      0\n",
       "eau       3     0      1    0     1      1      1\n",
       "lait      3     1      0    1     0      1      1\n",
       "petit     2     1      1    1     1      0      0\n",
       "vache     2     0      0    1     1      0      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = X.toarray()\n",
    "coocc = m.T.dot(m)\n",
    "np.fill_diagonal(coocc, 0)\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=coocc,\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d9480-0532-49d6-bd63-f80986240aa6",
   "metadata": {},
   "source": [
    "### L’encodage *one-hot*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c201a-8320-4b93-b393-55dbc27b9aa1",
   "metadata": {},
   "source": [
    "Parfois, l’objectif est simplement de repérer, pour un document, quels mots du vocabulaire apparaissent, sans compter forcément sa fréquence. Ce genre d’opération s’obtient à partir de la matrice d’occurrences par un encodage *one-hot*, ou encodage 1 parmi *n*, en fixant à 1 tous les éléments qui ne sont pas égaux à 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8018dfec-89dc-4531-9585-6e800279097b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 0, 1, 1, 0], [1, 0, 1, 1, 0, 1, 0], [1, 0, 0, 1, 1, 0, 1]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda row: [ 0 if e == 0 else 1 for e in row ], X.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d7d1c-9610-4795-ba07-2ab038c5d037",
   "metadata": {},
   "source": [
    "### La pondération TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e36cb0-b094-4e72-836c-aed858a72730",
   "metadata": {},
   "source": [
    "#### À partir d’une matrice d’occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c46b76-21e7-46b6-ab69-76cc265f35b8",
   "metadata": {},
   "source": [
    "À partir d’une matrice d’occurrences, obtenue par exemple avec le transformateur `CountVectorizer`, il est possible de calculer une matrice TF (*term frequency*) ou TF-IDF (*term frequency-inverse document frequency*). C’est le rôle du transformateur `TfidfTransformer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c46d384-8fce-4902-8270-cf1581d61969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "result = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce13f7-a0ab-4d53-9956-f35a11f7d1a7",
   "metadata": {},
   "source": [
    "Le résultat est une matrice creuse dont il est possible de n’afficher que les éléments différents de 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f37090-9fae-4405-8b1f-e274c9dd23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.4804583972923858\n",
      "  (0, 4)\t0.4804583972923858\n",
      "  (0, 1)\t0.6317450542765208\n",
      "  (0, 0)\t0.3731188059313277\n",
      "  (1, 5)\t0.4804583972923858\n",
      "  (1, 3)\t0.4804583972923858\n",
      "  (1, 2)\t0.6317450542765208\n",
      "  (1, 0)\t0.3731188059313277\n",
      "  (2, 6)\t0.5305873490316616\n",
      "  (2, 4)\t0.40352535506797127\n",
      "  (2, 3)\t0.40352535506797127\n",
      "  (2, 0)\t0.6267468712982053\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f4451-872f-47ae-933a-7aac3c1ec768",
   "metadata": {},
   "source": [
    "Chaque ligne donne la mesure TF-IDF d’un mot dans un document. Prenons la ligne :\n",
    "\n",
    "```txt\n",
    "(1, 5)\t0.4804583972923858\n",
    "```\n",
    "\n",
    "Il faut comprendre ici que dans le document n°1, la mesure TF-IDF du terme n°5 est de 0,48046. Autrement dit, l’importance du terme *petit* est évaluée à 0,48046 dans la deuxième phrase de notre corpus.\n",
    "\n",
    "Il est à noter que le transformateur ne garde pas trace des caractéristiques apprises, juste de leurs indices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7ef7d5b-0c82-447e-978f-30b9f591d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df152b6e-da9a-47dd-af14-4d72ccc7842d",
   "metadata": {},
   "source": [
    "Pour les retrouver, il faut interroger la matrice d’occurrences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c41bde-4bb3-4b6d-a14c-22ccd38285d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['boit', 'chat', 'chien', 'eau', 'lait', 'petit', 'vache'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ab458-0553-48c4-94cf-638868dd9ac0",
   "metadata": {},
   "source": [
    "#### Sans matrice d’occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97b95b-bbf7-45f1-bfc1-afbedd1282ab",
   "metadata": {},
   "source": [
    "Le même résultat s’obtient directement depuis le corpus avec le transformateur `TfidfVectorizer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09ef2501-6cdf-41fb-a4b5-e5e31643fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.4804583972923858\n",
      "  (0, 4)\t0.4804583972923858\n",
      "  (0, 1)\t0.6317450542765208\n",
      "  (0, 0)\t0.3731188059313277\n",
      "  (1, 5)\t0.4804583972923858\n",
      "  (1, 3)\t0.4804583972923858\n",
      "  (1, 2)\t0.6317450542765208\n",
      "  (1, 0)\t0.3731188059313277\n",
      "  (2, 6)\t0.5305873490316616\n",
      "  (2, 4)\t0.40352535506797127\n",
      "  (2, 3)\t0.40352535506797127\n",
      "  (2, 0)\t0.6267468712982053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306514f-f655-42ac-8842-1d7003cc64b8",
   "metadata": {},
   "source": [
    "#### Visualisation sous forme tabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee6788-875f-4efa-93a7-a81fa1ed9e79",
   "metadata": {},
   "source": [
    "La visualisation que l’on attend d’une matrice prend souvent la forme d’un tableau :\n",
    "\n",
    "|Document|boit|chat|chien|eau|lait|petit|vache|\n",
    "|:-:|-|-|-|-|-|-|-|\n",
    "|0|0.373119|0.631745|0|0|0.480458|0.480458|0|\n",
    "|1|0.373119|0|0.631745|0.480458|0|0.480458|0|\n",
    "|2|0.626747|0|0|0.403525|0.403525|0|0.530587|\n",
    "\n",
    "Notre corpus étant restreint et la plupart des mots du vocabulaire étant présents dans chaque document qui le compose, cette représentation est plutôt explicite. Il n’en sera **jamais** de même avec des données réelles. La matrice TF-IDF est une matrice creuse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c97fd-a725-4227-957e-97953b9eaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afda7f6-cc25-4421-9997-a63533a6bd99",
   "metadata": {},
   "source": [
    "Si *Scikit-Learn* choisit ce mode de représentation, c’est parce qu’une matrice creuse étant constituée majoritairement de 0, elle ne prend que peu de place en mémoire. Si toutefois nous souhaitons obtenir une visualisation tabulaire, il faut au préalable la convertir en une matrice dense :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fefb15-9916-44f9-a912-94bd0eca692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dense = tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3501bb-a2fb-4332-aa43-265794eb5f71",
   "metadata": {},
   "source": [
    "Puis la transmettre à *Pandas* en important les en-têtes de colonnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e61c4-c855-4a56-8041-f8b4b301c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=tfidf_dense, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dd628-6e36-4177-bb84-0f06aa806281",
   "metadata": {},
   "source": [
    "### L’information mutuelle ponctuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a37fe6-7aaa-406d-a3be-e2e65e1eb530",
   "metadata": {},
   "source": [
    "Mieux connu sous l’acronyme PMI de l’anglais *Pointwise mutual information*, le concept a été présenté en 1961 par l’italien Roberto Mario Fano dans son ouvrage *Transmission of Information: A Statistical Theory of Communications* publié aux éditions du MIT. Il peut être décrit comme une mesure de la fréquence de deux événements par rapport à ce que l’on attendrait s’ils étaient indépendants, sachant que la probabilité de la survenue de deux événements indépendants est simplement le produit de leurs probabilités respectives. La formule vaut ainsi :\n",
    "\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log_2 \\frac{P(x, y)}{P(x) \\cdot P(y)}\n",
    "$$\n",
    "\n",
    "Le résultat allant de $-\\infty$ à $+\\infty$, on utilise plus volontiers, en traitement automatique du langage naturel, la variante strictement positive, la PPMI. La technique consiste simplement à fixer à 0 tout résultat inférieur à 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b80ca-ece4-4b54-9c60-6bd102ebce4b",
   "metadata": {},
   "source": [
    "#### Une matrice d’occurrences des *n*-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b524ce-ffe3-499f-b95b-cf6194653db5",
   "metadata": {},
   "source": [
    "La seule matrice d’occurrences des unigrammes ne nous permettrait de calculer, dans la formule, que les termes $P(x)$ ou $P(y)$. Par exemple, pour le mot *boit* qui apparaît dans tout le corpus 4 fois sur un total de 13 occurrences pour tous les mots du vocabulaire, sa probabilité serait de 0.30770 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd758bdd-1841-43c1-9451-1572e9924228",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = X.toarray()\n",
    "p_unigrams = unigrams.sum(axis=0) / unigrams.sum().sum()\n",
    "\n",
    "df_unigrams = pd.DataFrame(\n",
    "    data=p_unigrams,\n",
    "    index=vectorizer.get_feature_names_out(),\n",
    "    columns=['p'])\n",
    "\n",
    "df_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b31be1-3bd1-442f-bef1-f15c57ea9f2f",
   "metadata": {},
   "source": [
    "Pour obtenir la même matrice pour les bigrammes du corpus, il est possible d’appeler une autre instance de la classe `CountVectorizer` en lui adjoignant le paramètre `ngram_range` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c1694-6d90-46a9-af45-e42481dfaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bigrams = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vect_bigrams.fit_transform(corpus)\n",
    "\n",
    "bigrams = X2.toarray()\n",
    "p_bigrams = bigrams.sum(axis=0) / bigrams.sum().sum()\n",
    "\n",
    "df_bigrams = pd.DataFrame(\n",
    "    data=p_bigrams,\n",
    "    index=vect_bigrams.get_feature_names_out(),\n",
    "    columns=['p'])\n",
    "\n",
    "df_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037dd3b2-5b6f-4d61-9b78-bfbadfa0fd23",
   "metadata": {},
   "source": [
    "#### Une matrice PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59a413-4d38-41bf-95e5-2e01f8a1a0f9",
   "metadata": {},
   "source": [
    "Intéressons-nous aux termes *petit* et *chat* afin de calculer leur PMI :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{PMI}(\\text{petit},\\text{chat}) &= \\log_2 \\frac{P(\\text{petit}, \\text{chat})}{P(\\text{petit}) \\cdot P(\\text{chat})} \\\\\n",
    "    &= \\log_2 \\frac{0.05}{0.153846 \\times 0.076923} \\\\\n",
    "    &= \\log_2 4.225008 \\\\\n",
    "    &= 2.07895\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Fusionnons tout d’abord les listes de probabilités dans un dictionnaire où la clé sera le terme, un unigramme ou un bigramme, et la valeur la probabilité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b1f8a-1528-4577-ae20-c15c3e28bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = dict()\n",
    "probabilities.update(df_unigrams.p.to_dict())\n",
    "probabilities.update(df_bigrams.p.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f927008-9490-4922-9e05-1964f3bbdf9a",
   "metadata": {},
   "source": [
    "Définissons maintenant une fonction qui calcule la PMI de deux termes à partir d’un dictionnaire des probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fae7b2-eeaa-4e5f-bac4-b43c512e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(*, word, context, probabilities):\n",
    "    \"\"\"Positive Pointwise Mutual Information.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    word -- a word (w)\n",
    "    context -- the context of the word (c)\n",
    "    probabilities -- probabilities for P(w), P(c) and P(w, c)\n",
    "    \"\"\"\n",
    "    wc = probabilities.get(f\"{word} {context}\", 0)\n",
    "    cw = probabilities.get(f\"{context} {word}\", 0)\n",
    "\n",
    "    expected = probabilities.get(word, 0) * probabilities.get(context, 0)\n",
    "\n",
    "    # division by zero is undefined\n",
    "    if expected == 0:\n",
    "        score = 0.0\n",
    "    else:\n",
    "        # silence warnings about log(0)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            score = np.log2( (wc + cw) / expected )\n",
    "        # log(0) = 0\n",
    "        score = 0.0 if np.isinf(score) or score < 0 else score\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cea53-29fe-43e5-b0b6-b7d350356620",
   "metadata": {},
   "source": [
    "Vérifions que la PPMI de *petit* et *chat* correspond à ce que nous avons calculé à la main :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b796832-f84f-4b23-a35e-a68d42a8fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI(word='petit', context='chat', probabilities=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedd032-5b82-43a0-be05-3941b962ca60",
   "metadata": {},
   "source": [
    "Pour construire la matrice PPMI, nous avons besoin d’une matrice carrée et symétrique des termes du vocabulaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14af9f5-cf57-4303-8087-2229bec6f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array([\n",
    "    [\n",
    "        PPMI(word=w1, context=w2, probabilities=probabilities)\n",
    "        for w2 in vectorizer.get_feature_names_out()\n",
    "    ]\n",
    "    for w1 in vectorizer.get_feature_names_out()\n",
    "])\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=vectors,\n",
    "    index=vectorizer.get_feature_names_out(),\n",
    "    columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55e65e-ec34-4613-bd30-514953fde8e2",
   "metadata": {},
   "source": [
    "#### Retrouver des vecteurs similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31e38a-a9ec-48e4-8475-7e25a2d38343",
   "metadata": {},
   "source": [
    "Concept-clé du TAL, la PPMI constitue un excellent moyen pour obtenir des vecteurs de mots. Il est ensuite facile de calculer leur similarité cosinus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c785ca-28b5-430d-b49f-b727f3188ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = cosine_similarity(vectors, vectors)\n",
    "np.fill_diagonal(cosine, 0)\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=cosine,\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed82b33-2161-4861-8ae2-ffdb36c3344e",
   "metadata": {},
   "source": [
    "Identifier le terme le plus similaire à un autre revient à repérer l’indice du vecteur dont le score est le plus haut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf89fe-923f-4c3e-bcc3-06c066e8d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector cat\n",
    "cat = cosine[1]\n",
    "# which vector has the highest score?\n",
    "most_similar = np.argmax(cat)\n",
    "\n",
    "# which vector is it?\n",
    "df_unigrams.index[most_similar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d7e02-a49b-49b2-a6de-6dc8089a3658",
   "metadata": {},
   "source": [
    "Dans la réalité, plutôt que d’analyser l’information mutuelle ponctuelle d’un terme et de son contexte direct droit ou gauche, il est bien plus fréquent de considérer des *n*-grammes avec une fenêtre de tolérance. Par exemple, dans la phrase :\n",
    "\n",
    "```text\n",
    "Le petit chat boit du lait.\n",
    "```\n",
    "\n",
    "Le bigramme (*chat*, *lait*) peut être retenu si l’on accepte un contexte avec une fenêtre de tolérance de 3 mots. Il constitue alors ce que l’on appelle un *skip-gram* de degré 2 et de distance 3.\n",
    "\n",
    "La librairie *NLTK* fournit un outil pour les recenser à partir de la liste ordonnée des mots d’une phrase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549cca5-9693-4041-bd16-6ad4d4cbce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "s = \"Le petit chat boit du lait\".split()\n",
    "list(skipgrams(s, n=2, k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

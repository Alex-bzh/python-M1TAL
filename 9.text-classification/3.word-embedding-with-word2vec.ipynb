{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a7f39f-0b54-4a32-ad89-d26f0baba9c4",
   "metadata": {},
   "source": [
    "# Plongement lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ec32a-a46e-4bc2-a545-c38ca0cf3e6d",
   "metadata": {},
   "source": [
    "L’expression renvoie à l’anglais *word embedding* qui décrit une méthode d’apprentissage à partir de la représentation en vecteurs de nombres réels des mots d’un lexique. Plus justement, on peut parler de vectorisation de mots.\n",
    "\n",
    "L’idée est donc, à partir d’un texte, d’obtenir une liste de tokens qui sera représentée par des vecteurs de mêmes dimensions comme, par exemple :\n",
    "\n",
    "$$\n",
    "\\vec{\\text{autrement}} = \\begin{pmatrix}\n",
    "    0.8765   \\\\\n",
    "    1.3567   \\\\\n",
    "    \\text{…} \\\\\n",
    "    -0.6823\n",
    "\\end{pmatrix}\n",
    "\\hspace{2em}\n",
    "\\vec{\\text{lune}} = \\begin{pmatrix}\n",
    "    0.2908   \\\\\n",
    "    -1.4002  \\\\\n",
    "    \\text{…} \\\\\n",
    "    0.1209\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Cette représentation n’aura de sens que dans l’espace vectoriel du texte. Ajoutons une phrase au texte, un commentaire au corpus de commentaires, et les vecteurs n’auront plus les mêmes valeurs.\n",
    "\n",
    "On en revient toujours à l’importance de bien équilibrer un corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99620373-f2a1-4d8c-9b8a-f4aee2278c3c",
   "metadata": {},
   "source": [
    "## Word2Vec : la vectorisation de mots par Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6527a6-ae45-43c8-bb76-cc9d1d96cc80",
   "metadata": {},
   "source": [
    "Word2Vec est en fait un ensemble de modèles développés par [Google](https://arxiv.org/abs/1301.3781) pour effectuer les tâches de vectorisation de mots. Ils prennent la forme de réseaux de neurones peu profonds, à deux couches, qui apprennent du contexte linguistique des mots afin d’en déduire une représentation vectorielle. L’intérêt de cette méthode est qu’elle suit l’hypothèse distributionnelle ([Harris 1954](https://doi.org/10.1080/00437956.1954.11659520)) résumée par John Rupert Firth en 1957 par la formule : « You shall know a word by the company it keeps. » En d’autres termes, des mots sémantiquement proches partageront un contexte similaire, ce qui, du point de vue mathématique, se traduit par des vecteurs voisins l’un de l’autre.\n",
    "\n",
    "Deux architectures sont proposées :\n",
    "- le modèle *skip-gram* ;\n",
    "- le modèle sac de mot continu (*Continuous Bag-Of-Words*, ou CBOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958a81b-0edf-44e0-845c-9a2ffbc36863",
   "metadata": {},
   "source": [
    "### Le modèle *skip-gram*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae544a-feb2-4064-a160-ef8faea74fe7",
   "metadata": {},
   "source": [
    "Dans le modèle *skip-gram*, le réseau de neurones estime pour chaque mot du vocabulaire la probabilité qu’il soit dans le voisinage d’un mot particulier.\n",
    "\n",
    "Prenons la phrase : *Le petit chat boit du lait.* Le modèle nous dira probablement que *petit* et *boit* ont de grandes chances d’accompagner le mot *chat*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2ba85-bd6a-42d4-a681-cee8af3dee28",
   "metadata": {},
   "source": [
    "#### Architecture du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45ca10-92b7-4b88-8119-b41b5615693d",
   "metadata": {},
   "source": [
    "L’architecture se présente sous la forme de deux couches :\n",
    "\n",
    "![Schéma du modèle *skip-gram](./images/skip-gram-model.png)\n",
    "\n",
    "Le vecteur d’entrée est soumis à la couche cachée, composée d’un certain nombre de neurones, puis envoyée à la couche de sortie, activée par une fonction *softmax* qui garantit d’abord pour chaque composante une valeur comprise entre 0 et 1 et ensuite pour l’ensemble une somme égale à 1.\n",
    "\n",
    "La fonction *softmax* prend l’exponentielle de la composante d’un vecteur et la divise par la somme de toutes ses exponentielles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab391e2-9628-47cf-8751-c6ee2243a0cf",
   "metadata": {},
   "source": [
    "#### Lors de la phase d’entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b33481-5fbd-47fc-bd1a-adb17a3a0164",
   "metadata": {},
   "source": [
    "Le modèle est paramétré pour une certaine fenêtre (i.e. : 2). Il parcourt le corpus d’entraînement et apprend depuis des paires de mots. Le vecteur d’entrée, décrit dans un certain nombre de dimensions (i.e. : 300), représente le mot en entrée, et le vecteur de sortie, décrit dans le même nombre de dimensions, le mot apparié.\n",
    "\n",
    "Reprenons la phrase *Le petit chat boit du lait* avec une fenêtre d’apprentissage de deux. D’abord, le modèle s’intéresse au mot *Le* et à ses deux voisins du contexte droit, *petit* et *chat*. Il les apparie pour l’entraînement : (*Le*, *petit*) et (*Le*, *chat*). Il passe ensuite au mot suivant, *petit*, et cherche ses voisins dans la fenêtre spécifiée (*Le*, *chat*, *boit*) puis les apparie de nouveau : (*petit*, *Le*), (*petit*, *chat*) et (*petit*, *boit*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab10357-00c8-47c5-bd4f-e8dce3ed4116",
   "metadata": {},
   "source": [
    "#### Lors de la phase d’évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1b612-ccb6-4e8b-9845-ec2ef238ff75",
   "metadata": {},
   "source": [
    "De la taille du vocabulaire, le vecteur d’entrée, au format *one-hot*, est alors composé d’une unique composante à 1 pour le mot interrogé (par exemple le mot *chat*), les autres restant fixées à 0. La couche de sortie présente quant à elle un vecteur de probabilités de la taille du vocabulaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab807838-ceeb-4e7d-9592-f07beb4010ae",
   "metadata": {},
   "source": [
    "### Le modèle CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1688ff-32c6-4ee1-bb69-1d7487644ee6",
   "metadata": {},
   "source": [
    "Le modèle CBOW (*Continuous Bag of words*) est l’exacte symmétrie du modèle *skip-gram* : pour un contexte donné, il va estimer quel est le mot du vocabulaire qui a le plus de chance d’être au centre.\n",
    "\n",
    "Pour une fenêtre de 2, considérant le contexte *Le*, *chat* et *boit*, on s’attend à ce que le modèle nous propose en sortie le mot *petit*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4553f3f-ffd7-43e8-bf74-6e54b98d8e3b",
   "metadata": {},
   "source": [
    "#### Architecture du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339b75d-63fe-4041-8203-82a919995f1e",
   "metadata": {},
   "source": [
    "![Le modèle CBOW](./images/cbow-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38a240-bc22-4016-8b6a-e024df9d182b",
   "metadata": {},
   "source": [
    "#### Lors de la phase d’entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8638303-4596-4958-b88c-1a81f05ada1f",
   "metadata": {},
   "source": [
    "L’appariement est différent du modèle *skip-gram*. Ici, avec le modèle CBOW, il prend en entrée les vecteurs qui décrivent le contexte, un vecteur par mot décrit dans un certain nombre de dimensions (i.e. : 300) et, en sortie, le vecteur du mot au centre.\n",
    "\n",
    "Toujours pour la phrase *Le petit chat boit du lait* avec une fenêtre de 2, le modèle va d’abord apprendre de la paire ((*petit*, *chat*), *Le*), puis ((*Le*, *chat*, *boit*), *petit*), ensuite ((*Le*, *petit*, *boit*, *du*), *chat*) et ainsi de suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22f066-8477-4b50-9548-57d0d0526c20",
   "metadata": {},
   "source": [
    "#### Lors de la phase d’évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a004c-18d4-49a4-ad14-c34c145b28d3",
   "metadata": {},
   "source": [
    "Le vecteur d’entrée est un *one-hot* avec des 1 pour les mots faisant partie du contexte et le vecteur de sortie sera une liste de probabilités, de la taille du vocabulaire, pour que le mot soit celui au centre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bff9f-1e84-451c-90cf-85a84b4f5ea3",
   "metadata": {},
   "source": [
    "## Entraîner un modèle Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42c84d-e56f-494f-8ed6-c047a419d3fd",
   "metadata": {},
   "source": [
    "Sans entrer dans les détails de l’apprentissage automatique, Word2Vec aura besoin en entrée d’un corpus nettoyé, constitué d’une liste de phrases découpées en mots. Le fichier *bow-salammbo.pickle* contient un sac de mots du roman *Salammbô* de Flaubert ayant subi quelques traitements (bas de casse, mots vides retirés) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e234e54-15b6-46f2-90dc-38a653b98782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/bow-salammbo.pickle', 'rb') as f:\n",
    "    bow = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98782e-f8d2-48ec-a1be-8c77ea081665",
   "metadata": {},
   "source": [
    "En python, la librairie *Gensim* se charge d’implémenter les algorithmes Word2Vec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70005640-c34d-4d23-8abd-9c2368dd0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea826b-c28a-41cb-b22e-58f9e8c41c85",
   "metadata": {},
   "source": [
    "Nous aurons besoin de déterminer quelques paramètres :\n",
    "\n",
    "- `vector_size` pour la taille du vecteur de mot. En théorie, plus un mot est décrit par un grand nombre de composantes, meilleur il sera défini. En pratique, on choisira entre 100 et 1000 composantes, le gain étant infime au-delà. À titre d’exemple, les équipes de Google à l’origine de Word2Vec ont établi des vecteurs à 300 composantes.\n",
    "- `window` pour la fenêtre des contextes gauche et droit.\n",
    "- `min_count` pour définir une fréquence d’occurrences minimale.\n",
    "- `workers` pour déterminer le nombre de fils d’exécution à utiliser au moment de l’apprentissage.\n",
    "- `sg` pour le modèle d’apprentissage (`1` pour *skip-gram*; CBOW autrement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c3d8b-58fe-4633-9e6c-f453b7e8fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a CBOW model\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences=bow,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=4,\n",
    "    sg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91a99e-ff17-4f06-bfc9-74babb25efcd",
   "metadata": {},
   "source": [
    "### Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704c00c-5cad-4371-ac0a-d582b1aa426b",
   "metadata": {},
   "source": [
    "La classe `wv` permet d’obtenir ensuite la description d’un vecteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198973c6-1573-4dd8-833d-899c8604ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['cité']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18bab3-13b3-4780-acd4-603917627d72",
   "metadata": {},
   "source": [
    "Une méthode de la classe, `.most_similar()`, permet quant à elle d’obtenir les mots similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417cecb-1c8b-4c11-a130-daa12eea9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('cité')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

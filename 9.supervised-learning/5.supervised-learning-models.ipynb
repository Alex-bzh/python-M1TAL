{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4612cc-a51a-4686-8bdd-4921ae068a4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Les modèles pour l’apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef28a8-6424-4372-adf1-169784168bf4",
   "metadata": {},
   "source": [
    "Pour mémoire, les modèles pour l’apprentissage supervisé sont classés en fonction de la tâche à réaliser :\n",
    "- La **régression**, quand la prévision porte sur une valeur continue comme le salaire d’un·e étudiant·e en fin de formation ;\n",
    "- ou la **classification** lorsqu’il s’agit de prédire l’appartenance à une classe.\n",
    "\n",
    "Certains modèles sont réservés à l’une ou l’autre de ces tâches quand les autres peuvent servir aux deux. Dans ce calepin, nous parcourons les plus populaires sans pour autant rentrer dans les détails.\n",
    "\n",
    "1. **Les modèles pour la régression :**\n",
    "    - [La régression linéaire](#La-régression-linéaire)\n",
    "    - [La régression polynomiale](#La-régression-polynomiale)\n",
    "2. **Les modèles pour la classification :**\n",
    "    - [Les K plus proches voisins](#Les-K-plus-proches-voisins)\n",
    "    - [La classification naïve bayésienne](#La-classification-naïve-bayésienne)\n",
    "    - [La régression logistique](#La-régression-logistique)\n",
    "3. **Les modèles mixtes :**\n",
    "    - [Les séparateurs à vaste marge](#Les-séparateurs-à-vaste-marge) (SVM)\n",
    "    - [Les arbres de décision](#Les-arbres-de-décision)\n",
    "    - [Les forêts aléatoires](#Les-forêts-aléatoires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966232c9-07e6-420a-9789-0f5e34597f5a",
   "metadata": {},
   "source": [
    "Avant de commencer, chargeons les librairies nécessaires et constituons des ensembles de données pour les différentes tâches à traiter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e8a9c-91d7-4179-b4df-46a44552d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_selector as col_selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "###################\n",
    "# Regression task #\n",
    "###################\n",
    "\n",
    "# for deterministic purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# hundred points\n",
    "X = np.random.rand(100, 1)\n",
    "\n",
    "# polynomial function: y = 2 - 3x + 3x^2 + 5x^3\n",
    "y = 2 - 3 * X + 3 * X ** 2 + 5 * X ** 3 + np.random.rand(100, 1)\n",
    "\n",
    "#######################\n",
    "# Classification task #\n",
    "#######################\n",
    "\n",
    "# penguin census\n",
    "df = pd.read_csv(\"./data/penguin-census.csv\").dropna()\n",
    "\n",
    "# features & target\n",
    "target = \"species\"\n",
    "features = [\"flipper_length_mm\", \"body_mass_g\", \"sex\"]\n",
    "\n",
    "# dataset\n",
    "X_b = df[features]\n",
    "y_b = df[target]\n",
    "\n",
    "# selectors\n",
    "num_col_selector = col_selector(dtype_exclude=object)\n",
    "cat_col_selector = col_selector(dtype_include=object)\n",
    "\n",
    "# num & cat cols\n",
    "num_cols = num_col_selector(X_b)\n",
    "cat_cols = cat_col_selector(X_b)\n",
    "\n",
    "# preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num-transformers\", StandardScaler(), num_cols),\n",
    "        (\"cat-transformers\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b22ba-a7f8-4426-a1cc-9cd959540bf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Avant-propos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274c526-70ac-46cd-b73e-f098b91dc2a3",
   "metadata": {},
   "source": [
    "Les modèles d’apprentissage de *Scikit-Learn*, programmés en différentes classes, sont conçus autour du même esprit de cohésion. Chaque classe expose notamment les mêmes méthodes suivantes :\n",
    "- `.fit()` pour ajuster le modèle sur des données distribuées dans deux paramètres `x` et `y` ;\n",
    "- `.predict()` pour effectuer des prédictions sur des données ;\n",
    "- `.score()` pour obtenir une évaluation des prédictions obtenues.\n",
    "\n",
    "Nous nous limitons ici aux aspects essentiels des modèles, en leur laissant un maximum de degrés de liberté quand ils devraient être régularisés pour éviter les effets de sous-ajustement (*underfitting*) ou de sur-ajustement (*overfitting*). Les scores obtenus, souvent très satisfaisants, ne doivent pas égarer : d’une part les modèles sont entraînés sur des jeux de donnés complets quand il aurait fallu les tester et les valider sur autant de partitions ; d’autre part, ils ne sont jamais évalués relativement à des prédictions sur des données nouvelles, interdisant de fait de se rendre compte à quel point ils se généraliseraient mal.\n",
    "\n",
    "Pour comprendre en détail le fonctionnement de chaque modèle, nous renvoyons à [la documentation officielle](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6edcf5-f27c-4c3b-839d-68143ffc91e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## La régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d9ba4-d87a-4a46-a36a-abec3fed6c06",
   "metadata": {},
   "source": [
    "Sans doute la méthode la plus populaire en apprentissage supervisé, la régression linéaire consiste à trouver la fonction affine $y$ d’une variable explicative $x$. Elle permet de comprendre rapidement la relation entre deux variables. Pour autant, sa sensibilité aux données aberrantes et aux *outliers* peut l’empêcher d’observer une corrélation ; elle se révèle alors une hypothèse trop simple pour les données. On parle alors de sous-ajustement (*underfitting*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670852c-b4ce-4213-90f3-0533fa9d8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "_ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e66302-3bd8-47b1-9a5c-ad401163c5fa",
   "metadata": {},
   "source": [
    "Une fois le modèle entraîné, il est possible d’accéder au coefficient directeur de la droite et à son ordonnée à l’origine grâce aux attributs `coef_` et `intercept_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13bbf1-5d8b-4130-a7c2-036508f1cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774622f-b818-44eb-ac1b-07d411f01c2d",
   "metadata": {},
   "source": [
    "Tout comme en connaître la précision, ici grâce au coefficient de détermination $R^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118f17a-4b58-457c-8e59-5b7cdf625657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e50bb-c0ed-49c9-ab51-db94f3fb775e",
   "metadata": {},
   "source": [
    "L’interprétation des résultats d’une régression linéaire est extrêmement simple et généralement immédiate :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739d511-ff94-4f9f-85c0-c9a7852ad630",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.regplot(x=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daace4c-d1d2-47f9-b4a1-3e0991e71bff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## La régression polynomiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da7a82-8758-4500-b52d-d6d0c7dd882e",
   "metadata": {},
   "source": [
    "Le modèle de régression linéaire par ajustement affine ne s’est pas révélé une bonne hypothèse pour nos données d’entrée. Pour rappel, nous avions utilisé un polynôme de degré 3 pour générer les coordonnées en $y$.\n",
    "\n",
    "Lorsque, en visualisant les données, on observe une relation non-linéaire, l’hypothèse est qu’il doit exister d’autres caractéristiques pour l’expliquer que simplement $X$. *Scikit-Learn* met à disposition une classe `PolynomialFeatures` pour les ajouter à $X$ en fonction du polynôme considéré dans un paramètre `degree` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e059b-45e0-4ed5-b345-50e394673837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    LinearRegression(),\n",
    ")\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332a3c0-9851-4bfe-ab67-86019fef0a29",
   "metadata": {},
   "source": [
    "Le modèle s’ajuste bien mieux aux données que la précédente régression linéaire, ce qui se traduit immédiatement dans une augmentation de son *$R^2$ score* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca798f64-dd40-4fe4-8023-c0a50263828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X[:, 0], y=y[:, 0])\n",
    "ax.plot(sorted(X), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cac8f-6539-4894-8ec6-7eba7ab941b5",
   "metadata": {},
   "source": [
    "Il serait tentant de chercher à améliorer le *$R^2$ score* en augmentant le degré du polynôme, mais le risque serait d’ajuster trop bien le modèle (*overfitting*) aux données d’entraînement qu’il ne parviendrait plus à obtenir de bonnes prédictions sur des données nouvelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a512563-eabb-4038-9e36-2ae1faeee123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=15),\n",
    "    LinearRegression(),\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "ax = sns.scatterplot(x=X[:, 0], y=y[:, 0])\n",
    "ax.plot(sorted(X), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653146f-47d3-4011-9d5c-bc92e7d2005d",
   "metadata": {},
   "source": [
    "La courbe semble plus fortement attirée par les données avec un polynôme de très haut degré, ce qui, en quelque sorte, la rend dépendante vis-à-vis d’elles. Il faut s’attendre à voir le *$R^2$ score* diminuer sensiblement sur des données nouvelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57c0d7-2e02-4cc4-8fab-dd4354cfb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X_prime = np.random.rand(100, 1)\n",
    "y_prime = 2 - 3 * X_prime + 3 * X_prime ** 2 + 5 * X_prime ** 3 + np.random.rand(100, 1)\n",
    "\n",
    "y_pred = model.predict(X_prime)\n",
    "\n",
    "ax = sns.scatterplot(x=X_prime[:, 0], y=y_prime[:, 0])\n",
    "ax.plot(sorted(X_prime), sorted(y_pred))\n",
    "_ = ax.set_title(f\"R2 score = {model.score(X_prime, y_prime):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff34f8-f9c0-4f97-84c2-e726b5d864a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les K plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6511452-6aea-4f51-ac9f-1541c19cb979",
   "metadata": {},
   "source": [
    "Le modèle de recherche des $k$ plus proches voisins (*K-Nearest Neighbors*) consiste à rechercher, pour toute nouvelle entrée, les $k$ entrées les plus proches dans la base d’apprentissage. Il est par ailleurs possible de préciser $k$ grâce à un paramètre `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42daf523-5379-48c0-95e2-7bdfbecfb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors=3))\n",
    "_ = model.fit(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2c43a-6101-44f3-96fe-0c039cab5b45",
   "metadata": {},
   "source": [
    "Parmi les attributs intéressants de la classe, citons `classes_` et `feature_names_in_` qui permettent de garder une trace des différentes modalités ainsi que des variables explicatives utilisées dans le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07dec6-c11e-443d-b938-747827cf093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Modalités : {model.classes_}\",\n",
    "    f\"Variables explicatives : {model.feature_names_in_}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643591d0-cbeb-484d-8cbc-4c9658f65675",
   "metadata": {
    "tags": []
   },
   "source": [
    "Le score obtenu est le taux de prédictions exactes sur l’ensemble du jeu d’apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5f629-9263-4ccf-a14b-3bcd8f8ebe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af81233-6003-4e16-98a6-69e6cebbc2cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## La classification naïve bayésienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a7442-2bb5-4f86-a98a-ca4afda3e9b7",
   "metadata": {},
   "source": [
    "La classification naïve bayésienne repose sur l’idée qu’il existe une forte indépendance entre les variables explicatives d’un jeu de données et, partant, qu’elles contribuent individuellement autant les unes que les autres à la probabilité qu’une observation appartienne à une modalité. La contraposée implique qu’un classifieur naïf bayésien ne fournira pas de bons résultats si les variables sont effectivement corrélées entre elles.\n",
    "\n",
    "S’il existe plusieurs types de classifieurs, nous ne verrons que l’algorithme appelé *Gaussian Naïve Bayes* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f17ac-239a-4f00-a488-ee74216b2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = make_pipeline(preprocessor, GaussianNB())\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbded0-4a95-4b53-abac-3e4f332338ee",
   "metadata": {},
   "source": [
    "Si ce modèle est rapide à mettre en place, il est en revanche moins performant que d’autres algorithmes que les arbres de décision ou les séparateurs à vaste marge. Il a toutefois l’avantage de pouvoir être entraîné sur un faible volume de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8b6ee-c7fb-4a4a-945b-3872e483ab57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## La régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b882bd-a492-4292-bbcc-c69818372541",
   "metadata": {},
   "source": [
    "L’algorithme de régression logistique est très souvent employé dans les tâches de classification car il peut estimer rapidement la probabilité qu’une observation appartienne à une modalité particulière. Un paramètre `class_weight` permet de transmettre un dictionnaire de poids pour les modalités afin de pondérer la classification, ce qui peut se révéler utile pour corriger un biais de représentativité. Dans le cas contraire, toutes les étiquettes sont réputées avoir un poids de 1. Il est également possible de modifier l’algorithme utilisé par défaut pour l’optimisation de la régression avec un paramètre `solver`. Notons également un paramètre `max_iter` fixé par défaut à 100, qu’il peut être utilse d’augmenter afin de faire disparaître un avertissement `ConvergenceWarning` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bdad6-323c-4e91-8d92-e5524e9e69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "weights = {\n",
    "    \"Adelie\": .4,\n",
    "    \"Gentoo\": .2,\n",
    "    \"Chinstrap\": .4\n",
    "}\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(\n",
    "        max_iter=100,\n",
    "        class_weight=weights,\n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efb32b-8a57-4ff1-95e3-b124fba0a2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les séparateurs à vaste marge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0f8ef-3e6d-483c-bf0a-4a8521f0f548",
   "metadata": {},
   "source": [
    "Les séparateurs à vaste marge (SVM), autrement appelés machines à vecteur de support (*Support Vector Machines*) sont réputées pour leur excellence à traiter une multitude de tâches : régression ou classification (que les données soient linéaires ou non) mais aussi de la détection de données aberrantes. Ils sont par ailleurs très sensibles aux différences entre les échelles des variables, aussi il est fortement recommandé de normaliser toutes les données avant la phrase d’entraînement.\n",
    "\n",
    "Pour les tâches de classification, la classe à utiliser est `LinearSVC` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e100f-bceb-4bcc-9c34-ba772478a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearSVC()\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a4963-7940-43e5-8619-81e1f1e7abb6",
   "metadata": {},
   "source": [
    "Et pour les tâches de régression, `LinearSVR` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9af13-d5d4-497a-90f9-594eb99738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = LinearSVR()\n",
    "model.fit(X, y[:, 0])\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc85380-82c8-4ff9-9b21-d1ca0e7de0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVM pour la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d714c-cfa0-4445-a2f9-6cdcd3405db7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classification à marge rigide ou souple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b8acc-9f49-40ee-a8ea-e451c961ef06",
   "metadata": {},
   "source": [
    "Pour opérer la classification, les SVM vont effectuer un découpage linéaire des données en veillant à respecter une distance égale avec les observations les plus proches, appelée marge (paramètre `C`). Cette marge peut être rigide, imposant qu’aucune observation ne se trouve à l’intérieur, ou souple, autorisant des empiètements de marge. En théorie, il est préférable d’avoir peu d’empiètements de marge, mais une classification à marge rigide ne pourra être mise en place que si les observations sont linéairement séparables par une frontière nette, c’est-à-dire sans aucune observation en dehors de son espace. Pour cette raison, on optera dans la pratique plutôt pour un modèle permissif qui se généralisera sans doute mieux.\n",
    "\n",
    "Afin de représenter la séparation, prenons une extraction de nos données avec seulement deux modalités (*Adelie* et *Gentoo*), et entraînons un classifieur SVM à marge souple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568963c6-7aa8-443e-bab0-02f699737526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset with two features + target\n",
    "data = df.filter([\"flipper_length_mm\", \"body_mass_g\", \"species\"])\n",
    "data = data[data[\"species\"] != \"Chinstrap\"]\n",
    "X_c = data.drop(columns=\"species\")\n",
    "y_c = data[\"species\"]\n",
    "\n",
    "# standardisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_c)\n",
    "\n",
    "# soft margin SVM\n",
    "model = LinearSVC(C=1, loss=\"hinge\")\n",
    "_ = model.fit(X_scaled, y_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a8343-62a7-4ee7-8738-7e88cd8b0b08",
   "metadata": {},
   "source": [
    "Le modèle fournit le coefficient directeur $a$, composé ici de deux valeurs, une pour chaque variable explicative, ainsi que l’ordonnée à l’origine $b$ qui vont permettre de calculer les coordonnées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82b9fc-7180-4963-b682-dcb09a243941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope (two weights: one for each feature) and intercept\n",
    "a = model.coef_[0]\n",
    "b = model.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5931654-415b-4157-aa27-71c656f8e7cf",
   "metadata": {},
   "source": [
    "L’équation générale sous forme vectorielle de la limite de décision vaut :\n",
    "\n",
    "$$\\left < a, x \\right > + b = 0$$\n",
    "\n",
    "Si nous la convertissons sous forme linéaire, nous obtenons :\n",
    "\n",
    "$$\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "        a_1 & a_2\n",
    "    \\end{array} } \\right] \\times\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "    \\end{array} } \\right] + b = 0\n",
    "$$\n",
    "\n",
    "$$a_2x_2 = -a_1x_1 -b$$\n",
    "\n",
    "$$x_2 = - \\frac{a_1}{a_2} x_1 - \\frac{b}{a_2}$$\n",
    "\n",
    "Dans cette dernière forme, $a_1$ et $a_2$ sont les coefficients directeurs, $x_1$ sont les coordonnées sur l’axe des abscisses et $x_2$ les coordonnées sur l’axe des ordonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57a394-31d6-477b-a8fd-af28056e21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-line equation\n",
    "x_coords = np.linspace(-1, 1)\n",
    "y_coords = -(a[0] / a[1]) * x_coords - b / a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422e4d5-5f03-446f-a1cc-4100b5fb81bf",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à afficher le graphique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31353892-d291-483d-84a7-c161e2cbc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax = sns.lineplot(x=x_coords, y=y_coords, color=\"fuchsia\")\n",
    "ax = sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y_c)\n",
    "\n",
    "ax.set(xlabel=\"Body mass (g)\", ylabel=\"Flipper length (mm)\", title=\"Frontière de décision d’un SVM à marge souple (C=1)\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbab68-6fc7-4026-b28d-e40630ca9d1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classification pour des données non linéaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb7881-cff0-4e61-b144-a3519b86e639",
   "metadata": {},
   "source": [
    "Dans l’exemple précédent, les données étaient, à l’exception de quelques observations qui empiétaient dans l’espace de l’autre modalité, linéairement séparables, et la classe `LinearSVC` était indiquée pour résoudre rapidement la tâche d’apprentissage. Si nous reprenons le jeu de données initial avec les trois modalités, la séparation entre les données devient moins évidente :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b783369-21ea-437c-b0f5-43ff9acb00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.scatterplot(data=X_b, x=\"body_mass_g\", y=\"flipper_length_mm\", hue=y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd82b0a7-42de-4795-aa40-472fab5ddd58",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Un SVM à noyau polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c539fe9-272b-4a8e-ab94-f81b5f1638dd",
   "metadata": {},
   "source": [
    "Rien n’interdit de recourir à un SVM linéaire mais, dans le cas de notre exemple, nous voudrons sans doute rigidifier les marges pour éviter le risque de sous-ajustement et les erreurs de généralisation qu’il entraînerait. Comment alors ajuster mieux la frontière de décision ? La solution est d’utiliser un polynôme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d62f3-ae71-430b-99df-da0714b80a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(degree=3),\n",
    "    LinearSVC(C=10, max_iter=50000)\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5127b6a-0b2c-4ae9-b56b-883ee31780f6",
   "metadata": {},
   "source": [
    "Les résultats sont meilleurs mais au détriment de temps de calculs nécessairement plus longs : l’ajout de variables polynomiales nuit d’autant plus à l’équilibre de l’ensemble que nous aurions besoin d’un polynôme à degré très élevé pour s’adapter à des données complexes.\n",
    "\n",
    "Une astuce consiste à utiliser plutôt la classe `SVC` en lui associant un noyau polynomial avec le paramètre `kernel` et en contrôlant l’influence des polynômes de degré élevé avec un paramètre `coef0` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c417882-4be6-4382-b517-6a2722460e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    SVC(kernel=\"poly\", degree=6, C=10, coef0=1)\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc13ef-a9c7-4400-ba85-238c90873d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Un SVM à noyau radial gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aad98a-deb2-4c19-ba65-1fb5db05da57",
   "metadata": {},
   "source": [
    "Notre jeu de données ne se prête pas plus à une séparation avec une droite qu’avec une fonction polynomiale. Heureusement, une autre méthode lui correspond mieux. L’idée consiste à calculer une fonction de similarité pour mesurer la ressemblance entre une observation et un point de repère. Pour chaque observation, une nouvelle variable calculée : le temps de traitement d’un vaste *dataset* risque d’exploser.\n",
    "\n",
    "Comme dans le cas du noyau polynomial, il existe une astuce pour obtenir le résultat sans ajouter dans les faits les variables de similarité : fixer le paramètre `kernel` à `rbf`. Le point clé consiste alors à régler le paramètre $\\gamma$ qui définit la forme de la frontière de décision : de lisse avec une valeur faible, elle devient plus irrégulière avec une valeur forte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a078d30-8756-4e92-969c-fa20f2ed69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    SVC(kernel=\"rbf\", gamma=10, C=10)\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac174f1-5fe1-4879-a569-0529147894e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### `SVC` ou `LinearSVC` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4583d81-f353-4a0c-b8e1-2df280fd9de9",
   "metadata": {},
   "source": [
    "La classe `LinearSVC` ne repose pas sur la même librairie que `SVC` et sera nettement plus rapide lorsque les modalités sont linéairement séparables. Dans les autres cas, `SVC` donnera de meilleurs résultats : il s’adapte mieux aux données complexes et intègre *l’astuce du noyau*. Toutefois, dès lors que le volume du jeu d’entraînement devient important, on observe un ralentissement notable de l’apprentissage dû à la complexité de son algorithme. Cette restriction ne concerne pas le nombre de variables explicatives, surtout si elles sont dites creuses (avec peu de valeurs non nulles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf2bc8-9c31-4e6a-9799-8ac2b4f4e40f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVM pour la régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2be6c7-e858-415c-84f5-aff863933565",
   "metadata": {},
   "source": [
    "Pour résoudre des tâches de classification, les modèles SVM s’efforcent de séparer les données en délimitant une frontière la plus large possible tout en évitant les empiètements à l’intérieur. Dans le cas des tâches de régression, c’est l’inverse : les modèles SVM tentent le mieux possible de contenir les observations à l’intérieur du chemin qui la délimite. Le paramètre `epsilon` gouverne la largeur du chemin. Le modèle est alors réputé être insensible à $\\epsilon$ près, car rajouter des observations d’entraînement ne l’affecte pas.\n",
    "\n",
    "La classe `LinearSVR` est prévue pour des applications impliquant des données linéaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db39fd-387f-4384-bbbb-f5eca9628adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = LinearSVR(epsilon=0.5, C=10, max_iter=2000)\n",
    "model.fit(X, y[:, 0])\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384cc59-7f44-446e-84d4-3e2d82495c75",
   "metadata": {},
   "source": [
    "La classe `SVR` est quant à elle parfaite pour traiter des données non linéaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0c97e-2d15-4170-abcb-47b8261450ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR(kernel=\"poly\", degree=4, epsilon=0.5, C=1)\n",
    "model.fit(X, y[:, 0])\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d1ce4-3b26-4d22-9cdb-da1b361de213",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bffbe-4711-4740-8022-8d2177a9a7a6",
   "metadata": {},
   "source": [
    "Briques fondamentales des forêts aléatoires, les arbres de décision sont capables de s’adapter à des jeux de données complexes pour effectuer tout à la fois des tâches de classification et de régression. En les laissant libre pendant la phase d’apprentissage, ils arborent fièrement des résultats au-delà de toute espérance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14344ee4-ec5d-48b8-88a9-c4cdd103b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae732b-3ef7-4bb9-ab6e-6ce5be28fb3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interprétation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226cbd6-eae4-4dd2-92f8-e7fad78193dc",
   "metadata": {},
   "source": [
    "La promesse d’une précision élevée à l’entraînement cache toutefois une tendance au sur-ajustement des arbres de décision. S’ils ne sont pas régularisés en amont, ils provoqueront des erreurs de généralisation. L’intuition serait alors de jouer sur les hyperparamètres, mais ils font partie des modèles non paramétriques et n’en exposent pas avant l’apprentissage. Aussi, la seule manière de limiter leur degré de liberté est de contrôler quelques paramères de la classe :\n",
    "- `max_depth`, pour contraindre la profondeur de l’arbre ;\n",
    "- `min_samples_split`, pour indiquer le nombre minimum d’observations à considérer avant que le nœud se divise ;\n",
    "- `min_samples_leaf`, pour forcer les nœuds terminaux à contenir un minimum d’observations ;\n",
    "- `max_features`, pour renseigner le nombre de variables explicatives considérées à chaque nœud.\n",
    "\n",
    "Le score de précision peut diminuer fortement, au bénéfice toutefois d’une meilleure généralisation.\n",
    "\n",
    "Un autre avantage des arbres de décision, et non des moindres, est qu’ils nécessitent très peu de préparation des données, comme dans l’exemple ci-dessous où la variable *sex* a simplement été recodée en 0 et 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f4ffb-7fe8-47b9-8b9a-f15aa020a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = X_b.replace({\"male\": 0, \"female\": 1})\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_c, y_b)\n",
    "model.score(X_c, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8b08c-f5a7-4a61-9978-6b059fe1abbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Algorithme de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae21854-a2fb-4b8f-8dbe-1b976a2b091b",
   "metadata": {},
   "source": [
    "Le modèle des arbres de décisions repose sur l’algorithme CART (*Classification And Regression Trees*) qui ne produit que des arbres binaires, comme dans l’organigramme ci-dessous qui reprend le dernier modèle où la profondeur maximale est fixée à 3 niveaux :\n",
    "\n",
    "![Modélisation des décisions de l’algorithme](./images/penguin-tree.png)\n",
    "\n",
    "Au moment de l’entraînement, le modèle a lui-même évalué laquelle des trois variables explicatives entre *flipper_length_mm*, *body_mass_g* et *sex* était la plus discriminante pour effectuer la première division. Il calcule ensuite une frontière de décision et suppose une modalité qui repose sur l’indice de diversité de Gini (ou indice d’impureté) :\n",
    "\n",
    "$$G_i = 1 - \\overset{n}{\\underset{k=1}{\\sum}} P_{i,k^2} $$\n",
    "\n",
    "Concrètement, tout en haut de l’arbre, l’algorithme se demande si les nageoires d’un manchot sont inférieures ou égales à 206,5 mm. Si tel est le cas, il émet l’hypothèse qu’il s’agit d’un manchot Adélie et passe la main au nœud inférieur situé à l’embranchement de gauche. Ici, la variable explicative est la même, mais la frontière est abaissée à 192,50 mm. Si l’on veut calculer l’indice d’impureté à ce niveau :\n",
    "\n",
    "$$G = 1 - \\left(\\frac{144}{208}\\right)^2 - \\left(\\frac{63}{208}\\right)^2 - \\left(\\frac{1}{208}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff4af2-4974-4539-b2b7-65448a08cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gini = 1 - (144/208) ** 2 - (63/208) ** 2 - (1/208) ** 2\n",
    "print(f\"Indice d’impureté de Gini : {gini:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61781d27-733f-46c2-a77d-13ffddf4622e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Estimation des probabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf1b84-1d64-4379-bae1-e8510407c08a",
   "metadata": {},
   "source": [
    "Pour en terminer sur les arbres de décision, notons la présence d’une méthode `.predict_proba()` qui renvoie l’estimation des probabilités d’appartenance à une modalité ou l’autre pour une observation. Prenons un manchot mâle de 3,950 kg et dont les nageoires mesurent 201 mm et suivons l’arbre de décision :\n",
    "1. Nageoires inférieures ou égales à 206,5 mm ? **Vrai**\n",
    "2. Nageoires inférieures ou égales à 192,5 mm ? **Faux**\n",
    "3. Masse corporelle inférieure ou égale à 4,175 kg ? **Vrai**\n",
    "\n",
    "L’algorithme devrait répondre qu’il s’agit d’un manchot à jugulaire (*Chinstrap*) avec une probabilité de 55 % (38 sur 69 observations) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3496f8-3b6f-4014-ac61-ba2e3a7afc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin = pd.DataFrame({\n",
    "    \"flipper_length_mm\": [201],\n",
    "    \"body_mass_g\": [3950],\n",
    "    \"sex\": [0]\n",
    "})\n",
    "print(\n",
    "    f\"Modalités : { ' '.join(model.classes_) }\",\n",
    "    f\"Probabilités : { model.predict_proba(penguin)[0] }\",\n",
    "    f\"Espèce : { model.predict(penguin)[0] }\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61108a87-fb2b-442c-a538-dc3a526d87ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Arbres de décision pour des tâches de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc53766-2848-4702-b5dc-7f382845461a",
   "metadata": {},
   "source": [
    "Les arbres de décision s’adaptent tout aussi bien aux tâches de régression. Au lieu de prédire une modalité, il prédit une valeur et décide de l’embranchement à suivre en prenant comme fonction de coût l’erreur quadratique moyenne (MSE) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871287c3-f36a-4c9c-a9f2-b6aaef3e4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5974a80-8de7-4291-821e-8572b0017fe6",
   "metadata": {},
   "source": [
    "Comme lorsqu’ils sont mobilisés pour des tâches de classification, les modèles avec arbres de décision ont une nette tendance au sur-ajustement. Le score $R^2$ très élévé tend à le suggérer. Pour le régulariser et améliorer ainsi ses performances au moment de la confrontation avec des données inédites, il faut chercher les bons ajustements des hyperparamètres.\n",
    "\n",
    "Construisons un modèle qui se généralisera mieux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce46ff-b75f-4c96-aeb7-f1a35551832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "model_b.fit(X, y)\n",
    "model_b.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1196e-5d3f-450d-b50b-a6036151b44c",
   "metadata": {},
   "source": [
    "Effectuons à présent des prédictions à l’aide des deux modèles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b8ac8-382f-4855-ba05-9395481a53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_over = model.predict(X)\n",
    "y_pred = model_b.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33bfbf-113d-4f39-84cd-f5c641ddfded",
   "metadata": {},
   "source": [
    "Et affichons en dernier lieu les graphiques afin de comparer les courbes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48107046-89e2-42da-8643-ec8b420eb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,5))\n",
    "\n",
    "# overfitted model\n",
    "sns.scatterplot(x=X[:, 0], y=y[:, 0], ax=ax1)\n",
    "sns.lineplot(x=X[:, 0], y=y_pred_over, ax=ax1, color=\"lightsalmon\")\n",
    "\n",
    "# more reasonable model\n",
    "sns.scatterplot(x=X[:, 0], y=y[:, 0], ax=ax2, color=\"orange\")\n",
    "sns.lineplot(x=X[:, 0], y=y_pred, ax=ax2, color=\"green\")\n",
    "\n",
    "ax1.set(title=\"Un modèle sur-ajusté ($R^2$ = 1)\")\n",
    "ax2.set(title=\"Un modèle plus convaincant ($R^2$ = 0,95)\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19d37f-622d-4212-9da1-678ce83d5aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les forêts aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d83e4b-1f0f-49d4-97f3-a4264a4f254a",
   "metadata": {},
   "source": [
    "Les forêts aléatoires, de l’anglais *random forest*, proposent un algorithme qui entraîne plusieurs arbres de décision en variant légèrement le jeu d’apprentissage. Elles reposent sur le principe de la sagesse des foules où l’erreur du groupe est réputée être inférieure à celle des individus qui le composent.\n",
    "\n",
    "Dans *Scikit-Learn*, la classe `RandomForestClassifier` permet de mobiliser des forêts aléatoires pour des tâches de classification. La modalité retenue sera alors issue d’un vote majoritaire entre les différents arbres de décision. En plus des paramètres classiques de la classe `DecisionTreeClassifier`, `n_estimators` règle le nombre d’arbres sollicités et `n_jobs` indique le nombre de CPU à mobiliser (-1 pour tous les utiliser) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc7105-d401-45e5-bff8-f9dffdd74a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    ")\n",
    "model.fit(X_b, y_b)\n",
    "model.score(X_b, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f809d4f-b694-46bf-aa44-f73af34a1ff9",
   "metadata": {},
   "source": [
    "Très pratique, soulignons la propriété `feature_importances_` pour connaître l’importance de chaque variable explicative dans l’estimation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f41066-6a54-49a2-841b-f897fe6f9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[1].feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cb8f3-62dd-4d6b-8b34-7bbdd6bdae5c",
   "metadata": {},
   "source": [
    "Et pour les tâches de régression, il faut appeler la classe `RandomForestRegressor` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4eb08-52f9-42bc-80a1-ebeb832c6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    min_samples_leaf=10,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X, y[:, 0])\n",
    "model.score(X, y[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e68a25-11db-4195-8e14-314b3da0d833",
   "metadata": {},
   "source": [
    "La précision des résultats obtenus peut surprendre : elle est au mieux égale à celle d’un arbre de décision classique. Rappelons déjà que nos exemples apprennent depuis le jeu de données complet ; autrement, c’est aussi le signe que nos forêts aléatoires sont mal réglées. Il ne faut effectivement pas oublier que les abres de décisions ont une forte tendance au sur-ajustement et qu’un ensemble d’estimateurs défaillants ne saurait produire de prédiction efficace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

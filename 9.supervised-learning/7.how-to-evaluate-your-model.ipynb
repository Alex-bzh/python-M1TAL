{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480e1ba-edcc-4dfd-bc9e-547e795be9f4",
   "metadata": {},
   "source": [
    "# Les méthodes d’évaluation d’un modèle prédictif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a87e26-5f2c-4b9b-ac75-0b45258184b1",
   "metadata": {},
   "source": [
    "Après des heures à paramétrer au mieux un modèle d’apprentissage avec la certitude d’avoir écarté les biais qui pourraient orienter les résultats – rappelons qu’un mauvais modèle peut fournir de très mauvais résultats avec une précision étonnante –, les premières prédictions sortent de la machine et nous souhaitons évaluer leur qualité afin de le passer en production ou non.\n",
    "\n",
    "Bien entendu, le cas présenté plus haut ne vaut que pour sa généralité ; dans la pratique, les méthodes d’évaluation sont présentes à chaque étape de la programmation d’un modèle si bien que presque aucun choix ne devrait être pris sans validation par une métrique ou une autre. Comme nous nous sommes concentré·es sur deux types d’algorithmes, nous n’aborderons que les méthodes d’évaluation pour les tâches de régression et de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1627dc-dd09-4814-b418-4b83cc091234",
   "metadata": {},
   "source": [
    "## Mesurer une erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1dab6e-842f-4271-99fd-b86703427985",
   "metadata": {},
   "source": [
    "Par métrique, on entend une façon d’évaluer la qualité d’une prédiction par mesure de la distance entre la réalité observée et la valeur calculée par un algorithme. Si l’on souhaite par exemple prédire la note d’un élève au prochain examen de français en se basant uniquement sur sa moyenne dans la matière – disons 12 –, l’algorithme de prédiction vaudra simplement :\n",
    "\n",
    "$$\\hat{y} = \\mu$$\n",
    "\n",
    "L’élève obtient finalement une note de 11. Pour mesurer l’erreur de la prédiction, il suffit de soustraire $\\hat{y}$ de $y$, soit un résultat de $-1$. Remarquons que si sa note avait été de 13, le résultat aurait été positif : $13 - 12 = 1$. Or, $-1$  et $+1$ étant situés à égales distances de la prédiction, ils représentent la même réalité géométrique. Dans les deux cas, l’erreur est réputée être de $1$. On utilise donc plutôt une formule impliquant les valeurs absolues :\n",
    "\n",
    "$$e = \\lvert y - \\hat{y}\\rvert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b623b68-2633-4669-a07f-7b11e681c8ea",
   "metadata": {},
   "source": [
    "## Métriques pour les tâches de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2a6dd-4409-4f72-9c92-48a8eac2a6da",
   "metadata": {},
   "source": [
    "Dans l’exemple de l’introduction, il s’agissait simplement de calculer l’erreur pour un couple unique prédiction/résultat. Qu’en serait-il si nous avions une série de prédictions et une série de résultats ? Plutôt que de calculer indépendamment les erreurs de chaque prédiction, nous préférerions obtenir une mesure de l’ensemble.\n",
    "\n",
    "Et pour corser le tout, il existe plusieurs métriques qui ne répondent pas tout à fait aux mêmes enjeux. Choisir la plus adaptée à la situation peut ainsi devenir une nécessité pour ajuster plus finement encore le modèle.\n",
    "\n",
    "Prenons le cas fictif de la pluviométrie au-dessus de la commune de Pont-Aven avec d’un côté les précipitations mesurées en millimètres pour les mois de janvier à mai 2022 et, de l’autre, des prédictions imaginaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17af8d-4bf5-4056-a2fd-cef33b2370b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# series\n",
    "series = {\n",
    "    \"months\": [\"Jan\", \"Feb\", \"March\", \"April\", \"May\"],\n",
    "    \"rainfall\": [70, 65, 55, 50, 9],\n",
    "    \"predictions\": [35, 60, 75, 45, 20]\n",
    "}\n",
    "# dataframe\n",
    "df = pd.DataFrame(series)\n",
    "\n",
    "# column 'months' still an id var, while two others are registered in a col 'Measure'\n",
    "df2 = pd.melt(df, id_vars=\"months\", var_name=\"Measure\", value_name=\"mm\")\n",
    "\n",
    "# graph\n",
    "_ = sns.lineplot(data=df2, x=\"months\", y=\"mm\", hue=\"Measure\", marker=\"o\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703dd24-a947-4099-b392-d9e05f95d9e2",
   "metadata": {},
   "source": [
    "### Le coefficient de détermination linéaire de Pearson ($R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d95eff-6b04-4457-9766-0ebb8da80d76",
   "metadata": {},
   "source": [
    "Le $R^2$ est un score qui mesure la qualité de la prédiction d’un modèle de régression linéaire en évaluant la variance d’une variable par rapport à une autre variable. Il est défini par la relation suivante pour un résultat généralement compris dans l’intervalle $[0,1]$ :\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^k(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^k(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Son analyse est très intuitive mais elle implique deux critères :\n",
    "- le modèle est linéaire ;\n",
    "- une seule variable explicative est concernée.\n",
    "\n",
    "Un $R^2$ de 1.0 est un score parfait quand un score de 0.0 indiquerait que le modèle prédit toujours la valeur attendue (la moyenne). Un score négatif reste possible mais serait révélateur d’une erreur de méthodologie (données arbitrairement mauvaises).\n",
    "\n",
    "Dans le cas de notre exemple, la prédiction n’est clairement pas linéaire, aussi le calcul du $R^2$ ne devrait pas servir pour l’évaluation de notre modèle. À titre d’exercice, voyons ce qu’il donne en invoquant la fonction `r2_score()` du module `metrics` de *Scikit-learn* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f555455-f42a-4820-97f6-e47ae4877d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(df.rainfall, df.predictions)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f038b-87db-45ba-a4db-b495d500856b",
   "metadata": {},
   "source": [
    "### L’erreur quadratique moyenne (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f7b56-3f10-4154-8dbd-a010cf5f7143",
   "metadata": {},
   "source": [
    "La MSE (*mean square error*) et sa cousine, la RMSE (*root mean square error*), sont les deux métriques les plus couramment utilisées en *machine learning*. La MSE calcule la moyenne des carrés des erreurs selon la formule :\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{k}\\sum_{i=0}^{k-1}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Comme entre en jeu un calcul au carré, la MSE pénalise plus fortement les grandes erreurs et, dans le même ordre d’idée, sera très sensible aux données aberrantes (*outliers*). La fonction dans *Scikit-learn* est `mean_squared_error()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89b185-ef58-42c7-ae8d-35fbe2399eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(df.rainfall, df.predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59c71a-d7df-4957-8864-55944ab2bbe0",
   "metadata": {},
   "source": [
    "### La racine de l’erreur quadratique moyenne (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ce050-8012-49eb-882c-c9a6edc7b5c9",
   "metadata": {},
   "source": [
    "Plus facile à interpréter que la MSE, la RMSE (*root mean square error*) s’exprime dans l’unité de la variable à prédire en extrayant la racine carrée de la MSE :\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{ \\frac{1}{k}\\sum_{i=0}^{k-1}(y_i - \\hat{y}_i)^2 }$$\n",
    "\n",
    "À noter qu’elle souffre des mêmes limites que la MSE : une grande sensibilité aux *outliers* ainsi qu’une incidence forte sur les erreurs importantes. Pour la calculer avec *Scikit-learn*, il suffit de prendre la racine carrée de la MSE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0104c8-d3e5-4512-b37d-66f62a21163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power of 0.5 = square root\n",
    "rmse = mse ** 0.5\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083a903-fe8e-4ac6-a481-91e499837d39",
   "metadata": {},
   "source": [
    "### L’erreur absolue moyenne (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc387dfe-36f0-4ee6-b5bc-b3be99f4eba7",
   "metadata": {},
   "source": [
    "Quand les valeurs extrêmes d’un jeu de données sont quantitativement importantes, la RMSE pourrait conduire à des erreurs d’interprétation. Dans un tel cas de figure, la MAE (*mean absolute error*) peut lui être préférée : en calculant la moyenne de valeurs absolues, elle ne pénalise plus autant les grandes erreurs et se rend moins sensible aux données aberrantes. La formule de la MAE vaut ainsi :\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{k}\\sum_{i=0}^{k-1} \\lvert y_i - \\hat{y}_i\\rvert$$\n",
    "\n",
    "Dans *Scikit-learn*, la fonction `mean_absolute_error()` se charge du calcul :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b0a8c-b1d0-4201-bceb-60319204c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(df.rainfall, df.predictions)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c8134-16c7-4a70-9ede-b37cc0d48d56",
   "metadata": {},
   "source": [
    "## Métriques pour les tâches de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd51d7e-e304-486d-ab12-33360a0fe851",
   "metadata": {},
   "source": [
    "Dans la grande famille des tâches de classification, nous reconnaissons trois catégories :\n",
    "- **la classification binaire** (l’observation appartient-elle à la classe cible ou non ?) ;\n",
    "- **la classification multi-classes** (parmi toutes, à quelle classe l’observation appartient-elle ?) ;\n",
    "- **la classification multi-étiquettes** (l’observation appartient-elle à plusieurs classes ?).\n",
    "\n",
    "Là encore, selon la nature de la tâche, nous ne choisirons pas forcément la même métrique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa52751-d394-47a2-b16a-2117d88035ec",
   "metadata": {},
   "source": [
    "### Mesures générales de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07799021-aca8-4588-b4cd-7010b1e69f5f",
   "metadata": {},
   "source": [
    "Évaluer les performances d’un classificateur étant plus complexe que pour les tâches de régression, il est nécessaire d’aborder en premier lieu certaines généralités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecceb17-2c23-41ee-8754-1d8dbb771b0b",
   "metadata": {},
   "source": [
    "#### L’exactitude (*accuracy*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b40bd-f0a4-4e93-9578-a57c33836d26",
   "metadata": {},
   "source": [
    "La toute première est de ne jamais oublier qu’un très mauvais classificateur peut obtenir des résultats stupéfiants. Prenons l’exemple d’un jeu de données factice qui comporte cent observations étiquetées selon deux modalités : *chat* ou *pas chat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d63299-6307-4839-a654-bceceac127a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# 5 out of an hundred are cats\n",
    "dataset = [\n",
    "    \"cat\" if i < 5 else \"not cat\"\n",
    "    for i in range(100)\n",
    "]\n",
    "# every day I'm shufflin\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d717ab-edf6-4e56-b729-83e6721b7ac5",
   "metadata": {},
   "source": [
    "Générons une autre liste pour les prédictions avec la seule étiquette *not cat* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18d07-fba3-4ba3-b726-113560a3c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [ \"not cat\" for i in range(100) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e79d89-67df-46df-a6a3-f91b1357c4ee",
   "metadata": {},
   "source": [
    "Et mesurons la performance de notre algorithme prédictif grâce au score d’exactitude (*accuracy*) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2588bb-7ae8-4389-ab30-4035f969005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(dataset, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed7767-033d-46ca-b7f1-2aa06a54bde7",
   "metadata": {},
   "source": [
    "**95 % !** Un taux d’exactitude à faire pâlir les diseuses de bonne aventure, non ? Pour cette raison, on ne se satisfera jamais du score d’exactitude, quitte même à s’en méfier dès que les jeux de données sont asymétriques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebdb8d-d01e-4e01-acf7-e16cfa98b8a4",
   "metadata": {},
   "source": [
    "#### Une matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080733f-4bcb-47a2-a1dc-b354a84b6962",
   "metadata": {},
   "source": [
    "La matrice de confusion repose sur un principe simple : compter le nombre de fois où les observations ont été bien ou mal étiquetées. Elle révèle ainsi quatre informations essentielles :\n",
    "- **les vrais positifs** (*true positive*), le classificateur a repéré qu’il s’agissait d’un chat ;\n",
    "- **les vrais négatifs** (*true negative*), le classificateur a repéré qu’il ne s’agissait pas d’un chat ;\n",
    "- **les faux positifs** (*false positive*), le classificateur a cru qu’il s’agissait d’un chat ;\n",
    "- **les faux négatifs** (*false negative*), le classificateur aurait dû voir qu’il s’agissait d’un chat.\n",
    "\n",
    "Préparons un jeu de données aléatoire avec la fonction `make_classification()` du module `sklearn.datasets` et effectuons des prédictions à partir d’un modèle de classification naïve bayésienne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9a4cf-8b14-4789-bb6a-061b459404f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# make dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "# make train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "# model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314b3ce-d27c-437b-8d75-68133df117a3",
   "metadata": {},
   "source": [
    "La matrice de confusion s’obtient avec la fonction `confusion_matrix()` du module `sklearn.metrics` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b262bba-de32-4549-b2be-39668fb6a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44becc-7d76-47bb-ad4b-7f2cbbec5a5b",
   "metadata": {},
   "source": [
    "Chaque ligne correspond à une classe réelle et chaque colonne à une classe prédite avec, sur la première ligne, la classe négative et, sur la seconde, la classe positive, tel que dans le tableau suivant :\n",
    "\n",
    "|prédites/réelles|Classe négative|Classe positive|\n",
    "|-|:-:|:-:|\n",
    "|Classe négative|TN (85)|FP (9)|\n",
    "|Classe positive|FN (3)|TP (103)|\n",
    "\n",
    "Une méthode `.ravel()` permet de récupérer chacun de ces indicateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddfdcf-b7ef-4ce1-88b5-b2e774a36792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c0098-a1b0-4d75-badc-38ab598805d5",
   "metadata": {},
   "source": [
    "Notons également l’existence d’une classe `ConfusionMatrixDisplay` pour afficher la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbe01a-5242-4857-8164-954d3a739fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels=model.classes_)\n",
    "_ = display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce4532-1644-42e9-b9c2-14f63c7b5823",
   "metadata": {},
   "source": [
    "En guise de conclusion, signalons que cette matrice de confusion détermine plusieurs scores : la précision, la sensibilité (ou rappel) et le score $F_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608cdd3-4ab0-485a-bfed-24987699aa9d",
   "metadata": {},
   "source": [
    "### Classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8933d",
   "metadata": {},
   "source": [
    "Dans le cas d’un projet basé sur un classificateur binaire, on fera toujours appel à une matrice de confusion à partir de laquelle on obtiendra d’autres métriques plus parlantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75faa4",
   "metadata": {},
   "source": [
    "#### La précision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b3e65",
   "metadata": {},
   "source": [
    "La précision s’intéresse à l’exactitude des prédictions positives. Elle se calcule en effectuant le rapport entre les vrais positifs – les fois où le classificateur a correctement étiqueté la classe positive, et la somme de toutes les prédictions positives – incluant donc les fois où le classificateur s’est trompé. La formule vaut :\n",
    "\n",
    "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "Soit, pour notre exemple à partir du classificateur naïf bayésien :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae676a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faeb6f2",
   "metadata": {},
   "source": [
    "Concrètement, la précision signifie que notre classificateur a raison dans 91,96 % des cas quand il prédit la classe positive (1).\n",
    "\n",
    "Il est à noté que *Scikit-Learn* met à disposition une fonction `precision_score()` dans le module `metrics` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154eb82",
   "metadata": {},
   "source": [
    "#### Le rappel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca874812",
   "metadata": {},
   "source": [
    "La métrique de précision d’un classificateur s’accompagne toujours du rappel (*recall*), ou sensibilité, qui détermine le taux de classes positives qu’il a correctement étiquetées. Elle s’obtient avec l’équation :\n",
    "\n",
    "$$\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "Dans notre exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44580d71",
   "metadata": {},
   "source": [
    "En termes plus parlants, notre classificateur parvient à détecter correctement 97,17 % des observations de la classe positive (1).\n",
    "\n",
    "Comme pour la précision, il existe une fonction dans *Scikit-Learn* pour calculer directement le rappel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22729ad5",
   "metadata": {},
   "source": [
    "#### Le score $F_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a568b3a",
   "metadata": {},
   "source": [
    "Entraînons un nouveau classificateur, basé cette fois-ci sur un arbre de décision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39a007",
   "metadata": {},
   "source": [
    "Examinons sa matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test, y_pred)\n",
    "_ = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels=model.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99635dea",
   "metadata": {},
   "source": [
    "Et ressortons les métriques de précision et de rappel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\n",
    "    f\"Précision : {precision:.2%}\",\n",
    "    f\"Rappel : {recall:.2%}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79cadf",
   "metadata": {},
   "source": [
    "Dans cet exemple, la précision de l’arbre de décision est supérieure à celle du classificateur naïf bayésien alors que son rappel est inférieur. Lequel des deux choisir ? Pour aider à la décision, il existe une autre métrique, le score $F_1$, qui effectue la moyenne harmonique entre les deux :\n",
    "\n",
    "$$F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "La moyenne harmonique donnant plus d’important aux valeurs faibles, un classificateur ne pourra être bien noté par cette métrique que si sa précision et son rappel sont élevés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7115351",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_bayes = 2 * ((91.96 * 97.17) / (91.96 + 96.17))\n",
    "F1_tree = 2 * ((94.39 * 95.28) / (94.39 + 95.28))\n",
    "\n",
    "print(\n",
    "    f\"Score F1 du classificateur naïf bayésien : {F1_bayes:.2f} %\",\n",
    "    f\"Score F1 de l’arbre de décision : {F1_tree:.2f} %\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3ed9a",
   "metadata": {},
   "source": [
    "Une fonction de *Scikit-Learn* permet d’obtenir directement le résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "F1_tree = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Score F1 de l’arbre de décision : {F1_tree:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74064fb9",
   "metadata": {},
   "source": [
    "Dans cet exemple, toutes choses étant égales par ailleurs, l’arbre de décision semble fournir le meilleur compromis précision/rappel. Pour autant, l’objet de votre étude lui-même pourrait dicter de privilégier l’une ou l’autre.\n",
    "\n",
    "Si nous élaborons un classificateur de spams, nous préférerons sans doute un modèle avec un fort rappel (aucun spam ne passe) même si sa précision laisse à désirer (certains mails sont faussement identifiés comme spams). À l’inverse, si nous cherchons un classificateur qui épargne nos oreilles de chansons nocives, nous serons prêt·es à quelques sacrifices : de bonnes chansons seront écartées mais au moins nous serons préservé·es de la soupe qu’écoute la jeunesse actuelle !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1e626",
   "metadata": {},
   "source": [
    "#### La courbe ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed408b",
   "metadata": {},
   "source": [
    "Une autre métrique couramment utilisée afin d’évaluer la performance d’un classificateur binaire est la courbe ROC (*Receiver Operating Characteristic*) ou courbe d’efficacité du récepteur. Cela consiste à croiser le taux de vrais positifs avec le taux de faux positifs (le rappel) et d’observer l’aire sous la courbe. Un modèle purement aléatoire serait proche de 0,5 quand un modèle parfait afficherait 1.\n",
    "\n",
    "Afin d’en comprendre le fonctionnement, entraînons un nouveau modèle, basé sur une régression logistique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77b8be",
   "metadata": {},
   "source": [
    "Grâce à la fonction `cross_val_predict()`, nous effectuons une validation croisée à $K$ passes qui nous renvoie les prédictions pour chaque bloc de test. L’argument `method` fixé à `predict_proba` permet de ressortir des probabilités pour chaque classe plutôt que la classe prédite :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(model, X_train, y_train, cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e57783",
   "metadata": {},
   "source": [
    "Récupérons à présent les scores pour la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = y_pred[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dcffa",
   "metadata": {},
   "source": [
    "*Scikit-Learn* donne accès aux fonctions `roc_curve()` et `roc_auc_score()` pour, dans l’ordre, obtenir la courbe ROC et l’aire sous la courbe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "roc_auc = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48acdb",
   "metadata": {},
   "source": [
    "Il est ensuite possible d’afficher le résultat avec, en pointillés, ce que donnerait un classificateur purement aléatoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = sns.lineplot(x=fpr, y=tpr)\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(f\"ROC AUC : {roc_auc:.4f }\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=.2)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f355ef0",
   "metadata": {},
   "source": [
    "#### Le compromis précision/rappel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc0fae",
   "metadata": {},
   "source": [
    "La courbe ROC aura parfois la mauvaise tendance à nous faire croire que notre classificateur est parfaitement paramétré. Une autre courbe, nommée PR pour *precision-recall*, est à préférer si la classe positive est rare ou si l’on attache plus d’importance aux fois où le classificateur s’est trompé (les faux positifs et négatifs).\n",
    "\n",
    "Le module `sklearn.metrics` expose une fonction `precision_recall_curve()` qui renvoie les scores de précision et de rappel en fonction de seuils (ici, des probabilités) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec872df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19309bd9",
   "metadata": {},
   "source": [
    "Croisons ensuite les précisions et les rappels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ac874",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lineplot(x=recalls, y=precisions)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR curve\")\n",
    "plt.plot([0.85, 0.85], [0.94, 0.5], 'k--', alpha=.2)\n",
    "plt.plot([0, 0.85], [0.94, 0.94], 'k--', alpha=.2)\n",
    "plt.text(0.87, 0.97, \"P : 94 %\")\n",
    "plt.text(0.87, 0.95, \"R : 85 %\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fd9f5",
   "metadata": {},
   "source": [
    "On observe sur ce graphique que la précision chute après 85 % de rappel. Si nous voulons une meilleure précision, elle se fera au détriment du rappel et inversement.\n",
    "\n",
    "Il peut aussi être utile de représenter la précision et le rappel en fonction du seuil :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lineplot(x=thresholds, y=precisions[:-1], label=\"Precision\")\n",
    "_ = sns.lineplot(x=thresholds, y=recalls[:-1], label=\"Recall\")\n",
    "\n",
    "plt.plot([0.63, 0.63], [0, 0.94], 'k--', alpha=.2)\n",
    "plt.text(0.64, 0.98, \"P : 94 %\")\n",
    "plt.text(0.64, 0.86, \"R : 85 %\")\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c581bfa",
   "metadata": {},
   "source": [
    "La projection de la ligne pointillée révèle le seuil à choisir pour obtenir cette précision de 94 %. Plus finement, nous pouvons sélectionner les indices qui correspondent à cette contrainte avec la fonction `argmax()` de *Numpy* qui renvoie l’indice du premier élément qui respecte la condition de précision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "i_94 = np.argmax(precisions >= 0.94)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09077c",
   "metadata": {},
   "source": [
    "À présent, nous pouvons sélectionner le seuil le plus bas qui réponde à la précision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_94 = thresholds[i_94]\n",
    "\n",
    "print(f\"Seuil de probabilité pour une précision de 94 % : { t_94 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea6160",
   "metadata": {},
   "source": [
    "Plus haut, nous avons appelé la fonction `cross_val_predict()` pour obtenir dans une variable `y_scores` des prédictions sur le jeu d’entraînement. Repérons les observations qui ont une probabilité au-dessus du seuil qui répond à la précision de 94 % :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_94 = (y_scores >= t_94)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45956cd",
   "metadata": {},
   "source": [
    "Vérifions pour se rassurer que les valeurs de précision et de rappel correspondent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(y_train, y_train_94)\n",
    "r = recall_score(y_train, y_train_94)\n",
    "f1 = f1_score(y_train, y_train_94)\n",
    "\n",
    "print(\n",
    "    f\"Précision : {p:.2%}\",\n",
    "    f\"Rappel : {r:.2%}\",\n",
    "    f\"F1 score : {f1:.2%}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fe178",
   "metadata": {},
   "source": [
    "Grâce à ces manipulations, il est assez aisé de paramétrer un classificateur avec un certain taux de précision. Attention toutefois à ne jamais négliger le rappel ! Dans notre exemple, le F1 score est descendu en dessous des 90 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6461376-be1f-4428-999f-577fb70123d9",
   "metadata": {},
   "source": [
    "### Classification multi-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffffde-3600-4936-9266-babdc2b73cf2",
   "metadata": {},
   "source": [
    "Lorsqu’un classificateur binaire prédit l’appartenance d’une observation à une classe ou à une autre, un classificateur multi-classes détermine laquelle, parmi toutes, a sa préférence. Et, pour réaliser une telle tâche, il suffit d’entraîner plusieurs classificateurs binaires.\n",
    "\n",
    "Mettons que nous entraînons une machine pour détecter si telle musique appartient au mouvement punk, au genre métal, au grunge ou encore au hardcore. Deux stratégies se présentent :\n",
    "\n",
    "- OvA (*One versus All*), qui consiste à décider avec un premier classificateur si la musique en question appartient à la première classe (punk) ou non, puis, avec un deuxième classificateur, si c’est du métal ou non, etc.\n",
    "- OvO (*One versus One*), qui consiste à entraîner $\\frac{n(n-1)}{2}$ classificateurs, soit 6 pour notre objet d’étude fictif :\n",
    "    - punk ou métal ?\n",
    "    - punk ou grunge ?\n",
    "    - punk ou hardcore ?\n",
    "    - métal ou grunge ?\n",
    "    - métal ou hardcore ?\n",
    "    - grunge ou hardcore ?\n",
    "\n",
    "On peut penser intuitivement qu’il est coûteux d’entraîner de nombreux classificateurs plutôt qu’un seul lorsque les classes sont multiples, mais certains algorithmes comme les SVM manquent de performance face à des jeux d’entraînement de grande taille. Dans ce cas, la stratégie à privilégier sera clairement la OvO quand, dans la plupart des autres, on préférera la OvA.\n",
    "\n",
    "À cette subtilité, il convient de rajouter que certains algorithmes gèrent nativement la classification multi-classes (les forêts aléatoires ou la classification naïve bayésienne) quand d’autres sont exclusivement binaires (les SVM ou la régression logistique). Heureusement, si l’on invoque l’un de ces derniers algorithmes pour une tâche multi-classes, *Scikit-Learn* le comprend imédiatement et applique l’une des deux stratégies. Par exemple, avec un SVM, il utilisera une stratégie OvO.\n",
    "\n",
    "Entraînons un classificateur SVM à partir d’un jeu de données avec quatre modalités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5245c0b-4a72-44bb-a4ec-272d45859c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# make dataset\n",
    "X, y = make_classification(n_samples=10000, n_classes=4, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "# make train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "# model\n",
    "model = SVC()\n",
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd011e-5dd5-4bb5-b48a-a20a29491360",
   "metadata": {},
   "source": [
    "Prenons la première observation du jeu de test afin d’effectuer une prédiction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3563c9e-5290-478b-9b09-c1df1163c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([ X_test[0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010faee4-a53a-43cc-b1e4-725b6d6b442d",
   "metadata": {},
   "source": [
    "Les algorithmes basés sur une fonction de décision exposent une méthode `.decision_function()` afin de consulter les scores attribués à chaque modalité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f21ca-3640-4d3c-857c-837612472e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the decision is made?\n",
    "model.decision_function([ X_test[0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a168a68-2d39-4aa6-bfaa-981dfa4190eb",
   "metadata": {},
   "source": [
    "Le score le plus haut est bien attribué à la dernière classe, positionnée à l’indice 3. Pour s’assurer que l’indice 3 correspond bien à la classe étiquetée *3*, ne pas oublier de consulter la propriété `.classes_` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61b556-a863-4ef3-9592-e54fd28015ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are ordered, could have been other way!\n",
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a22d3-5b24-4a9f-8243-1db5bf5a71f7",
   "metadata": {},
   "source": [
    "À noter que pour obliger un algorithme à adopter une stratégie qui n’est pas conforme à sa nature, il existe des classes spécifiques `OneVsRestClassifier` et `OneVsOneClassifier` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed467d8-4392-4945-be42-3fab79b28cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model = OneVsRestClassifier(SVC(random_state=42))\n",
    "_ = model.fit(X_train, y_train)\n",
    "\n",
    "model.predict([ X_test[0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe1600-ef34-48c7-a382-df50ebeb9832",
   "metadata": {},
   "source": [
    "La propriété `estimators_` permet de vérifier que la stratégie OvA utilise bien autant d’estimateurs que de modalités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20285c17-8e35-4d0c-b4df-fb479eddfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0de820-ab52-44f4-995c-5ad29ad61e50",
   "metadata": {},
   "source": [
    "#### Analyser la matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfa643-e1cb-4e64-a5eb-d9a11153724b",
   "metadata": {},
   "source": [
    "Comme pour la classification binaire, le premier réflexe est de s’attarder sur la matrice de confusion. Commençons par obtenir des prédictions grâce à la validation croisée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b10a3f-df0c-4204-b4cd-822827b6eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom class names\n",
    "model.classes_ = [\"punk\", \"métal\", \"grunge\", \"hardcore\"]\n",
    "\n",
    "y_pred = cross_val_predict(model, X_test, y_test, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d69069-087d-4fea-8ae1-a4b7f7097287",
   "metadata": {},
   "source": [
    "Affichons à présent la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246a480-f8ed-4fb9-8b5c-ff2e0e08dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test, y_pred)\n",
    "_ = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels=model.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114e322-f4a1-4992-bed4-0cd4d4863c59",
   "metadata": {},
   "source": [
    "Les scores les plus hauts étant sur la diagonale, d’un coup d’œil nous savons que notre classificateur est plutôt performant. Une hésitation toufois pour le grunge, qui semble moins facilement identifiable que les autres styles musicaux : 65 ont été considérés comme du punk.\n",
    "\n",
    "Notre machine n’est finalement pas si performante que cela ou bien notre jeu de données comporte-t-il moins de musiques de ce style que les autres ?\n",
    "\n",
    "Faisons le total pour chaque genre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7470b-d71e-4c96-8762-535ebb68fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = cfm.sum(axis=1, keepdims=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2181d-6fd2-4357-8e27-63425338e56b",
   "metadata": {},
   "source": [
    "En divisant les deux matrices, nous calculons le taux d’erreur pour chaque modalité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee6341-d71e-4320-a758-74e9c5a2cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rates = cfm / row_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568203b-ea16-4925-9b6b-3e63b24f75e6",
   "metadata": {},
   "source": [
    "Avant d’afficher de nouveau une carte thermique, remplissons la diagonale de zéros pour plus de lisibilité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba02540-a3f7-48cb-a577-48436d6084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(error_rates, 0)\n",
    "_ = sns.heatmap(error_rates, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4b68e-5881-4876-b8e6-531440eb7372",
   "metadata": {},
   "source": [
    "La carte se lit dans les deux sens :\n",
    "\n",
    "- en lignes, les classes véritables ;\n",
    "- en colonnes, les classes prédites.\n",
    "\n",
    "La ligne 2, comme elle est plus sombre, montre que le grunge est souvent mal étiqueté, quand le style punk (ligne 0) semble être lui bien identifié. La colonne 3, étant plus claire, tend à indiquer que de nombreuses musiques lui sont faussement attribuées.\n",
    "\n",
    "De l’analyse de la matrice de confusion, il est souhaitable de tirer des pistes d’amélioration, comme, dans notre exemple, enrichir le jeu de données de musiques hardcore ou se poser la question de son échec à classer les morceaux punks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
